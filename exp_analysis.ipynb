{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Classes_New import * #Fix this later\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Class_fxns import * #Fix this later\n",
    "from bo_methods_lib.bo_methods_lib.analyze_data import * #Fix this later\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Classes_plotters import * #Fix this later\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import os\n",
    "mpl.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import signac\n",
    "import os\n",
    "from itertools import combinations\n",
    "import signac\n",
    "\n",
    "import bo_methods_lib\n",
    "from bo_methods_lib.bo_methods_lib.analyze_data import General_Analysis\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Classes_plotters import Plotters\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "# from sklearn.exceptions import InconsistentVersionWarning\n",
    "# warnings.filterwarnings(action='ignore', category=InconsistentVersionWarning)\n",
    "\n",
    "# Set Stuff\n",
    "meth_name_val_list = [1]\n",
    "save_csv = False  # Set to False if you don't want to save/resave csvs\n",
    "save_figs = False\n",
    "modes = [\"act\"]\n",
    "project = signac.get_project(\"GPBO_Fix\")\n",
    "\n",
    "for val in [15]:\n",
    "    criteria_dict = {\n",
    "        \"cs_name_val\": val,\n",
    "        \"ep_enum_val\": 1,\n",
    "        \"gp_package\": \"gpflow\",\n",
    "        \"meth_name_val\": {\"$in\": meth_name_val_list},\n",
    "    }\n",
    "    for mode in modes:\n",
    "        analyzer = General_Analysis(criteria_dict, project, mode, save_csv)\n",
    "        plotters = Plotters(analyzer, save_figs)\n",
    "\n",
    "        ###Get all data from experiments\n",
    "        df_all_jobs, job_list, theta_true_data = analyzer.get_df_all_jobs(\n",
    "            criteria_dict, save_csv\n",
    "        )\n",
    "        ### Get Best Data from ep experiment\n",
    "        df_best, job_list_best = analyzer.get_best_data()\n",
    "\n",
    "        # Loop over z_choices to make comparison line plots\n",
    "        z_choices = [\"min_sse\"]\n",
    "        titles = [\n",
    "            \"Min SSE Parameter Values\",\n",
    "            \"Min SSE Parameter Values Overall\",\n",
    "            \"Optimal Acq Func Parameter Values\",\n",
    "        ]\n",
    "\n",
    "        # Make Parity Plots\n",
    "        # plotters.make_parity_plots()\n",
    "\n",
    "        # Get best plots for all objectives with all 7 methods on each subplot\n",
    "        plotters.plot_objs_all_methods(z_choices)\n",
    "\n",
    "        # Get plot with each method on a different subplot for each obj\n",
    "        for i in range(len(z_choices)):\n",
    "            plotters.plot_one_obj_all_methods(z_choices[i])\n",
    "\n",
    "        #Can optionally make plots for hyperparameters and theta values\n",
    "        # for i in range(len(job_list_best)):\n",
    "        #     #Plot hyperparameters\n",
    "        #     plotters.plot_hypers(job_list_best[i])\n",
    "\n",
    "        #     #Plot param values at min_sse, the best theta_values of min_sse overall, and param values at max ei\n",
    "        #     for j in range(len(z_choices)):\n",
    "        #         plotters.plot_thetas(job_list_best[i], z_choices[j], title = titles[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import signac\n",
    "from itertools import combinations\n",
    "from bo_methods_lib.bo_methods_lib.analyze_data import General_Analysis\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Classes_plotters import Plotters\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Classes_New import * #Fix this later\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Class_fxns import * #Fix this later\n",
    "from bo_methods_lib.bo_methods_lib.analyze_data import * #Fix this later\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Classes_plotters import * #Fix this later\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "meth_name_val_list = [1,2,3,5,6]\n",
    "save_csv = False  # Set to False if you don't want to save/resave csvs\n",
    "save_figs = False\n",
    "modes = [\"act\"]\n",
    "project = signac.get_project(\"GPBO_rand\")\n",
    "tot_runs_nls = 1000\n",
    "\n",
    "for val in [15]:\n",
    "    criteria_dict = {\n",
    "        \"cs_name_val\": val,\n",
    "        \"ep_enum_val\": 1,\n",
    "        \"gp_package\": \"gpflow\",\n",
    "        \"meth_name_val\": {\"$in\": meth_name_val_list},\n",
    "    }\n",
    "\n",
    "    for mode in modes:\n",
    "        # analyzer = General_Analysis(criteria_dict, project, mode, save_csv)\n",
    "        simulator = simulator_helper_test_fxns(val, 0, None, 1) #This is a dummy simulator object\n",
    "        analyzer = LS_Analysis(criteria_dict, project, save_csv, simulator=simulator)\n",
    "        plotters = Plotters(analyzer, save_figs)\n",
    "\n",
    "        ###Get all data from experiments\n",
    "        df_all_jobs, job_list, theta_true_data = analyzer.get_df_all_jobs(\n",
    "            criteria_dict, save_csv\n",
    "        )\n",
    "\n",
    "        # Get Best Data from ep experiment\n",
    "        ### Get Best Data from ep experiment\n",
    "        df_best, job_list_best = analyzer.get_best_data()\n",
    "\n",
    "        # Set z_choices and levels\n",
    "        # z_choices = [\"sse_sim\", \"sse_mean\", \"sse_var\", \"acq\"]\n",
    "        # levels = [100, 100, 100, 100]\n",
    "        levels = 10\n",
    "\n",
    "        # Loop over best jobs\n",
    "        for i in range(len(job_list_best)):\n",
    "            # Get jobs, runs, and iters to examine\n",
    "            job = job_list_best[i]\n",
    "            run_num = int(df_best[\"Run Number\"].iloc[i])\n",
    "            bo_iter = int(df_best[\"BO Iter\"].iloc[i])\n",
    "\n",
    "            # Back out number of parameters\n",
    "            string_val = df_best[\"Theta Min Obj\"].iloc[0]\n",
    "            try:\n",
    "                numbers = [\n",
    "                    float(num)\n",
    "                    for num in string_val.replace(\"[\", \"\").replace(\"]\", \"\").split()\n",
    "                ]\n",
    "            except:\n",
    "                numbers = [float(num) for num in string_val]\n",
    "\n",
    "            # Create list of parameter pair combinations\n",
    "            dim_theta = len(np.array(numbers).reshape(-1, 1))\n",
    "            dim_list = np.linspace(0, dim_theta - 1, dim_theta)\n",
    "            pairs = len((list(combinations(dim_list, 2))))\n",
    "\n",
    "            # Loop over parameter pairs\n",
    "            if pairs < 3:\n",
    "                for pair in range(pairs):\n",
    "                    if i == 0:\n",
    "                        plotters.plot_local_min_hms(pair, levels, tot_runs_nls)\n",
    "                    # plotters.plot_hms_gp_compare(\n",
    "                    #     job, run_num, bo_iter, pair, z_choices, levels\n",
    "                    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signac\n",
    "import os\n",
    "from bo_methods_lib.bo_methods_lib.analyze_data import LS_Analysis\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Class_fxns import simulator_helper_test_fxns\n",
    "\n",
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#Set Stuff\n",
    "meth_name_val_list = [1,2,3,4,5,6,7]\n",
    "save_csv = False #Set to False if you don't want to save/resave csvs\n",
    "save_figs = True\n",
    "project = signac.get_project(\"GPBO_Fix\")\n",
    "seed = 1\n",
    "\n",
    "for val in [15]:\n",
    "    criteria_dict = {\n",
    "        \"cs_name_val\": val,\n",
    "        \"ep_enum_val\": 1,\n",
    "        \"gp_package\": \"gpflow\",\n",
    "        \"meth_name_val\": {\"$in\": meth_name_val_list},\n",
    "    }\n",
    "\n",
    "    analyzer = LS_Analysis(criteria_dict, project, save_csv)\n",
    "    \n",
    "    #Get Simulator Object\n",
    "    simulator = simulator_helper_test_fxns(val, 0, None, 1) #This is a dummy simulator object\n",
    "    #Get all least squares solutions\n",
    "    tot_runs = 1000\n",
    "    ls_analyzer = LS_Analysis(criteria_dict, project, save_csv, simulator=simulator)\n",
    "    plotters = Plotters(ls_analyzer, save_figs)\n",
    "    # local_mins = ls_analyzer.categ_min(tot_runs)\n",
    "    hist_categ_min = plotters.hist_categ_min(tot_runs, True, True)\n",
    "    # df = pd.read_csv(\"all_sets1.csv\")\n",
    "    # df[\"Theta Min Obj Cum.\"] = df[\"Theta Min Obj Cum.\"].to_numpy()\n",
    "\n",
    "    # df = df.drop_duplicates(\n",
    "    #     subset=\"Theta Min Obj Cum.\", keep=\"first\"\n",
    "    # )\n",
    "    # df = df[df[\"Optimality\"] < 1e-4]\n",
    "    # df[\"Theta Min Obj Cum.\"] = df[\"Theta Min Obj Cum.\"].apply(ls_analyzer.str_to_array_df_col)\n",
    "    # # Drop minima with optimality > 1e-4\n",
    "    \n",
    "    # # Drop duplicate minima\n",
    "    \n",
    "    # print(len(df))\n",
    "    # print(df[\"Theta Min Obj Cum.\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/scratch365/mcarlozo/Toy_Problem/GPBO_Fix/Results_act/cs_name_val_in_11_14_2_15_17_12_13_3/meth_name_val_in_1_2_3_4_5_6_7/full_results.csv\")\n",
    "#Filter out the Min Obj Run for each Run, CS Name, and Method\n",
    "result_df = df.loc[df.groupby(['BO Method', 'CS Name Val', 'Run Number'])['Min Obj Act Cum'].idxmin()]\n",
    "for cs_name, group in result_df.groupby('CS Name Val'):\n",
    "    if cs_name == 3:\n",
    "        df_GPBO = group[~group['BO Method'].isin(['Conventional', 'Log Conventional'])]\n",
    "        df_ls = pd.read_csv(\"GPBO_Fix/Results_act/cs_name_val_\" + str(cs_name)+\"/ls_local_min_1000.csv\")\n",
    "        df1 = np.vstack(df_GPBO[\"Theta Obj Act Cum\"].apply(ls_analyzer.str_to_array_df_col).values)\n",
    "        df2 = np.vstack(df_ls[\"Theta Min Obj Cum.\"].apply(ls_analyzer.str_to_array_df_col).values)\n",
    "\n",
    "        print(df1.shape, df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the arrays are already scaled (if not, scale them using MinMaxScaler as you did previously)\n",
    "theta_bounds = ls_analyzer.simulator.bounds_theta_reg\n",
    "scaler = MinMaxScaler()\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit([theta_bounds[0], theta_bounds[1]])\n",
    "all_param_sets_GPBO = np.array(list(map(np.array, df_GPBO[\"Theta Obj Act Cum\"].apply(ls_analyzer.str_to_array_df_col))))\n",
    "all_param_sets_ls = np.array(list(map(np.array, df_ls[\"Theta Min Obj Cum.\"].apply(ls_analyzer.str_to_array_df_col))))\n",
    "GPBO_scl = scaler.transform(all_param_sets_GPBO)\n",
    "ls_scl = scaler.transform(all_param_sets_ls)\n",
    "all_param_sets = np.vstack((GPBO_scl, ls_scl))\n",
    "# Calculate the pairwise distances between df1 and df2\n",
    "dist = pdist(all_param_sets)  # Stack the arrays for pairwise distance calculation\n",
    "dist_sq = squareform(dist)  # Convert to a square distance matrix\n",
    "\n",
    "# Tolerance threshold (adjust as necessary)\n",
    "threshold = 0.01\n",
    "\n",
    " # Initialize an array to count occurrences of each unique set\n",
    "minima_count = np.zeros(all_param_sets.shape[0], dtype=int)\n",
    "#Initialize a boolean array to keep track of unique sets\n",
    "unique_mask = np.ones(all_param_sets_GPBO.shape[0], dtype=bool)\n",
    "print(len(unique_mask))\n",
    "print(len(df1))\n",
    "matches = 0\n",
    "match_count_vector = np.zeros(len(ls_scl), dtype=int)\n",
    "\n",
    "# Iterate over the upper triangle of the distance matrix\n",
    "for row in GPBO_scl:\n",
    "    # If the current set is already marked as non-unique, skip it\n",
    "    distances = np.linalg.norm(ls_scl - row, axis=1)\n",
    "    # Mark sets within the threshold distance as non-unique\n",
    "    # If any distance is less than 0.1, consider it a match\n",
    "    matched_indices = np.where(distances < 0.1)[0]    \n",
    "    if matched_indices.size > 0:\n",
    "        matches += 1\n",
    "        unique_mask[matches] = True\n",
    "        # Increment the count for each matching row in array_3x2\n",
    "        for idx in matched_indices:\n",
    "            match_count_vector[idx] += 1\n",
    "\n",
    "# Filter out the unique sets from the pandas df\n",
    "local_min_sets = df_GPBO[unique_mask]\n",
    "# print(local_min_sets)\n",
    "\n",
    "# Change tuples to arrays\n",
    "local_min_sets = local_min_sets.copy()  # Ensure you're working with a copy\n",
    "# local_min_sets[\"Num Occurrences\"] = minima_count[unique_mask]\n",
    "local_min_sets[\"Theta Obj Act Cum\"] = local_min_sets[\"Theta Obj Act Cum\"].apply(np.array)\n",
    "\n",
    "# Put in order of lowest sse and reset index\n",
    "local_min_sets = local_min_sets.sort_values(\n",
    "    by=[\"Min Obj Act Cum\"], ascending=True\n",
    ")\n",
    "local_min_sets = local_min_sets.reset_index(drop=True)\n",
    "\n",
    "print(sum(match_count_vector)/len(GPBO_scl))\n",
    "print(\"Matched rows (25x2 index, matched 3x2 indices):\", match_count_vector)\n",
    "print(local_min_sets[\"Theta Obj Act Cum\"].shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Scale values between 0 and 1 with minmax scaler\n",
    "all_sets = df\n",
    "theta_bounds = ls_analyzer.simulator.bounds_theta_reg\n",
    "scaler = MinMaxScaler()\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit([theta_bounds[0], theta_bounds[1]])\n",
    "all_param_sets = np.array(list(map(np.array, all_sets[\"Theta Min Obj Cum.\"].values)))\n",
    "all_param_sets_scaled = scaler.transform(all_param_sets)\n",
    "#Calculate the scaled euclidean distance between each pair of scaled points\n",
    "dist = pdist(all_param_sets_scaled)/np.sqrt(all_param_sets.shape[1])\n",
    "#Convert the condensed distance matrix to square form\n",
    "dist_sq = squareform(dist)\n",
    "\n",
    "# Initialize an array to count occurrences of each unique set\n",
    "minima_count = np.zeros(all_param_sets.shape[0], dtype=int)\n",
    "#Initialize a boolean array to keep track of unique sets\n",
    "unique_mask = np.ones(all_param_sets.shape[0], dtype=bool)\n",
    "\n",
    "# Iterate over the upper triangle of the distance matrix\n",
    "for i in range(all_param_sets.shape[0]):\n",
    "    # If the current set is already marked as non-unique, skip it\n",
    "    if not unique_mask[i]:\n",
    "        continue\n",
    "    # Mark sets within the threshold distance as non-unique\n",
    "    within_threshold = dist_sq[i] <= 0.01\n",
    "    minima_count[i] += np.sum(within_threshold)\n",
    "    unique_mask[within_threshold] = False\n",
    "    unique_mask[i] = True  # Keep the current set\n",
    "\n",
    "\n",
    "# Filter out the unique sets from the pandas df\n",
    "local_min_sets = all_sets[unique_mask]\n",
    "print(len(local_min_sets))\n",
    "print(minima_count[unique_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ls_best[\"Theta Min Obj Cum.\"] = np.array(list(map(np.array, df_ls_best[\"Theta Min Obj Cum.\"].values)))\n",
    "# df_ls_best = local_min_sets\n",
    "# theta_vals = np.vstack(local_min_sets['Theta Min Obj Cum.'].values)\n",
    "# all_theta_vals = np.vstack(df[\"Theta Min Obj Cum.\"].values)\n",
    "# unique_theta, theta_counts = np.unique(all_theta_vals, axis=0, return_counts=True)\n",
    "# print(len(unique_theta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.float_format', '{:.2e}'.format)\n",
    "for i in [11,14,2,15,1,17,12,13,3,10]:\n",
    "    filename_in = \"/scratch365/mcarlozo/Toy_Problem/GPBO_Fix/Results_act/cs_name_val_\" +str(i) +\"/ep_enum_val_1/gp_package_gpflow/meth_name_val_in_1_2_3_4_5_6_7/ls_best_run.csv\"\n",
    "    df = pd.read_csv(filename_in)\n",
    "    df[\"Run Time\"] = df[\"Run Time\"].apply(lambda x: x/60)\n",
    "    row = df.loc[[0], [\"Min Obj Act\", \"l2 norm\", \"Run Time\", \"Iter\",\"Termination\"]]\n",
    "    print(\"CS: \", i)\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signac\n",
    "from bo_methods_lib.bo_methods_lib.analyze_data import All_CS_Analysis\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Classes_plotters import All_CS_Plotter\n",
    "\n",
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#Set parameters\n",
    "meth_list = [1, 2, 3, 4, 5, 6, 7]\n",
    "cs_list = [11,14,2,15,1,17,12,13,3,10]\n",
    "save_csv = False #Set to False if you don't want to save/resave csvs\n",
    "save_figs = False\n",
    "bar_modes = [\"time\", \"si_time\"] #time and/or objs, si_time\n",
    "project = signac.get_project(\"GPBO_Fix\")\n",
    "\n",
    "analyzer = All_CS_Analysis(cs_list, meth_list, project, \"act\", save_csv)\n",
    "plotters = All_CS_Plotter(analyzer, save_figs)\n",
    "\n",
    "#Get % true found\n",
    "cs_list2 = [2,15,3,17] #[10,14] #[1,12,13,11] #[2,15,3,17]\n",
    "#Change cs_list here to get averages over select case studies\n",
    "vals = analyzer.get_percent_true_found(cs_list2)\n",
    "#print min max, and median l2 norm\n",
    "print(vals)\n",
    "# vals.to_csv(\"percent_true_found.csv\")\n",
    "#Make Overall GPBO bar charts\n",
    "# for bmode in bar_modes:\n",
    "#     df_average = plotters.make_bar_charts(bmode)\n",
    "\n",
    "#Make Derivative Free Bar Charts\n",
    "# df_med_derivfree = plotters.make_derivfree_bar(s_meths = [\"NLS\", \"SHGO-Sob\", \"NM\", \"GA\"], ver = \"med\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import signac\n",
    "import os\n",
    "from itertools import combinations\n",
    "import signac\n",
    "\n",
    "import bo_methods_lib\n",
    "from bo_methods_lib.bo_methods_lib.analyze_data import General_Analysis, open_file_helper\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Classes_plotters import Plotters\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "# from sklearn.exceptions import InconsistentVersionWarning\n",
    "# warnings.filterwarnings(action='ignore', category=InconsistentVersionWarning)\n",
    "\n",
    "# Set Stuff\n",
    "meth_name_val_list = [1]\n",
    "save_csv = False  # Set to False if you don't want to save/resave csvs\n",
    "save_figs = False\n",
    "modes = [\"act\"] #, \"gp\", \"acq\"\n",
    "project = signac.get_project(\"GPBO_RNG\")\n",
    "# project = signac.get_project(\"GPBO_Fix\")\n",
    "\n",
    "for val in [17]:\n",
    "    criteria_dict = {\n",
    "        \"cs_name_val\": val,\n",
    "        \"ep_enum_val\": 1,\n",
    "        \"gp_package\": \"gpflow\",\n",
    "        \"meth_name_val\": {\"$in\": meth_name_val_list},\n",
    "    }\n",
    "    for mode in modes:\n",
    "        analyzer = General_Analysis(criteria_dict, project, mode, save_csv)\n",
    "        plotters = Plotters(analyzer, save_figs)\n",
    "\n",
    "        ##Get all data from experiments\n",
    "        df_all_jobs, job_list, theta_true_data = analyzer.get_df_all_jobs(\n",
    "            criteria_dict, save_csv\n",
    "        )\n",
    "        # ### Get Best Data from ep experiment\n",
    "        df_best, job_list_best = analyzer.get_best_data()\n",
    "\n",
    "        # # Loop over z_choices to make comparison line plots\n",
    "        z_choices = [\"min_sse\"]\n",
    "        titles = [\n",
    "            \"Min SSE Parameter Values\",\n",
    "            \"Min SSE Parameter Values Overall\",\n",
    "            \"Optimal Acq Func Parameter Values\",\n",
    "        ]\n",
    "\n",
    "        # Get best plots for all objectives with all 7 methods on each subplot\n",
    "        # plotters.plot_objs_all_methods(z_choices)\n",
    "\n",
    "        # # Get plot with each method on a different subplot for each obj\n",
    "        for i in range(len(z_choices)):\n",
    "            plotters.plot_one_obj_all_methods(z_choices[i])\n",
    "\n",
    "        #Can optionally make plots for hyperparameters and theta values\n",
    "        # for i in range(len(job_list_best)):\n",
    "            #Plot hyperparameters\n",
    "            # plotters.plot_hypers(job_list_best[i])\n",
    "\n",
    "            #Plot param values at min_sse, the best theta_values of min_sse overall, and param values at max ei\n",
    "            # for j in range(len(z_choices)):\n",
    "            #     plotters.plot_thetas(job_list_best[i], z_choices[j], title = titles[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import signac\n",
    "import os\n",
    "from itertools import combinations\n",
    "import signac\n",
    "\n",
    "import bo_methods_lib\n",
    "from bo_methods_lib.bo_methods_lib.analyze_data import General_Analysis\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Classes_plotters import Plotters\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "# from sklearn.exceptions import InconsistentVersionWarning\n",
    "# warnings.filterwarnings(action='ignore', category=InconsistentVersionWarning)\n",
    "\n",
    "# Set Stuff\n",
    "meth_name_val_list = [2]\n",
    "save_csv = False  # Set to False if you don't want to save/resave csvs\n",
    "save_figs = False\n",
    "modes = [\"act\"] #, \"gp\", \"acq\"\n",
    "project = signac.get_project(\"GPBO_Fix\")\n",
    "\n",
    "for val in [10]:\n",
    "    criteria_dict = {\n",
    "        \"cs_name_val\": val,\n",
    "        \"ep_enum_val\": 1,\n",
    "        \"gp_package\": \"gpflow\",\n",
    "        \"meth_name_val\": {\"$in\": meth_name_val_list},\n",
    "    }\n",
    "    for mode in modes:\n",
    "        analyzer = General_Analysis(criteria_dict, project, mode, save_csv)\n",
    "        plotters = Plotters(analyzer, save_figs)\n",
    "\n",
    "        ###Get all data from experiments\n",
    "        # df_all_jobs, job_list, theta_true_data = analyzer.get_df_all_jobs(\n",
    "        #     criteria_dict, save_csv\n",
    "        # )\n",
    "        # ### Get Best Data from ep experiment\n",
    "        # df_best, job_list_best = analyzer.get_best_data()\n",
    "\n",
    "        #Load the training data from each job in criteria dict\n",
    "        jobs = signac.get_project(\"GPBO_Fix\").find_jobs(criteria_dict)\n",
    "        print(\"Number of jobs: \", list(jobs)[0], list(jobs)[1])\n",
    "\n",
    "        #Open the results file\n",
    "        results = open_file_helper(\"GPBO_Fix/workspace/f577d8b53002f15c3e91c9a8acd7c2c8/BO_Results_GPs.gz\")\n",
    "        results2 = open_file_helper(\"GPBO_Fix/workspace/98641f0f92dbface9a35f2912eaa3d2a/BO_Results_GPs.gz\")\n",
    "        print(results[0].list_gp_emulator_class[0].gp_sim_data.y_vals)\n",
    "        print(results2[0].list_gp_emulator_class[0].gp_sim_data.y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Simulator Class (Export your Simulator Object Here)\n",
    "simulator = simulator_helper_test_fxns(1,0,0, 1)\n",
    "\n",
    "# Generate Exp Data (OR Add your own experimental data as a Data class object)\n",
    "set_seed = 1  # Set set_seed to 1 for data generation\n",
    "gen_meth_x = Gen_meth_enum(2)\n",
    "num_pts = 100\n",
    "exp_data = simulator.gen_exp_data(num_pts, gen_meth_x, 1)\n",
    "\n",
    "# Set simulator noise_std artifically as 5% of y_exp mean (So that noise will be set rather than trained)\n",
    "simulator.noise_std = np.abs(np.mean(exp_data.y_vals)) * 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp_data.y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the data\n",
    "ax.plot(exp_data.x_vals, exp_data.y_vals, color='blue', linestyle='-', linewidth=2)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('X-Value')\n",
    "ax.set_ylabel('Y-Value')\n",
    "ax.set_title('BOD Curve Example')\n",
    "\n",
    "# Add grid\n",
    "ax.grid(True)\n",
    "\n",
    "# Add legend\n",
    "# ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate sample 2D data\n",
    "X, Y = exp_data.x_vals.reshape(num_pts, num_pts,2).T\n",
    "Z = exp_data.y_vals.reshape(X.shape)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot filled contours with contourf\n",
    "contour_filled = ax.contourf(X, Y, Z, levels=20, cmap='jet')\n",
    "\n",
    "# Add contour lines\n",
    "contour_lines = ax.contour(X, Y, Z, levels=20, colors='black', linestyle = '--', linewidths=0.5)\n",
    "\n",
    "# Add a colorbar for the filled contours\n",
    "cbar = fig.colorbar(contour_filled, ax=ax)\n",
    "cbar.set_label('Magnitude')\n",
    "\n",
    "# Add labels to the contour lines\n",
    "ax.clabel(contour_lines, inline=True, fontsize=8)\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "\n",
    "# Title\n",
    "ax.set_title('Müller ' + r'$x_0$' ' Data')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Toy_Problem_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
