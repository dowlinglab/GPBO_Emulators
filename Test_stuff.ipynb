{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e3b690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from itertools import combinations_with_replacement\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "from scipy.stats import qmc\n",
    "\n",
    "import bo_methods_lib\n",
    "from bo_methods_lib.bo_functions_generic import gen_theta_set, find_train_doc_path, test_train_split, clean_1D_arrays, norm_unnorm\n",
    "from bo_methods_lib.normalize import normalize_p_data, normalize_x, normalize_p_true, normalize_p_bounds\n",
    "from bo_methods_lib.CS2_create_data import create_sse_data\n",
    "import itertools\n",
    "from itertools import combinations_with_replacement\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "\n",
    "# import matplotlib as mpl\n",
    "# mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98a6579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.61032438 0.43428296 0.72599213 0.66055616 0.52820981 0.95271155\n",
    "#   0.94534902 0.87020441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5b42f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[2],[2]])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad50c497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2,)\n",
      "(5, 1)\n",
      "(400, 2)\n",
      "(100, 5)\n",
      "torch.Size([20, 3])\n",
      "torch.Size([80, 3])\n"
     ]
    }
   ],
   "source": [
    "CS = 1\n",
    "\n",
    "Constants = np.array([[-200,-100,-170,15],\n",
    "                      [-1,-1,-6.5,0.7],\n",
    "                      [0,0,11,0.6],\n",
    "                      [-10,-10,-6.5,0.7],\n",
    "                      [1,0,-0.5,-1],\n",
    "                      [0,0.5,1.5,1]])\n",
    "if CS == 2.2:\n",
    "    skip_param_types = 1 #This is what changes for subpoint\n",
    "    Theta_True = Constants[skip_param_types:skip_param_types+2].flatten()\n",
    "    param_dict = {0 : 'a_1', 1 : 'a_2', 2 : 'a_3', 3 : 'a_4',\n",
    "                  4 : 'b_1', 5 : 'b_2', 6 : 'b_3', 7 : 'b_4'}\n",
    "    exp_d = 2\n",
    "    n = 15 #Number of experimental data points to use\n",
    "    bounds_p = np.array([[-2, -2, -10, -2, -2, -2,  5, -2],\n",
    "                       [ 2,  2,   0,  2,  2,  2, 15,  2]])\n",
    "    bounds_x = np.array([[-1.5,-0.5],[1,2]])\n",
    "    LHS = True\n",
    "\n",
    "else:\n",
    "    Constants = Theta_True = np.array([1,-1])\n",
    "    skip_param_types = 0\n",
    "    param_dict = {0 : '\\\\theta_1', 1 : '\\\\theta_2'}\n",
    "    exp_d = 1\n",
    "    bounds_x = np.array([[-2],[2]])\n",
    "    bounds_p = np.array([[-2,-2],\n",
    "                     [2 , 2]])\n",
    "    LHS = False\n",
    "    n = 5\n",
    "\n",
    "print(Constants.shape)\n",
    "print(Theta_True.shape)\n",
    "\n",
    "p = 20\n",
    "d = len(Theta_True)\n",
    "\n",
    "obj = np.array([\"obj\"])\n",
    "emulator = np.array([True])\n",
    "\n",
    "if emulator == True:\n",
    "    t = 20*n\n",
    "else:\n",
    "    t = 20\n",
    "\n",
    "# #Pull Experimental data from CSV\n",
    "exp_data_doc = 'Input_CSVs/Exp_Data/d='+str(exp_d)+'/n='+str(n)+'.csv'\n",
    "exp_data = np.array(pd.read_csv(exp_data_doc, header=0,sep=\",\"))\n",
    "Xexp = exp_data[:,1:exp_d+1]\n",
    "Yexp = exp_data[:,-1]\n",
    "Xexp = clean_1D_arrays(Xexp)\n",
    "\n",
    "m = Xexp.shape[1]\n",
    "print(Xexp.shape)\n",
    "\n",
    "theta_mesh = gen_theta_set(LHS = LHS, n_points = p, dimensions = d, bounds = bounds_p)\n",
    "print(theta_mesh.shape)\n",
    "theta_set = theta_mesh\n",
    "\n",
    "all_data_doc = find_train_doc_path(emulator, obj, d, t)\n",
    "all_data = np.array(pd.read_csv(all_data_doc, header=0,sep=\",\")) \n",
    "print(all_data.shape)\n",
    "\n",
    "train_data, test_data = test_train_split(all_data, sep_fact = 0.2, shuffle_seed=9)\n",
    "\n",
    "if CS == 1 and emulator == False:\n",
    "    train_p, test_p = train_data[:,1:-2], test_data[:,1:-2]\n",
    "else:\n",
    "    train_p, test_p = train_data[:,1:-1], test_data[:,1:-1]\n",
    "# print(train_p)\n",
    "print(train_p.shape)\n",
    "print(test_p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5228515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  ]\n",
      " [0.25]\n",
      " [0.5 ]\n",
      " [0.75]\n",
      " [1.  ]]\n",
      "[0.]\n",
      "(array([[-2.]]), MinMaxScaler())\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,8) (2,) (1,8) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(normalize_x(Xexxp, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, scaler_x))\n\u001b[1;32m     11\u001b[0m Guess_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.61032438\u001b[39m,\u001b[38;5;241m0.43428296\u001b[39m,\u001b[38;5;241m0.72599213\u001b[39m,\u001b[38;5;241m0.66055616\u001b[39m,\u001b[38;5;241m0.52820981\u001b[39m,\u001b[38;5;241m0.95271155\u001b[39m,\u001b[38;5;241m0.94534902\u001b[39m,\u001b[38;5;241m0.87020441\u001b[39m])\n\u001b[0;32m---> 12\u001b[0m Guess_0 \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_p_true\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGuess_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m Guess_1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0.370735521\u001b[39m, \u001b[38;5;241m9.23236422E-04\u001b[39m,\u001b[38;5;241m0.166088842\u001b[39m, \u001b[38;5;241m0.169900862\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0.024381367\u001b[39m])\n\u001b[1;32m     14\u001b[0m Guess_1 \u001b[38;5;241m=\u001b[39m normalize_p_true(Guess_1, scaler_theta, norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/scratch365/mcarlozo/Toy_Problem/bo_methods_lib/normalize.py:197\u001b[0m, in \u001b[0;36mnormalize_p_true\u001b[0;34m(p_true, scaler_theta, norm)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03mNormalizes or unnormalizes parameter data for theta_true\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    theta_true_scl.flatten(): ndarray, rescaled values of theta_true flattened to correct dimensions\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m    \n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m#Normalize or un normalize a 2D shape (1, len(theta_true) array and flatten\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m theta_true_scl, scaler_theta \u001b[38;5;241m=\u001b[39m \u001b[43mnorm_unnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_1D_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_clean\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_theta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m theta_true_scl\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[0;32m/scratch365/mcarlozo/Toy_Problem/bo_methods_lib/bo_functions_generic.py:48\u001b[0m, in \u001b[0;36mnorm_unnorm\u001b[0;34m(X, norm, scaler)\u001b[0m\n\u001b[1;32m     46\u001b[0m     X \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m norm \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#Make scaled data a tensor if it went into the function as a tensor\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor_val \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(X) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/Toy_Problem_env/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:532\u001b[0m, in \u001b[0;36mMinMaxScaler.inverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    526\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    528\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m    529\u001b[0m     X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy, dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    530\u001b[0m )\n\u001b[0;32m--> 532\u001b[0m X \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_\n\u001b[1;32m    533\u001b[0m X \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,8) (2,) (1,8) "
     ]
    }
   ],
   "source": [
    "#Calculating actual SSE Values\n",
    "norm = True\n",
    "bounds_x_scl, scaler_x = normalize_x(bounds_x, None, norm, scaler = None)\n",
    "bounds_p_scl, scaler_theta = normalize_p_bounds(bounds_p, norm)\n",
    "Xexp_scl = normalize_x(Xexp, None, norm, scaler = scaler_x)[0]\n",
    "print(Xexp_scl)\n",
    "Xexxp = Xexp_scl[0]\n",
    "print(Xexxp)\n",
    "print(normalize_x(Xexxp, None, False, scaler_x))\n",
    "\n",
    "Guess_0 = np.array([0.61032438,0.43428296,0.72599213,0.66055616,0.52820981,0.95271155,0.94534902,0.87020441])\n",
    "Guess_0 = normalize_p_true(Guess_0, scaler_theta, norm = False)\n",
    "Guess_1 = np.array([0,1,0.370735521, 9.23236422E-04,0.166088842, 0.169900862,1,0.024381367])\n",
    "Guess_1 = normalize_p_true(Guess_1, scaler_theta, norm = False)\n",
    "Guess_2 = Theta_True\n",
    "\n",
    "\n",
    "\n",
    "SSE_guess_0 = create_sse_data(Guess_0, Xexp, Yexp, Constants, obj = \"obj\", skip_param_types = 1)\n",
    "# GP_guess_0 = 1236476096410.422\n",
    "GP_guess_0 = 74323.09891172957\n",
    "SSE_guess_1 = create_sse_data(Guess_1, Xexp, Yexp, Constants, obj = \"obj\", skip_param_types = 1)\n",
    "GP_guess_1 =-945327999181.6357\n",
    "SSE_guess_2 = create_sse_data(Guess_2, Xexp, Yexp, Constants, obj = \"obj\", skip_param_types = 1)\n",
    "print(Guess_0, Guess_1)\n",
    "print(\"True Params \\n\", Theta_True)\n",
    "print(\"True SSE \\n\", float(SSE_guess_2))\n",
    "print(\"Guess 1 \\n\", np.round(Guess_0,3))\n",
    "print(\"True SSE Guess 1 \\n\", float(SSE_guess_0), \"\\n GP Guess 1 \\n\", GP_guess_0, \"\\n difference %\", abs((GP_guess_0 - float(SSE_guess_0))/float(SSE_guess_0)))\n",
    "# print(\"Guess 2 \\n\", np.round(Guess_1,3))\n",
    "# print(\"True SSE Guess 2 \\n\", float(SSE_guess_1), \"\\n GP Guess 2 \\n\", GP_guess_1, \"\\n difference %\", abs((GP_guess_1 - float(SSE_guess_1))/float(SSE_guess_1)))\n",
    "# print(SSE_guess_2, GP_guess_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4736bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_unnorm(X, norm = True, scaler = None):\n",
    "    \"\"\"\n",
    "    Normalizes and unnormalizes data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "        X: ndarray, The array containing the data to be normalized or unnormalized\n",
    "        norm: bool, default True: Determines whether the data is normalized to 0 and 1 or from 0 and 1 (False)\n",
    "        scaler: MinMaxScaler(), default None, Must be provided when moving from normalized to unnormalized values\n",
    "    Returns:\n",
    "    --------\n",
    "        X: ndarray, New scaled (norm = True) or unscaled (norm = False) values\n",
    "        scaler: MinMaxScaler(), The scaler used in the normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    #Determines whether the original data is a tensor or ndarray\n",
    "#     print(X[0:3])\n",
    "    tensor_val = False\n",
    "    if torch.is_tensor(X) == True:\n",
    "        tensor_val = True\n",
    "    #Normalize or unnormalize data\n",
    "    if norm == True:\n",
    "        if scaler is None:\n",
    "            scaler =  MinMaxScaler()\n",
    "            scaler.fit(X)\n",
    "#         print(\"Transforming without scaler\")\n",
    "        X = scaler.transform(X)\n",
    "    else:\n",
    "        assert scaler is not None, \"Scaler must exist to scale back to normal values\"\n",
    "        X = scaler.inverse_transform(X)\n",
    "    \n",
    "    #Make scaled data a tensor if it went into the function as a tensor\n",
    "    if tensor_val == True and torch.is_tensor(X) == False:\n",
    "        X = torch.from_numpy(X)\n",
    "#     print(torch.is_tensor(X)) \n",
    "    return X, scaler\n",
    "\n",
    "def normalize_x(X_val, train_p_x = None, norm = True, scaler = None):\n",
    "    \"\"\"\n",
    "    Normalizes or unnormalizes x data from training/testing data and experimental data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        X_val: ndarray, experimental x values (Xexp) or Xexp bounds\n",
    "        train_p_x: ndarray or None, x_values to normalize from train/test data or Xexp when standard approaches are used. Default None\n",
    "        norm: bool, whether the value will be normalized to 0 and 1 (True) or from 0 and 1 (False). Default True\n",
    "        scaler: None or MinMaxScaler(), used to un-normalize data or normalize data based on another sets normalization\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        x_scl: ndarray, rescaled values of x\n",
    "        scaler_x: MinMaxScaler(), scaler used to obtain these values\n",
    "    \"\"\"\n",
    "    #Change 1D array to 2S with shape (len(X),1)\n",
    "    X_val = clean_1D_arrays(X_val)\n",
    "       #Changes train_p values if they exist \n",
    "    if train_p_x is not None:\n",
    "        train_p_x = clean_1D_arrays(train_p_x)\n",
    "        #If scaling train_p_x data, scale x data using x training data\n",
    "        x_scl, scaler_x = norm_unnorm(train_p_x, norm, scaler)\n",
    "        \n",
    "    else:\n",
    "        #Scale x data using experimental x data\n",
    "        x_scl, scaler_x = norm_unnorm(X_val, norm, scaler)\n",
    "#     print(x_scl)\n",
    "    return x_scl, scaler_x\n",
    "\n",
    "def normalize_p_bounds(param_vals_data, norm = True, scaler = None):\n",
    "    \"\"\"\n",
    "    Normalizes or unnormalizes parameter data from training/testing data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        param_vals_data: ndarray, parameter values to normalize from training/testing data\n",
    "        m: int, dimensionality of x data\n",
    "        emulator: bool, whether GP is emulating fxn or error\n",
    "        norm: bool, whether the value will be normalized to 0 and 1 (True) or from 0 and 1 (False). Default True\n",
    "        scaler: None or MinMaxScaler(), used to un-normalize data or normalize data based on another sets normalization\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        param_data_scl: ndarray, rescaled values of x\n",
    "        scaler_theta: MinMaxScaler(), scaler used to obtain these values\n",
    "    \"\"\"\n",
    "    #Overwite scaled/normal values with normal/scaled values for bounds\n",
    "    param_data_scl, scaler_theta = norm_unnorm(param_vals_data, norm, scaler)\n",
    "    \n",
    "    return param_data_scl, scaler_theta\n",
    "\n",
    "def normalize_p_data(param_vals_data, m, emulator, norm = True, scaler = None):\n",
    "    \"\"\"\n",
    "    Normalizes or unnormalizes parameter data from training/testing data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        param_vals_data: ndarray, parameter values to normalize from training/testing data\n",
    "        m: int, dimensionality of x data\n",
    "        emulator: bool, whether GP is emulating fxn or error\n",
    "        norm: bool, whether the value will be normalized to 0 and 1 (True) or from 0 and 1 (False). Default True\n",
    "        scaler: None or MinMaxScaler(), used to un-normalize data or normalize data based on another sets normalization\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        param_data_scl: ndarray, rescaled values of x\n",
    "        scaler_theta: MinMaxScaler(), scaler used to obtain these values\n",
    "    \"\"\"\n",
    "    if emulator == True:\n",
    "        if norm == True:\n",
    "            #If using emulator approach and normalizing data, overwrite normal parameter values with scaled values\n",
    "            param_data_scl, scaler_theta = norm_unnorm(param_vals_data[:,0:-m], norm, scaler)\n",
    "        else:\n",
    "            #If using emulator approach and un-normalizing data, overwrite scaled values with normal values\n",
    "            param_data_scl, scaler_theta = norm_unnorm(param_vals_data, norm, scaler)\n",
    "    else:\n",
    "         #If using standard approach overwrite scaled/normal values with normal/scaled values\n",
    "        param_data_scl, scaler_theta = norm_unnorm(param_vals_data, norm, scaler)\n",
    "    return param_data_scl\n",
    "\n",
    "def normalize_p_set(p_set, scaler_theta, norm = True):\n",
    "    \"\"\"\n",
    "    Normalizes or unnormalizes parameter data for theta_set\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        param_vals_data: ndarray, parameter values to normalize from training/testing data\n",
    "        scaler_theta: None or MinMaxScaler(), used to (un)normalize data based on another sets normalization\n",
    "        norm: bool, whether the value will be normalized to 0 and 1 (True) or from 0 and 1 (False). Default True\n",
    "           \n",
    "    Returns\n",
    "    -------\n",
    "        p_set_scl: ndarray, rescaled values of x\n",
    "    \"\"\"\n",
    "    #Normalize or unnormalize set data\n",
    "    p_set_scl, scaler_theta = norm_unnorm(p_set, norm, scaler = scaler_theta)\n",
    "    return p_set_scl\n",
    "\n",
    "def normalize_p_true(p_true, scaler_theta, norm= True):\n",
    "    \"\"\"\n",
    "    Normalizes or unnormalizes parameter data for theta_true\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        p_true: ndarray, True parameter values\n",
    "        scaler_theta: None or MinMaxScaler(), used to (un)normalize data based on another sets normalization\n",
    "        norm: bool, whether the value will be normalized to 0 and 1 (True) or from 0 and 1 (False). Default True\n",
    "           \n",
    "    Returns\n",
    "    -------\n",
    "        theta_true_scl.flatten(): ndarray, rescaled values of theta_true flattened to correct dimensions\n",
    "    \"\"\"    \n",
    "    #Normalize or un normalize a 2D shape (1, len(theta_true) array and flatten\n",
    "    theta_true_scl, scaler_theta = norm_unnorm(clean_1D_arrays(p_true, param_clean = True), \n",
    "                                               norm, scaler_theta)\n",
    "    return theta_true_scl.flatten()\n",
    "\n",
    "def normalize_constants(Constants, p_true, scaler_theta, skip_params, CS, norm = True, scaler_C_before = None, \n",
    "                        scaler_C_after = None):\n",
    "    \"\"\"\n",
    "    Normalizes or unnormalizes data for constants\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        Constants: ndarray, ndarray, True values of Muller potential constants OR p_true (CS1 only)\n",
    "        p_true: ndarray, True parameter values\n",
    "        scaler_theta: None or MinMaxScaler(), used to (un)normalize data based on another sets normalization\n",
    "        skip_params: int, number of sets of parameters in Constants to skip before reaching the first iterable parameter row\n",
    "        CS: float, case study label \n",
    "        norm: bool, whether the value will be normalized to 0 and 1 (True) or from 0 and 1 (False). Default True\n",
    "        scaler_C_before: None or MinMaxScaler(), used to un-normalize normalized constansts before theta constants\n",
    "        scaler_C_after: None or MinMaxScaler(), used to un-normalize normalized constansts after theta constants\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        x_scl: ndarray, rescaled values of x\n",
    "        scaler_x: MinMaxScaler(), scaler used to obtain these values\n",
    "    \"\"\"   \n",
    "    #Determine number of parameter types and the length of each: Ex, Muller Case study has 6, and 2D case study has 1\n",
    "    num_param_types = clean_1D_arrays(Constants).shape[1] #4 - Represents how many indecies are in each param type A, a, b, c, x0, y0\n",
    "    len_param_type = int(len(p_true)/num_param_types)\n",
    "    \n",
    "    #For the case study, the constants are identical to theta_True and we scale them as such\n",
    "    if CS == 1:\n",
    "        Constants_scl = normalize_p_true(p_true, scaler_theta, norm)\n",
    "    #For the Muller potential case studies, we need to normalize constants in chunks\n",
    "    else:\n",
    "        #Define constants before, after, and representing theta_true and normalize them by splitting the constants array is 3 parts\n",
    "#         print(Constants)\n",
    "        Constants_before, Constants_theta, Constants_after = np.split(Constants, [skip_params,len_param_type+1])\n",
    "#         print(Constants_before.shape, Constants_theta.shape, Constants_after.shape)\n",
    "        Constants_before_scl, scaler_C_before = norm_unnorm(Constants_before.T, norm, scaler_C_before)\n",
    "        \n",
    "        Constants_theta_scl, scaler_theta =  norm_unnorm(clean_1D_arrays(Constants_theta.flatten(), param_clean = True),\n",
    "                                                 norm, scaler_theta)\n",
    "        Constants_theta_scl = Constants_theta_scl.reshape((len_param_type,num_param_types)) #(2,4) for 2.2\n",
    "        \n",
    "        Constants_after_scl, scaler_C_after = norm_unnorm(Constants_after.T, norm, scaler_C_after)\n",
    "        \n",
    "        #Restack scaled constants and return their value and necessary scalers\n",
    "        Constants_scl = np.vstack((Constants_before_scl.T, Constants_theta_scl, Constants_after_scl.T))\n",
    "    return Constants_scl, scaler_C_before, scaler_C_after      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c7fc5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3342, 0.4147, 0.5350, 0.8630, 0.3860, 0.9109, 0.8506, 0.6507, 0.4701,\n",
      "         0.7259],\n",
      "        [0.3342, 0.4147, 0.5350, 0.8630, 0.3860, 0.9109, 0.8506, 0.6507, 0.7684,\n",
      "         0.1295],\n",
      "        [0.5134, 0.2081, 0.9344, 0.6391, 0.9617, 0.9742, 0.0992, 0.8902, 0.2140,\n",
      "         0.9082],\n",
      "        [0.2065, 0.0857, 0.0198, 0.1111, 0.6642, 0.5042, 0.9570, 0.4541, 0.2720,\n",
      "         0.4480],\n",
      "        [0.9702, 0.3203, 0.0866, 0.7915, 0.6117, 0.3164, 0.4600, 0.7390, 0.5130,\n",
      "         0.5713]], dtype=torch.float64) tensor([[-0.6631, -0.3411, -4.6496,  1.4519, -0.4559,  1.6435, 13.5061,  0.6028,\n",
      "         -0.3248,  1.3148],\n",
      "        [-0.6631, -0.3411, -4.6496,  1.4519, -0.4559,  1.6435, 13.5061,  0.6028,\n",
      "          0.4211, -0.1763],\n",
      "        [ 0.0537, -1.1677, -0.6558,  0.5563,  1.8467,  1.8968,  5.9925,  1.5609,\n",
      "         -0.9651,  1.7704],\n",
      "        [-1.1740, -1.6574, -9.8016, -1.5555,  0.6568,  0.0169, 14.5698, -0.1836,\n",
      "         -0.8200,  0.6200],\n",
      "        [ 1.8808, -0.7189, -9.1337,  1.1661,  0.4467, -0.7345,  9.5999,  0.9561,\n",
      "         -0.2176,  0.9283]], dtype=torch.float64)\n",
      "tensor([[0.3342, 0.4147, 0.5350, 0.8630, 0.3860, 0.9109, 0.8506, 0.6507, 0.4701,\n",
      "         0.7259],\n",
      "        [0.3342, 0.4147, 0.5350, 0.8630, 0.3860, 0.9109, 0.8506, 0.6507, 0.7684,\n",
      "         0.1295],\n",
      "        [0.5134, 0.2081, 0.9344, 0.6391, 0.9617, 0.9742, 0.0992, 0.8902, 0.2140,\n",
      "         0.9082],\n",
      "        [0.2065, 0.0857, 0.0198, 0.1111, 0.6642, 0.5042, 0.9570, 0.4541, 0.2720,\n",
      "         0.4480],\n",
      "        [0.9702, 0.3203, 0.0866, 0.7915, 0.6117, 0.3164, 0.4600, 0.7390, 0.5130,\n",
      "         0.5713]], dtype=torch.float64)\n",
      "tensor([[0.3342, 0.4147, 0.5350, 0.8630, 0.3860, 0.9109, 0.8506, 0.6507, 0.4701,\n",
      "         0.7259],\n",
      "        [0.3342, 0.4147, 0.5350, 0.8630, 0.3860, 0.9109, 0.8506, 0.6507, 0.7684,\n",
      "         0.1295],\n",
      "        [0.5134, 0.2081, 0.9344, 0.6391, 0.9617, 0.9742, 0.0992, 0.8902, 0.2140,\n",
      "         0.9082],\n",
      "        [0.2065, 0.0857, 0.0198, 0.1111, 0.6642, 0.5042, 0.9570, 0.4541, 0.2720,\n",
      "         0.4480],\n",
      "        [0.9702, 0.3203, 0.0866, 0.7915, 0.6117, 0.3164, 0.4600, 0.7390, 0.5130,\n",
      "         0.5713]], dtype=torch.float64)\n",
      "True\n",
      "True\n",
      "tensor([[ 0.0537, -1.1677, -0.6558,  0.5563,  1.8467,  1.8968,  5.9925,  1.5609,\n",
      "         -0.9651,  1.7704],\n",
      "        [-1.1740, -1.6574, -9.8016, -1.5555,  0.6568,  0.0169, 14.5698, -0.1836,\n",
      "         -0.8200,  0.6200]], dtype=torch.float64) tensor([[ 0.0537, -1.1677, -0.6558,  0.5563,  1.8467,  1.8968,  5.9925,  1.5609,\n",
      "         -0.9651,  1.7704],\n",
      "        [-1.1740, -1.6574, -9.8016, -1.5555,  0.6568,  0.0169, 14.5698, -0.1836,\n",
      "         -0.8200,  0.6200]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "train_p_scl = train_p.clone()\n",
    "train_p_unscl = train_p.clone()\n",
    "norm = True\n",
    "#X values\n",
    "bounds_x_scl, scaler_x = normalize_x(bounds_x, None, norm, scaler = None)\n",
    "train_p_scl[:,-m:] = normalize_x(Xexp, train_p[:,-m:], norm, scaler_x)[0]\n",
    "Xexp_scl = normalize_x(Xexp, None, norm, scaler = scaler_x)[0]\n",
    "# print(Xexp_scl, train_p_scl[:,-m:], bounds_x_scl)\n",
    "\n",
    "#Theta Values\n",
    "bounds_p_scl, scaler_theta = normalize_p_bounds(bounds_p, norm)\n",
    "train_p_scl[:,0:-m] = normalize_p_data(train_p, m, emulator, norm, scaler_theta)\n",
    "theta_set_scl = normalize_p_set(theta_set, scaler_theta, norm)\n",
    "Theta_True_scl = normalize_p_true(Theta_True, scaler_theta, norm)\n",
    "print(train_p_scl[0:5], train_p[0:5])\n",
    "# print(bounds_p_scl, train_p_scl[0:5,0:-m], train_p[0:5,0:-m], theta_set_scl[0:5], theta_set[0:5], Theta_True_scl )\n",
    "\n",
    "train_p_unscl = train_p_scl.clone()\n",
    "norm = False\n",
    "#X values\n",
    "bounds_x_unscl, scaler_x = normalize_x(bounds_x_scl, None, norm, scaler = scaler_x)\n",
    "print(train_p_scl[0:5])\n",
    "train_p_unscl[:,-m:] = normalize_x(Xexp, train_p_scl[:,-m:], norm, scaler_x)[0]\n",
    "print(train_p_scl[0:5])\n",
    "Xexp_unscl = normalize_x(Xexp, None, norm, scaler = scaler_x)[0]\n",
    "\n",
    "#Theta values\n",
    "bounds_p_unscl = normalize_p_bounds(bounds_p, norm, scaler_theta)[0]\n",
    "# print(train_p_scl[:,0:-m])\n",
    "train_p_unscl[:,0:-m] = normalize_p_data(train_p_scl[:,0:-m], m, emulator, norm, scaler_theta)\n",
    "theta_set_unscl = normalize_p_set(theta_set_scl, scaler_theta, norm)\n",
    "Theta_True_unscl = normalize_p_true(Theta_True_scl, scaler_theta, norm)\n",
    "print(np.allclose(train_p_unscl, train_p, rtol=1e-10))\n",
    "print(np.allclose([2,2], [2,2.0000001], rtol=1e-7))\n",
    "print(train_p_unscl[2:4] , train_p[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ecf3839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  5  1]\n",
      " [-1 -8 -4]\n",
      " [ 2  6 -1]]\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bo_functions_generic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(X)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print(Y)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# from sklearn.model_selection import LeaveOneOut\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# X = scaler.inverse_transform(X)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(X)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbo_functions_generic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m norm_unnorm\n\u001b[1;32m     27\u001b[0m norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     28\u001b[0m X_scl, scaler_X \u001b[38;5;241m=\u001b[39m norm_unnorm(X, norm \u001b[38;5;241m=\u001b[39m norm)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bo_functions_generic'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "random.seed(10)\n",
    "X = np.random.randint(-10,10, size = (3,3))\n",
    "Y = np.random.randint(-10,10, size = (3,3))\n",
    "print(X)\n",
    "# print(Y)\n",
    "# from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# y_true  = []\n",
    "# y_pred = []\n",
    "\n",
    "# print(loo.get_n_splits(X))\n",
    "# for train_index, test_index in loo.split(X):\n",
    "#     df_test = X[test_index]\n",
    "#     df_train = X[train_index]\n",
    "#     print(df_train, df_test)\n",
    "    \n",
    "\n",
    "# scaler =  MinMaxScaler()\n",
    "# scaler.fit(X)\n",
    "# X = scaler.transform(X)\n",
    "# print(X)\n",
    "# X = scaler.inverse_transform(X)\n",
    "# print(X)\n",
    "\n",
    "from bo_functions_generic import norm_unnorm\n",
    "norm = True\n",
    "X_scl, scaler_X = norm_unnorm(X, norm = norm)\n",
    "Y_scl, scaler_Y = norm_unnorm(Y, norm = norm)\n",
    "\n",
    "print(X_scl, scaler_X)\n",
    "# print(Y, scaler_Y)\n",
    "\n",
    "X2 = np.array([[0, 1, 0.33],\n",
    "               [0.25 , 0.75, 0.66]])\n",
    "\n",
    "X3 = np.array([[2, 5, -2],\n",
    "               [-5 , 4, 0]])\n",
    "print(X3)\n",
    "X3, scaler_X = norm_unnorm(X3, norm = True, scaler = scaler_X)\n",
    "print(X3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474bc899",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xexp_bounds = np.array([[-1.5, -0.5],\n",
    "                        [1 , 2]])\n",
    "LHS = gen_theta_set(LHS = False, n_points = 20, dimensions = 2, bounds = Xexp_bounds)\n",
    "# print(LHS)\n",
    "# print(LHS.reshape((20,20,-1)).T)\n",
    "\n",
    "\n",
    "X1 = np.linspace(-1.5,1,20)\n",
    "X2 = np.linspace(-0.5,2,20)\n",
    "X_mesh = np.meshgrid(X1,X2)\n",
    "print(np.array(X_mesh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a211af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://realpython.com/python-timer/\n",
    "import time\n",
    "start = 3678393\n",
    "res = datetime.timedelta(seconds =start)\n",
    "print(res)\n",
    "\n",
    "time_tot = 0\n",
    "x = 0\n",
    "for i in range(89900):\n",
    "    start2 = time.time()\n",
    "    x += 1+2\n",
    "    end = time.time()\n",
    "\n",
    "    time_tot+= (end - start2)\n",
    "\n",
    "\n",
    "print(time_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f742f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[4,3,0,0],\n",
    "              [4,0,0,3]])\n",
    "print(A[np.nonzero(A)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e064a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.zeros((3,3))\n",
    "B = pd.DataFrame(B)\n",
    "type(B)\n",
    "isinstance(B, pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2c4e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(9)\n",
    "Array_1 = np.random.rand(3,2)\n",
    "Array_2 = np.random.rand(3,2)\n",
    "\n",
    "print(Array_1)\n",
    "print(Array_2)\n",
    "\n",
    "ds = gdal.Open('image.tif')\n",
    "# loop through each band\n",
    "\n",
    "path = \n",
    "\n",
    "with open(\"converted.raw\", \"wb\") as outfile:\n",
    "    for bi in range(ds.RasterCount):\n",
    "        band = ds.GetRasterBand(bi + 1)\n",
    "        # Read this band into a 2D NumPy array\n",
    "        ar = band.ReadAsArray()\n",
    "        print('Band %d has type %s'%(bi + 1, ar.dtype))   \n",
    "        np.save(outfile, ar.astype('uint16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b847f32f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba21891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
