{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e3b690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from itertools import combinations_with_replacement\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "from scipy.stats import qmc\n",
    "\n",
    "import bo_methods_lib\n",
    "from bo_methods_lib.bo_functions_generic import eval_GP_sparse_grid, LHS_Design, normalize_bounds, gen_theta_set, find_train_doc_path, test_train_split, clean_1D_arrays, norm_unnorm, calc_ei_basic, calc_ei_emulator\n",
    "from bo_methods_lib.normalize import normalize_p_data, normalize_x, normalize_p_true, normalize_p_bounds\n",
    "from bo_methods_lib.CS2_create_data import *\n",
    "from bo_methods_lib.CS2_bo_functions_multi_dim import argmax_multiple\n",
    "# from bo_methods_lib.CS1_create_data import *\n",
    "# from bo_methods_lib.test_CS1_create_data import *\n",
    "import itertools\n",
    "from itertools import combinations_with_replacement\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "\n",
    "# import matplotlib as mpl\n",
    "# mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f38fcf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "argmax= np.array([0,1,2,3])\n",
    "train_p = np.array([[-1.5, -1.5], [-1.6, -1.6], [-1.7, -1.7], [-1.8, -1.8]])\n",
    "theta_set = np.array([[1.5, 1.5], [1.6, 1.6], [1.7, 1.7], [1.8, 1.8]])\n",
    "print(argmax_multiple(argmax, train_p, theta_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cded141b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]]\n",
      "-0.046258395230017964\n"
     ]
    }
   ],
   "source": [
    "# bounds = np.array([[-1,-1], [1,1]])\n",
    "bounds = None\n",
    "Theta = np.linspace(0,1,2)\n",
    "#Generate the equivalent of all meshgrid points\n",
    "df = pd.DataFrame(list(itertools.product(Theta, repeat=2)))\n",
    "df2 = df.drop_duplicates()\n",
    "theta_set = df2.to_numpy()\n",
    "#Normalize to bounds \n",
    "if bounds is not None:\n",
    "    for i in range(theta_set.shape[1]):\n",
    "        print(theta_set[:,i].T, bounds[:,i])\n",
    "        theta_set[:,i] = normalize_bounds(theta_set[:,i], newRange = (bounds[:,i]))   #, newRange = (bounds[:,i])\n",
    "print(theta_set)\n",
    "\n",
    "def ei_approx_ln_term(epsilon, error_best, pred_mean, pred_stdev, y_target, ep): \n",
    "    \"\"\" \n",
    "    Calculates the integrand of expected improvement of the emulator approach using the log version\n",
    "    Parameters\n",
    "    ----------\n",
    "        epsilon: The random variable. This is the variable that is integrated w.r.t\n",
    "        error_best: float, the best predicted error encountered\n",
    "        pred_mean: ndarray, model mean\n",
    "        pred_stdev: ndarray, model stdev\n",
    "        y_target: ndarray, the expected value of the function from data or other source\n",
    "        ep: float, the numerical bias towards exploration, zero is the default\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        ei_term_2_integral: ndarray, the expected improvement for term 2 of the GP model for method 2B\n",
    "    \"\"\"\n",
    "#     EI = ( (error_best - ep) - np.log( (y_target - pred_mean - pred_stdev*epsilon)**2 ) )*norm.pdf(epsilon)\n",
    "\n",
    "    ei_term_2_integral = np.log( abs((y_target - pred_mean - pred_stdev*epsilon)) )*norm.pdf(epsilon)\n",
    "#     ei_term_2_integral = np.log( (y_target - pred_mean - pred_stdev*epsilon)**2 )*norm.pdf(epsilon)\n",
    "    return ei_term_2_integral\n",
    "\n",
    "print(ei_approx_ln_term(0.1, 0.1, 1, 0.1, 0.12, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5b42f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "<class 'numpy.ndarray'> float32\n",
      "torch.Size([3])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[2],[2]])\n",
    "print(X.shape)\n",
    "\n",
    "F = np.array([2.2, 3.3]).astype('float32')\n",
    "print(type(F), F.dtype)\n",
    "\n",
    "H = [1,2,3]\n",
    "print(torch.tensor(H).shape)\n",
    "H = torch.tensor(H)\n",
    "print(np.min(np.array(H)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5228515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating actual SSE Values\n",
    "norm = True\n",
    "bounds_x_scl, scaler_x = normalize_x(bounds_x, None, norm, scaler = None)\n",
    "bounds_p_scl, scaler_theta = normalize_p_bounds(bounds_p, norm)\n",
    "Xexp_scl = normalize_x(Xexp, None, norm, scaler = scaler_x)[0]\n",
    "print(Xexp_scl)\n",
    "Xexxp = Xexp_scl[0]\n",
    "print(Xexp[0])\n",
    "print(normalize_x(clean_1D_arrays(Xexxp, param_clean = True), None, False, scaler_x))\n",
    "\n",
    "Guess_0 = np.array([0.61032438,0.43428296,0.72599213,0.66055616,0.52820981,0.95271155,0.94534902,0.87020441])\n",
    "Guess_0 = normalize_p_true(Guess_0, scaler_theta, norm = False)\n",
    "Guess_1 = np.array([0,1,0.370735521, 9.23236422E-04,0.166088842, 0.169900862,1,0.024381367])\n",
    "Guess_1 = normalize_p_true(Guess_1, scaler_theta, norm = False)\n",
    "Guess_2 = Theta_True\n",
    "\n",
    "\n",
    "\n",
    "SSE_guess_0 = create_sse_data(Guess_0, Xexp, Yexp, Constants, obj = \"obj\", skip_param_types = 1)\n",
    "# GP_guess_0 = 1236476096410.422\n",
    "GP_guess_0 = 74323.09891172957\n",
    "SSE_guess_1 = create_sse_data(Guess_1, Xexp, Yexp, Constants, obj = \"obj\", skip_param_types = 1)\n",
    "GP_guess_1 =-945327999181.6357\n",
    "SSE_guess_2 = create_sse_data(Guess_2, Xexp, Yexp, Constants, obj = \"obj\", skip_param_types = 1)\n",
    "print(Guess_0, Guess_1)\n",
    "print(\"True Params \\n\", Theta_True)\n",
    "print(\"True SSE \\n\", float(SSE_guess_2))\n",
    "print(\"Guess 1 \\n\", np.round(Guess_0,3))\n",
    "print(\"True SSE Guess 1 \\n\", float(SSE_guess_0), \"\\n GP Guess 1 \\n\", GP_guess_0, \"\\n difference %\", abs((GP_guess_0 - float(SSE_guess_0))/float(SSE_guess_0)))\n",
    "# print(\"Guess 2 \\n\", np.round(Guess_1,3))\n",
    "# print(\"True SSE Guess 2 \\n\", float(SSE_guess_1), \"\\n GP Guess 2 \\n\", GP_guess_1, \"\\n difference %\", abs((GP_guess_1 - float(SSE_guess_1))/float(SSE_guess_1)))\n",
    "# print(SSE_guess_2, GP_guess_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad50c497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2,)\n",
      "(5, 1)\n",
      "(400, 2)\n",
      "(100, 5)\n",
      "(20, 3)\n",
      "(80, 3)\n"
     ]
    }
   ],
   "source": [
    "CS = 1\n",
    "\n",
    "Constants = np.array([[-200,-100,-170,15],\n",
    "                      [-1,-1,-6.5,0.7],\n",
    "                      [0,0,11,0.6],\n",
    "                      [-10,-10,-6.5,0.7],\n",
    "                      [1,0,-0.5,-1],\n",
    "                      [0,0.5,1.5,1]])\n",
    "if CS == 2.2:\n",
    "    skip_param_types = 1 #This is what changes for subpoint\n",
    "    Theta_True = Constants[skip_param_types:skip_param_types+2].flatten()\n",
    "    param_dict = {0 : 'a_1', 1 : 'a_2', 2 : 'a_3', 3 : 'a_4',\n",
    "                  4 : 'b_1', 5 : 'b_2', 6 : 'b_3', 7 : 'b_4'}\n",
    "    exp_d = 2\n",
    "    n = 15 #Number of experimental data points to use\n",
    "    bounds_p = np.array([[-2, -2, -10, -2, -2, -2,  5, -2],\n",
    "                       [ 2,  2,   0,  2,  2,  2, 15,  2]])\n",
    "    bounds_x = np.array([[-1.5,-0.5],[1,2]])\n",
    "    LHS = True\n",
    "\n",
    "else:\n",
    "    Constants = Theta_True = np.array([1,-1])\n",
    "    skip_param_types = 0\n",
    "    param_dict = {0 : '\\\\theta_1', 1 : '\\\\theta_2'}\n",
    "    exp_d = 1\n",
    "    bounds_x = np.array([[-2],[2]])\n",
    "    bounds_p = np.array([[-2,-2],\n",
    "                     [2 , 2]])\n",
    "    LHS = False\n",
    "    n = 5\n",
    "\n",
    "print(Constants.shape)\n",
    "print(Theta_True.shape)\n",
    "\n",
    "p = 20\n",
    "d = len(Theta_True)\n",
    "\n",
    "obj = np.array([\"obj\"])\n",
    "emulator = np.array([True])\n",
    "\n",
    "if emulator == True:\n",
    "    t = 20*n\n",
    "else:\n",
    "    t = 20\n",
    "\n",
    "# #Pull Experimental data from CSV\n",
    "exp_data_doc = 'Input_CSVs/Exp_Data/d='+str(exp_d)+'/n='+str(n)+'.csv'\n",
    "exp_data = np.array(pd.read_csv(exp_data_doc, header=0,sep=\",\"))\n",
    "Xexp = exp_data[:,1:exp_d+1]\n",
    "Yexp = exp_data[:,-1]\n",
    "Xexp = clean_1D_arrays(Xexp)\n",
    "\n",
    "m = Xexp.shape[1]\n",
    "print(Xexp.shape)\n",
    "\n",
    "theta_mesh = gen_theta_set(LHS = LHS, n_points = p, dimensions = d, bounds = bounds_p)\n",
    "print(theta_mesh.shape)\n",
    "theta_set = theta_mesh\n",
    "\n",
    "all_data_doc = find_train_doc_path(emulator, obj, d, t)\n",
    "all_data = np.array(pd.read_csv(all_data_doc, header=0,sep=\",\")) \n",
    "print(all_data.shape)\n",
    "\n",
    "train_data, test_data = test_train_split(all_data, sep_fact = 0.2, shuffle_seed=9)\n",
    "\n",
    "if CS == 1 and emulator == False:\n",
    "    train_p, test_p = train_data[:,1:-2], test_data[:,1:-2]\n",
    "else:\n",
    "    train_p, test_p = train_data[:,1:-1], test_data[:,1:-1]\n",
    "# print(train_p)\n",
    "print(train_p.shape)\n",
    "print(test_p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4736bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_unnorm(X, norm = True, scaler = None):\n",
    "    \"\"\"\n",
    "    Normalizes and unnormalizes data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "        X: ndarray, The array containing the data to be normalized or unnormalized\n",
    "        norm: bool, default True: Determines whether the data is normalized to 0 and 1 or from 0 and 1 (False)\n",
    "        scaler: MinMaxScaler(), default None, Must be provided when moving from normalized to unnormalized values\n",
    "    Returns:\n",
    "    --------\n",
    "        X: ndarray, New scaled (norm = True) or unscaled (norm = False) values\n",
    "        scaler: MinMaxScaler(), The scaler used in the normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    #Determines whether the original data is a tensor or ndarray\n",
    "#     print(X[0:3])\n",
    "    tensor_val = False\n",
    "    if torch.is_tensor(X) == True:\n",
    "        tensor_val = True\n",
    "    #Normalize or unnormalize data\n",
    "    if norm == True:\n",
    "        if scaler is None:\n",
    "            scaler =  MinMaxScaler()\n",
    "            scaler.fit(X)\n",
    "#         print(\"Transforming without scaler\")\n",
    "        X = scaler.transform(X)\n",
    "    else:\n",
    "        assert scaler is not None, \"Scaler must exist to scale back to normal values\"\n",
    "        X = scaler.inverse_transform(X)\n",
    "    \n",
    "    #Make scaled data a tensor if it went into the function as a tensor\n",
    "    if tensor_val == True and torch.is_tensor(X) == False:\n",
    "        X = torch.from_numpy(X)\n",
    "#     print(torch.is_tensor(X)) \n",
    "    return X, scaler\n",
    "\n",
    "def normalize_x(X_val, train_p_x = None, norm = True, scaler = None):\n",
    "    \"\"\"\n",
    "    Normalizes or unnormalizes x data from training/testing data and experimental data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        X_val: ndarray, experimental x values (Xexp) or Xexp bounds\n",
    "        train_p_x: ndarray or None, x_values to normalize from train/test data or Xexp when standard approaches are used. Default None\n",
    "        norm: bool, whether the value will be normalized to 0 and 1 (True) or from 0 and 1 (False). Default True\n",
    "        scaler: None or MinMaxScaler(), used to un-normalize data or normalize data based on another sets normalization\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        x_scl: ndarray, rescaled values of x\n",
    "        scaler_x: MinMaxScaler(), scaler used to obtain these values\n",
    "    \"\"\"\n",
    "    #Change 1D array to 2S with shape (len(X),1)\n",
    "    X_val = clean_1D_arrays(X_val)\n",
    "       #Changes train_p values if they exist \n",
    "    if train_p_x is not None:\n",
    "        train_p_x = clean_1D_arrays(train_p_x)\n",
    "        #If scaling train_p_x data, scale x data using x training data\n",
    "        x_scl, scaler_x = norm_unnorm(train_p_x, norm, scaler)\n",
    "        \n",
    "    else:\n",
    "        #Scale x data using experimental x data\n",
    "        x_scl, scaler_x = norm_unnorm(X_val, norm, scaler)\n",
    "#     print(x_scl)\n",
    "    return x_scl, scaler_x\n",
    "\n",
    "def normalize_p_bounds(param_vals_data, norm = True, scaler = None):\n",
    "    \"\"\"\n",
    "    Normalizes or unnormalizes parameter data from training/testing data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        param_vals_data: ndarray, parameter values to normalize from training/testing data\n",
    "        m: int, dimensionality of x data\n",
    "        emulator: bool, whether GP is emulating fxn or error\n",
    "        norm: bool, whether the value will be normalized to 0 and 1 (True) or from 0 and 1 (False). Default True\n",
    "        scaler: None or MinMaxScaler(), used to un-normalize data or normalize data based on another sets normalization\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        param_data_scl: ndarray, rescaled values of x\n",
    "        scaler_theta: MinMaxScaler(), scaler used to obtain these values\n",
    "    \"\"\"\n",
    "    #Overwite scaled/normal values with normal/scaled values for bounds\n",
    "    param_data_scl, scaler_theta = norm_unnorm(param_vals_data, norm, scaler)\n",
    "    \n",
    "    return param_data_scl, scaler_theta\n",
    "\n",
    "def normalize_p_data(param_vals_data, m, emulator, norm = True, scaler = None):\n",
    "    \"\"\"\n",
    "    Normalizes or unnormalizes parameter data from training/testing data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        param_vals_data: ndarray, parameter values to normalize from training/testing data\n",
    "        m: int, dimensionality of x data\n",
    "        emulator: bool, whether GP is emulating fxn or error\n",
    "        norm: bool, whether the value will be normalized to 0 and 1 (True) or from 0 and 1 (False). Default True\n",
    "        scaler: None or MinMaxScaler(), used to un-normalize data or normalize data based on another sets normalization\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        param_data_scl: ndarray, rescaled values of x\n",
    "        scaler_theta: MinMaxScaler(), scaler used to obtain these values\n",
    "    \"\"\"\n",
    "    if emulator == True:\n",
    "        if norm == True:\n",
    "            #If using emulator approach and normalizing data, overwrite normal parameter values with scaled values\n",
    "            param_data_scl, scaler_theta = norm_unnorm(param_vals_data[:,0:-m], norm, scaler)\n",
    "        else:\n",
    "            #If using emulator approach and un-normalizing data, overwrite scaled values with normal values\n",
    "            param_data_scl, scaler_theta = norm_unnorm(param_vals_data, norm, scaler)\n",
    "    else:\n",
    "         #If using standard approach overwrite scaled/normal values with normal/scaled values\n",
    "        param_data_scl, scaler_theta = norm_unnorm(param_vals_data, norm, scaler)\n",
    "    return param_data_scl\n",
    "\n",
    "def normalize_p_set(p_set, scaler_theta, norm = True):\n",
    "    \"\"\"\n",
    "    Normalizes or unnormalizes parameter data for theta_set\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        param_vals_data: ndarray, parameter values to normalize from training/testing data\n",
    "        scaler_theta: None or MinMaxScaler(), used to (un)normalize data based on another sets normalization\n",
    "        norm: bool, whether the value will be normalized to 0 and 1 (True) or from 0 and 1 (False). Default True\n",
    "           \n",
    "    Returns\n",
    "    -------\n",
    "        p_set_scl: ndarray, rescaled values of x\n",
    "    \"\"\"\n",
    "    #Normalize or unnormalize set data\n",
    "    p_set_scl, scaler_theta = norm_unnorm(p_set, norm, scaler = scaler_theta)\n",
    "    return p_set_scl\n",
    "\n",
    "def normalize_p_true(p_true, scaler_theta, norm= True):\n",
    "    \"\"\"\n",
    "    Normalizes or unnormalizes parameter data for theta_true\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        p_true: ndarray, True parameter values\n",
    "        scaler_theta: None or MinMaxScaler(), used to (un)normalize data based on another sets normalization\n",
    "        norm: bool, whether the value will be normalized to 0 and 1 (True) or from 0 and 1 (False). Default True\n",
    "           \n",
    "    Returns\n",
    "    -------\n",
    "        theta_true_scl.flatten(): ndarray, rescaled values of theta_true flattened to correct dimensions\n",
    "    \"\"\"    \n",
    "    #Normalize or un normalize a 2D shape (1, len(theta_true) array and flatten\n",
    "    theta_true_scl, scaler_theta = norm_unnorm(clean_1D_arrays(p_true, param_clean = True), \n",
    "                                               norm, scaler_theta)\n",
    "    return theta_true_scl.flatten()\n",
    "\n",
    "def normalize_constants(Constants, p_true, scaler_theta, skip_params, CS, norm = True, scaler_C_before = None, \n",
    "                        scaler_C_after = None):\n",
    "    \"\"\"\n",
    "    Normalizes or unnormalizes data for constants\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        Constants: ndarray, ndarray, True values of Muller potential constants OR p_true (CS1 only)\n",
    "        p_true: ndarray, True parameter values\n",
    "        scaler_theta: None or MinMaxScaler(), used to (un)normalize data based on another sets normalization\n",
    "        skip_params: int, number of sets of parameters in Constants to skip before reaching the first iterable parameter row\n",
    "        CS: float, case study label \n",
    "        norm: bool, whether the value will be normalized to 0 and 1 (True) or from 0 and 1 (False). Default True\n",
    "        scaler_C_before: None or MinMaxScaler(), used to un-normalize normalized constansts before theta constants\n",
    "        scaler_C_after: None or MinMaxScaler(), used to un-normalize normalized constansts after theta constants\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        x_scl: ndarray, rescaled values of x\n",
    "        scaler_x: MinMaxScaler(), scaler used to obtain these values\n",
    "    \"\"\"   \n",
    "    #Determine number of parameter types and the length of each: Ex, Muller Case study has 6, and 2D case study has 1\n",
    "    num_param_types = clean_1D_arrays(Constants).shape[1] #4 - Represents how many indecies are in each param type A, a, b, c, x0, y0\n",
    "    len_param_type = int(len(p_true)/num_param_types)\n",
    "    \n",
    "    #For the case study, the constants are identical to theta_True and we scale them as such\n",
    "    if CS == 1:\n",
    "        Constants_scl = normalize_p_true(p_true, scaler_theta, norm)\n",
    "    #For the Muller potential case studies, we need to normalize constants in chunks\n",
    "    else:\n",
    "        #Define constants before, after, and representing theta_true and normalize them by splitting the constants array is 3 parts\n",
    "#         print(Constants)\n",
    "        Constants_before, Constants_theta, Constants_after = np.split(Constants, [skip_params,len_param_type+1])\n",
    "#         print(Constants_before.shape, Constants_theta.shape, Constants_after.shape)\n",
    "        Constants_before_scl, scaler_C_before = norm_unnorm(Constants_before.T, norm, scaler_C_before)\n",
    "        \n",
    "        Constants_theta_scl, scaler_theta =  norm_unnorm(clean_1D_arrays(Constants_theta.flatten(), param_clean = True),\n",
    "                                                 norm, scaler_theta)\n",
    "        Constants_theta_scl = Constants_theta_scl.reshape((len_param_type,num_param_types)) #(2,4) for 2.2\n",
    "        \n",
    "        Constants_after_scl, scaler_C_after = norm_unnorm(Constants_after.T, norm, scaler_C_after)\n",
    "        \n",
    "        #Restack scaled constants and return their value and necessary scalers\n",
    "        Constants_scl = np.vstack((Constants_before_scl.T, Constants_theta_scl, Constants_after_scl.T))\n",
    "    return Constants_scl, scaler_C_before, scaler_C_after      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c7fc5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1.]\n"
     ]
    }
   ],
   "source": [
    "train_p_scl = train_p.copy()\n",
    "train_p_unscl = train_p.copy()\n",
    "norm = True\n",
    "#X values\n",
    "bounds_x_scl, scaler_x = normalize_x(bounds_x, None, norm, scaler = None)\n",
    "train_p_scl[:,-m:] = normalize_x(Xexp, train_p[:,-m:], norm, scaler_x)[0]\n",
    "Xexp_scl = normalize_x(Xexp, None, norm, scaler = scaler_x)[0]\n",
    "# print(Xexp_scl, train_p_scl[:,-m:], bounds_x_scl)\n",
    "# print(Xexp_scl)\n",
    "\n",
    "#Theta Values\n",
    "bounds_p_scl, scaler_theta = normalize_p_bounds(bounds_p, norm)\n",
    "train_p_scl[:,0:-m] = normalize_p_data(train_p, m, emulator, norm, scaler_theta)\n",
    "theta_set_scl = normalize_p_set(theta_set, scaler_theta, norm)\n",
    "Theta_True_scl = normalize_p_true(Theta_True, scaler_theta, norm)\n",
    "# print(train_p_scl[0:5], train_p[0:5])\n",
    "# print(bounds_p_scl, train_p_scl[0:5,0:-m], train_p[0:5,0:-m], theta_set_scl[0:5], theta_set[0:5], Theta_True_scl )\n",
    "\n",
    "# train_p_unscl = train_p_scl.copy()\n",
    "# norm = False\n",
    "# #X values\n",
    "# bounds_x_unscl, scaler_x = normalize_x(bounds_x_scl, None, norm, scaler = scaler_x)\n",
    "# print(train_p_scl[0:5])\n",
    "# train_p_unscl[:,-m:] = normalize_x(Xexp, train_p_scl[:,-m:], norm, scaler_x)[0]\n",
    "# print(train_p_scl[0:5])\n",
    "# Xexp_unscl = normalize_x(Xexp, None, norm, scaler = scaler_x)[0]\n",
    "\n",
    "#Theta values\n",
    "# bounds_p_unscl = normalize_p_bounds(bounds_p, norm, scaler_theta)[0]\n",
    "# # print(train_p_scl[:,0:-m])\n",
    "# train_p_unscl[:,0:-m] = normalize_p_data(train_p_scl[:,0:-m], m, emulator, norm, scaler_theta)\n",
    "# theta_set_unscl = normalize_p_set(theta_set_scl, scaler_theta, norm)\n",
    "# Theta_True_unscl = normalize_p_true(Theta_True_scl, scaler_theta, norm)\n",
    "# print(np.allclose(train_p_unscl, train_p, rtol=1e-10))\n",
    "# print(np.allclose([2,2], [2,2.0000001], rtol=1e-7))\n",
    "# print(train_p_unscl[2:4] , train_p[2:4])\n",
    "\n",
    "\n",
    "#Test Constants:\n",
    "Constants_scl, scaler_C_before, scaler_C_after  = normalize_constants(Theta_True, Theta_True, scaler_theta, \n",
    "                                                                      skip_param_types, CS, norm = True)\n",
    "# print(Constants_scl, Theta_True_scl)\n",
    "Constants_reg = normalize_constants(Constants_scl, Theta_True_scl, scaler_theta, skip_param_types, CS, False, scaler_C_before, \n",
    "                        scaler_C_after)[0]\n",
    "\n",
    "print(Constants_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf3839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "random.seed(10)\n",
    "X = np.random.randint(-10,10, size = (3,3))\n",
    "Y = np.random.randint(-10,10, size = (3,3))\n",
    "print(X)\n",
    "# print(Y)\n",
    "# from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# y_true  = []\n",
    "# y_pred = []\n",
    "\n",
    "# print(loo.get_n_splits(X))\n",
    "# for train_index, test_index in loo.split(X):\n",
    "#     df_test = X[test_index]\n",
    "#     df_train = X[train_index]\n",
    "#     print(df_train, df_test)\n",
    "    \n",
    "\n",
    "# scaler =  MinMaxScaler()\n",
    "# scaler.fit(X)\n",
    "# X = scaler.transform(X)\n",
    "# print(X)\n",
    "# X = scaler.inverse_transform(X)\n",
    "# print(X)\n",
    "\n",
    "from bo_functions_generic import norm_unnorm\n",
    "norm = True\n",
    "X_scl, scaler_X = norm_unnorm(X, norm = norm)\n",
    "Y_scl, scaler_Y = norm_unnorm(Y, norm = norm)\n",
    "\n",
    "print(X_scl, scaler_X)\n",
    "# print(Y, scaler_Y)\n",
    "\n",
    "X2 = np.array([[0, 1, 0.33],\n",
    "               [0.25 , 0.75, 0.66]])\n",
    "\n",
    "X3 = np.array([[2, 5, -2],\n",
    "               [-5 , 4, 0]])\n",
    "print(X3)\n",
    "X3, scaler_X = norm_unnorm(X3, norm = True, scaler = scaler_X)\n",
    "print(X3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474bc899",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xexp_bounds = np.array([[-1.5, -0.5],\n",
    "                        [1 , 2]])\n",
    "LHS = gen_theta_set(LHS = False, n_points = 20, dimensions = 2, bounds = Xexp_bounds)\n",
    "# print(LHS)\n",
    "# print(LHS.reshape((20,20,-1)).T)\n",
    "\n",
    "\n",
    "X1 = np.linspace(-1.5,1,20)\n",
    "X2 = np.linspace(-0.5,2,20)\n",
    "X_mesh = np.meshgrid(X1,X2)\n",
    "print(np.array(X_mesh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a211af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://realpython.com/python-timer/\n",
    "import time\n",
    "start = 3678393\n",
    "res = datetime.timedelta(seconds =start)\n",
    "print(res)\n",
    "\n",
    "time_tot = 0\n",
    "x = 0\n",
    "for i in range(89900):\n",
    "    start2 = time.time()\n",
    "    x += 1+2\n",
    "    end = time.time()\n",
    "\n",
    "    time_tot+= (end - start2)\n",
    "\n",
    "\n",
    "print(time_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f742f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[4,3,0,0],\n",
    "              [4,0,0,3]])\n",
    "print(A[np.nonzero(A)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e064a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.zeros((3,3))\n",
    "B = pd.DataFrame(B)\n",
    "type(B)\n",
    "isinstance(B, pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2c4e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(9)\n",
    "Array_1 = np.random.rand(3,2)\n",
    "Array_2 = np.random.rand(3,2)\n",
    "\n",
    "print(Array_1)\n",
    "print(Array_2)\n",
    "\n",
    "ds = gdal.Open('image.tif')\n",
    "# loop through each band\n",
    "\n",
    "path = \n",
    "\n",
    "with open(\"converted.raw\", \"wb\") as outfile:\n",
    "    for bi in range(ds.RasterCount):\n",
    "        band = ds.GetRasterBand(bi + 1)\n",
    "        # Read this band into a 2D NumPy array\n",
    "        ar = band.ReadAsArray()\n",
    "        print('Band %d has type %s'%(bi + 1, ar.dtype))   \n",
    "        np.save(outfile, ar.astype('uint16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b847f32f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba21891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
