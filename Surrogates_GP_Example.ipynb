{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2da72de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import math\n",
    "import itertools\n",
    "from scipy.stats import norm\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.stats import qmc\n",
    "import scipy.special\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, WhiteKernel, ConstantKernel\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae61afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LHS_Design(num_points, dimensions, seed):\n",
    "        \"\"\"\n",
    "        Design LHS Samples\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_points: int, number of points in LHS, should be greater than # of dimensions\n",
    "        ndim: Number of dimensions\n",
    "        seed: int, seed of random generation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        lhs_data: ndarray, array of LHS sampling points with length (num_points) \n",
    "        \"\"\"\n",
    "        #Define sampler\n",
    "        sampler = qmc.LatinHypercube(d=dimensions, seed = seed)\n",
    "        lhs_data = sampler.random(n=num_points)\n",
    "\n",
    "        #Generate LHS data given bounds\n",
    "        lhs_data = qmc.scale(lhs_data)\n",
    "\n",
    "        return lhs_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45711339",
   "metadata": {},
   "source": [
    "# 5.1 Gaussian Process Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be48b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set number of experimental data/ set rng\n",
    "np.random.seed(0) #Set seed\n",
    "rng = np.random.default_rng() # sets up a \"random\" number generator\n",
    "ndim= 1\n",
    "nsamples = 100\n",
    "eps = 1e-7\n",
    "# eps = 0\n",
    "\n",
    "#Define X data\n",
    "# Xexp = np.random.rand(nsamples)*10 \n",
    "Xexp = np.linspace(0,100,nsamples).reshape(-1,ndim)\n",
    "# Yexp = np.sin(Xexp)\n",
    "# print(Xexp.shape)\n",
    "\n",
    "#Create a function for euclidean distance between 2 x-vector values\n",
    "def kernel(x1,x2):\n",
    "    vector_diff = x1-x2\n",
    "    distance = np.exp(-np.linalg.norm(vector_diff)**2)\n",
    "    return distance\n",
    "\n",
    "#Define Covariance Matrix\n",
    "cov = np.zeros((nsamples,nsamples))\n",
    "#Loop over each sample\n",
    "for i in range(nsamples):\n",
    "    #Match with each sample\n",
    "    for j in range(nsamples):\n",
    "        x1 = Xexp[i]\n",
    "        x2 = Xexp[j]\n",
    "        cov[i,j] = kernel(x1,x2)\n",
    "\n",
    "#Alternatively,\n",
    "def covkernel(xi,xj,l=1.0):\n",
    "    dij = cdist(xi,xj,metric='euclidean') # returns matrix of pairwise distances\n",
    "    return np.exp(-1*dij**2/l)\n",
    "\n",
    "cov2 = covkernel(Xexp,Xexp)\n",
    "# print(cov == cov2)\n",
    "\n",
    "#Add jitter\n",
    "cov += eps * np.eye(cov.shape[0])\n",
    "\n",
    "#Set mean function (in this case 0)\n",
    "mean=np.zeros(nsamples) # zero mean\n",
    "# mean = np.ones(Xexp)*2 #Linear mean example\n",
    "\n",
    "#Generate multivariate normal distribution\n",
    "Y = rng.multivariate_normal(mean,cov)\n",
    "\n",
    "#Plot data\n",
    "plt.figure(figsize = (6.4,4))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(direction=\"in\",top=True, right=True)\n",
    "plt.locator_params(axis='y', nbins=5)\n",
    "plt.locator_params(axis='x', nbins=5)\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "#         plt.gca().axes.xaxis.set_ticklabels([]) # remove tick labels\n",
    "#         plt.gca().axes.yaxis.set_ticklabels([])\n",
    "\n",
    "#plot training data, testing data, and true values\n",
    "plt.plot(Xexp, Y, label=\"Random Func\")\n",
    "\n",
    "#Set plot details\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0)\n",
    "#         plt.legend(loc = \"best\")\n",
    "x_label = \"X-Values\"\n",
    "y_label = \"y-Values\"\n",
    "plt.xlabel(x_label, fontsize=16, fontweight='bold')\n",
    "plt.ylabel(y_label, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9568d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set number of experimental data/ set rng\n",
    "np.random.seed(0) #Set seed\n",
    "rng = np.random.default_rng() # sets up a \"random\" number generator\n",
    "ndim= 1\n",
    "nsamples = 100\n",
    "eps = 1e-7\n",
    "# eps = 0\n",
    "\n",
    "#Define X data\n",
    "# Xexp = np.random.rand(nsamples)*10 \n",
    "Xexp = np.linspace(0,10,nsamples).reshape(-1,ndim)\n",
    "# Yexp = np.sin(Xexp)\n",
    "# print(Xexp.shape)\n",
    "\n",
    "#Create a function for euclidean distance between 2 x-vector values\n",
    "def kernel(x1,x2):\n",
    "    vector_diff = x1-x2\n",
    "    distance = np.exp(-np.linalg.norm(vector_diff)**2)\n",
    "    return distance\n",
    "\n",
    "#Define Covariance Matrix\n",
    "cov = np.zeros((nsamples,nsamples))\n",
    "#Loop over each sample\n",
    "for i in range(nsamples):\n",
    "    #Match with each sample\n",
    "    for j in range(nsamples):\n",
    "        x1 = Xexp[i]\n",
    "        x2 = Xexp[j]\n",
    "        cov[i,j] = kernel(x1,x2)\n",
    "\n",
    "#Alternatively,\n",
    "def covkernel(xi,xj,l=1.0):\n",
    "    dij = cdist(xi,xj,metric='euclidean') # returns matrix of pairwise distances\n",
    "    return np.exp(-1*dij**2/l)\n",
    "\n",
    "cov2 = covkernel(Xexp,Xexp)\n",
    "# print(cov == cov2)\n",
    "\n",
    "#Add jitter\n",
    "cov += eps * np.eye(cov.shape[0])\n",
    "\n",
    "#Set mean function (in this case 0)\n",
    "mean=np.zeros(nsamples) # zero mean\n",
    "# mean = np.ones(Xexp)*2 #Linear mean example\n",
    "\n",
    "#Generate multivariate normal distribution\n",
    "Y = rng.multivariate_normal(mean,cov, size = 3)\n",
    "\n",
    "#Plot data\n",
    "plt.figure(figsize = (6.4,4))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(direction=\"in\",top=True, right=True)\n",
    "plt.locator_params(axis='y', nbins=5)\n",
    "plt.locator_params(axis='x', nbins=5)\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "#         plt.gca().axes.xaxis.set_ticklabels([]) # remove tick labels\n",
    "#         plt.gca().axes.yaxis.set_ticklabels([])\n",
    "\n",
    "#plot training data, testing data, and true values\n",
    "plt.plot(Xexp, Y.T, label=\"Random Func\")\n",
    "\n",
    "#Set plot details\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0)\n",
    "#         plt.legend(loc = \"best\")\n",
    "x_label = \"X-Values\"\n",
    "y_label = \"y-Values\"\n",
    "plt.xlabel(x_label, fontsize=16, fontweight='bold')\n",
    "plt.ylabel(y_label, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e98c55",
   "metadata": {},
   "source": [
    "## 5.1.1 Gaussian Process Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d762d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set number of experimental data/ set rng\n",
    "np.random.seed(10) #Set seed\n",
    "rng = np.random.default_rng() # sets up a \"random\" number generator\n",
    "ndim= 1\n",
    "nsamples = 8\n",
    "nsamples_test = 100\n",
    "eps = 1e-7\n",
    "\n",
    "#Define X data\n",
    "X = np.linspace(0,2*np.pi,nsamples)[:,np.newaxis]\n",
    "Y = np.sin(X)\n",
    "\n",
    "#Create a kernel function\n",
    "def covkernel(xi,xj,l=1.0):\n",
    "    # returns matrix of pairwise distances\n",
    "    dij = cdist(xi,xj,metric='euclidean') \n",
    "    return np.exp(-1*dij**2/l)\n",
    "\n",
    "#Define Covariance matrix for Xexp training data and add jitter\n",
    "Sigma = covkernel(X,X,l=1.0)\n",
    "Sigma += eps * np.eye(Sigma.shape[0])\n",
    "\n",
    "#Define Covariance Matrix for testing data set and add jitter\n",
    "XX = np.linspace(-0.5,2*np.pi+0.5,nsamples_test)[:,np.newaxis]\n",
    "SXX = covkernel(XX,XX,l=1.0)\n",
    "SXX += eps * np.eye(SXX.shape[0])\n",
    "\n",
    "#Define covariance between testing and training data and add jitter\n",
    "#Note SX does not have jitter because it is not decomposed and isn't necessarily square\n",
    "SX = covkernel(XX,X,l=1.0)\n",
    "\n",
    "#Calculate posterior mean and variance\n",
    "inv_Sig = np.linalg.inv(Sigma)\n",
    "mup = SX@inv_Sig@Y\n",
    "sigmap = SXX - SX@inv_Sig@SX.T\n",
    "sdp = np.sqrt(np.diag(sigmap))\n",
    "\n",
    "#Generate 100 predictions, YY for XX\n",
    "YY = rng.multivariate_normal(mup[:,0], sigmap, nsamples_test)\n",
    "\n",
    "#Calculate 5th and 95th percentiles for calculations\n",
    "q1 = mup[:,0] + norm.ppf(0.05, loc=0, scale=sdp)\n",
    "q2 = mup[:,0] + norm.ppf(0.95, loc=0, scale=sdp)\n",
    "\n",
    "#Plot Posterior distribution\n",
    "plt.figure(figsize = (7,6))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(direction=\"in\",top=True, right=True)\n",
    "plt.locator_params(axis='y', nbins=7)\n",
    "plt.locator_params(axis='x', nbins=5)\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "plt.plot(XX[:,0], YY.T, color = \"grey\")\n",
    "plt.plot(X[:,0], Y[:,0], marker = 'o',color='k', label = \"Training\", linestyle = \"None\")\n",
    "plt.plot(XX[:,0], np.sin(XX), color = \"blue\", label = \"True Fxn\")\n",
    "plt.plot(XX[:,0], q1,  linestyle='dashed', color = \"red\", label = \"95th Percentile\")\n",
    "plt.plot(XX[:,0], q2,  linestyle='dashed', color = \"red\")\n",
    "plt.plot(XX[:,0], mup[:,0], color = \"black\", label = \"GP Mean\")\n",
    "\n",
    "#Set plot details\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0)\n",
    "x_label = \"X-Values\"\n",
    "y_label = \"y-Values\"\n",
    "plt.xlabel(x_label, fontsize=16, fontweight='bold')\n",
    "plt.ylabel(y_label, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0929a458",
   "metadata": {},
   "source": [
    "## 5.1.2 Higher Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4951ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In higher dimensions\n",
    "#Set number of experimental data/ set rng\n",
    "np.random.seed() #Set seed\n",
    "rng = np.random.default_rng() # sets up a \"random\" number generator\n",
    "ndim = 2\n",
    "nx = 20\n",
    "eps = 1e-14\n",
    "\n",
    "#Define 2D X data\n",
    "x = np.linspace(0,2,nx)\n",
    "X = np.array(list(itertools.product(x, repeat=ndim)))\n",
    "\n",
    "#Create a kernel function\n",
    "def covkernel(xi,xj,l=1.0):\n",
    "     # returns matrix of pairwise distances\n",
    "    dij = cdist(xi,xj,metric='euclidean')\n",
    "    return np.exp(-1*dij**2/l)\n",
    "\n",
    "#Define Covariance matrix for Xexp training data and add jitter\n",
    "Sigma = covkernel(X,X,l=1.0)\n",
    "Sigma += eps * np.eye(Sigma.shape[0])\n",
    "\n",
    "#Set mean function (in this case 0)\n",
    "mean=np.zeros(nx**ndim) # zero mean\n",
    "\n",
    "#Generate multivariate normal distribution\n",
    "Y = rng.multivariate_normal(mean,Sigma)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "# Data for a three-dimensional line\n",
    "ax.contour3D(x, x, Y.reshape(nx,nx), 100, cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef97f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set number of experimental data/ set rng\n",
    "np.random.seed(10) #Set seed\n",
    "rng = np.random.default_rng() # sets up a \"random\" number generator\n",
    "ndim= 2\n",
    "nsamples = 40\n",
    "eps = 1e-14\n",
    "#Generate X data from LHS\n",
    "X = LHS_Design(nsamples, ndim, seed = 10)\n",
    "#Modify X data to correct scale\n",
    "X[:,0] = (X[:,0] - 0.5)*6 +1\n",
    "X[:,1] = (X[:,1] - 0.5)*6 +1\n",
    "#Generate y data\n",
    "y = X[:,0]*np.exp(-X[:,0]**2 -X[:,1]**2)\n",
    "#Generate mesh data\n",
    "xx = np.linspace(-2,4,nsamples)\n",
    "XX_mesh = np.array(np.meshgrid(xx,xx))\n",
    "XX = XX_mesh.reshape((2,nsamples**2)).T\n",
    "#Define Covariance matrix for Xexp training data and add jitter\n",
    "Sigma = covkernel(X,X,l=1.0)\n",
    "Sigma += eps * np.eye(Sigma.shape[0])\n",
    "#Define Covariance Matrix for testing data set and add jitter\n",
    "SXX = covkernel(XX,XX,l=1.0)\n",
    "SXX += eps * np.eye(SXX.shape[0])\n",
    "#Define covariance between testing and training data and add jitter\n",
    "#Note SX does not have jitter because it is not decomposed and isn't necessarily square\n",
    "SX = covkernel(XX,X,l=1.0)\n",
    "#Calculate posterior mean and variance\n",
    "inv_Sig = np.linalg.inv(Sigma)\n",
    "mup = SX@inv_Sig@y\n",
    "sigmap = SXX - SX@inv_Sig@SX.T\n",
    "sdp = np.sqrt(np.diag(sigmap))\n",
    "\n",
    "#Plot\n",
    "X1, X2 = XX_mesh\n",
    "Z1 = mup.reshape(nsamples,nsamples)\n",
    "Z2 = sdp.reshape(nsamples,nsamples)\n",
    "z = [Z1,Z2]\n",
    "tot_lev = [60,60]\n",
    "#Set plot details for mean predictions\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = len(z), figsize = (14,6))\n",
    "ax = axes\n",
    "title = [\"Mean\", \"StDev\"]\n",
    "\n",
    "for i in range(len(z)):      \n",
    "    #Create a colormap and colorbar for each subplot\n",
    "    cs_fig = ax[i].contourf(X1, X2,z[i], levels = 900, cmap = \"jet\")\n",
    "    if np.amax(z[i]) < 1e-1 or np.amax(z[i]) > 1000:\n",
    "        cbar = plt.colorbar(cs_fig, ax = ax[i], format='%.2e')\n",
    "    else:\n",
    "        cbar = plt.colorbar(cs_fig, ax = ax[i], format = '%2.2f')\n",
    "\n",
    "    #Create a line contour for each colormap\n",
    "    cs2_fig = ax[i].contour(cs_fig, levels=cs_fig.levels[::tot_lev[i]], colors='k', alpha=0.7, linestyles='dashed', linewidths=3)\n",
    "    ax[i].clabel(cs2_fig,  levels=cs_fig.levels[::tot_lev[i]][1::2], fontsize=10, inline=1)\n",
    "\n",
    "    #plot training data X values\n",
    "    ax[i].scatter(X[:,0],X[:,1], color=\"green\",s=25, label = \"Training Data\", marker = \"o\")  \n",
    "\n",
    "    #Get legend information\n",
    "    if i == len(z)-1:\n",
    "        handles, labels = ax[i].get_legend_handles_labels()\n",
    "\n",
    "    #Plots axes such that they are scaled the same way (eg. circles look like circles) and name axes\n",
    "    ax[i].axis('scaled')  \n",
    "    ax[i].set_xlabel('$x_1$',weight='bold',fontsize=16)\n",
    "    ax[i].set_ylabel('$x_2$',weight='bold',fontsize=16)\n",
    "\n",
    "    #Plot title and set axis scale\n",
    "    ax[i].set_title(title[i], weight='bold',fontsize=16)\n",
    "    ax[i].set_xlim(left = np.amin(X1), right = np.amax(X1))\n",
    "    ax[i].set_ylim(bottom = np.amin(X2), top = np.amax(X2))      \n",
    "\n",
    "#Plots legend and title\n",
    "plt.tight_layout()\n",
    "fig.legend(handles, labels, loc=\"upper left\")  #bbox_to_anchor=(-0.01, 0.9), borderaxespad=0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23635239",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "# Data for a three-dimensional line\n",
    "ax.plot_wireframe(X1, X2, Z1.T, rstride=2, cstride=4, color = \"black\")\n",
    "ax.view_init(20, -30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eba249",
   "metadata": {},
   "source": [
    "# Section 5.2 GP Hyperparameters\n",
    "## Section 5.2.1 Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e71865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set number of experimental data/ set rng\n",
    "np.random.seed(0) #Set seed\n",
    "# sets up a \"random\" number generator\n",
    "rng = np.random.default_rng() \n",
    "ndim= 1 #Number of dimensions\n",
    "nsamples = 100 #Number of samples\n",
    "eps = 1e-7 #Jitter factor\n",
    "tau_2 = 25 #scale parameter tau^2\n",
    "#Define X data\n",
    "Xexp = np.linspace(0,10,nsamples).reshape(-1,ndim)\n",
    "#Define covariance matrix\n",
    "Sigma = covkernel(Xexp,Xexp,l=1.0)\n",
    "#Add jitter\n",
    "cov += eps * np.eye(cov.shape[0])\n",
    "#Set mean function (in this case 0)\n",
    "mean=np.zeros(nsamples) # zero mean\n",
    "#Generate multivariate normal distribution\n",
    "Y = rng.multivariate_normal(mean,cov*tau_2, size = 10)\n",
    "\n",
    "#Plot data\n",
    "plt.figure(figsize = (6.4,4))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(direction=\"in\",top=True, right=True)\n",
    "plt.locator_params(axis='y', nbins=5)\n",
    "plt.locator_params(axis='x', nbins=5)\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "#         plt.gca().axes.xaxis.set_ticklabels([]) # remove tick labels\n",
    "#         plt.gca().axes.yaxis.set_ticklabels([])\n",
    "\n",
    "#plot training data, testing data, and true values\n",
    "plt.plot(Xexp, Y.T, label=\"Random Func\")\n",
    "\n",
    "#Set plot details\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0)\n",
    "#         plt.legend(loc = \"best\")\n",
    "x_label = \"X-Values\"\n",
    "y_label = \"y-Values\"\n",
    "plt.xlabel(x_label, fontsize=16, fontweight='bold')\n",
    "plt.ylabel(y_label, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b5967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a higher amplitude sine function example\n",
    "#Set number of experimental data/ set rng\n",
    "np.random.seed(0) #Set seed\n",
    "rng = np.random.default_rng() # sets up a \"random\" number generator\n",
    "ndim= 1\n",
    "nsamples = 8\n",
    "eps = 1e-7\n",
    "\n",
    "#Define X data\n",
    "X = np.linspace(0,2*np.pi,nsamples).reshape(-1,ndim)\n",
    "XX = np.linspace(-0.5,2*np.pi+0.5,100).reshape(-1,ndim)\n",
    "Y = 5*np.sin(X)\n",
    "\n",
    "#Define Covariance matrix for Xexp training data and add jitter\n",
    "Sigma = covkernel(X,X,l=1.0)\n",
    "# Sigma += eps * np.eye(Sigma.shape[0])\n",
    "\n",
    "#Define Covariance Matrix for testing data set and add jitter\n",
    "SXX = covkernel(XX,XX,l=1.0)\n",
    "SXX += eps * np.eye(SXX.shape[0])\n",
    "\n",
    "#Define covariance between testing and training data and add jitter\n",
    "#Note SX does not have jitter because it is not decomposed and isn't necessarily square\n",
    "SX = covkernel(XX,X,l=1.0)\n",
    "\n",
    "#Calculate posterior mean and variance\n",
    "inv_Sig = np.linalg.inv(Sigma)\n",
    "mup = SX@inv_Sig@Y\n",
    "sigmap = SXX - SX@inv_Sig@SX.T\n",
    "sdp = np.sqrt(np.diag(sigmap))\n",
    "\n",
    "#Generate 100 predictions, YY for XX\n",
    "YY = rng.multivariate_normal(mup[:,0], sigmap, nsamples_test)\n",
    "\n",
    "#Calculate 5th and 95th percentiles for calculations\n",
    "# sample_mean = np.mean(YY,axis=0)\n",
    "# sample_stdev = np.std(YY, axis =0)\n",
    "# q1   = sample_mean + 1*sample_stdev\n",
    "# q2  = sample_mean - 1*sample_stdev\n",
    "q1 = mup[:,0] + norm.ppf(0.05, loc=0, scale=sdp)\n",
    "q2 = mup[:,0] + norm.ppf(0.95, loc=0, scale=sdp)\n",
    "\n",
    "#Plot Posterior distribution\n",
    "plt.figure(figsize = (7,6))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(direction=\"in\",top=True, right=True)\n",
    "plt.locator_params(axis='y', nbins=7)\n",
    "plt.locator_params(axis='x', nbins=5)\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "plt.plot(XX[:,0], YY.T, color = \"grey\")\n",
    "plt.plot(X[:,0], Y[:,0], marker = 'o',color='k', label = \"Training\", linestyle = \"None\")\n",
    "plt.plot(XX[:,0], 5*np.sin(XX), color = \"blue\", label = \"True Fxn\")\n",
    "plt.plot(XX[:,0], q1,  linestyle='dashed', color = \"red\", label = \"95th Percentile\")\n",
    "plt.plot(XX[:,0], q2,  linestyle='dashed', color = \"red\")\n",
    "plt.plot(XX[:,0], mup[:,0], color = \"black\", label = \"GP Mean\")\n",
    "\n",
    "#Set plot details\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0)\n",
    "x_label = \"X-Values\"\n",
    "y_label = \"y-Values\"\n",
    "plt.xlabel(x_label, fontsize=16, fontweight='bold')\n",
    "plt.ylabel(y_label, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4a50cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add scaling factor to approximation above and repeat\n",
    "#Define covariance matricies which will be scaled by scaling factor tau\n",
    "CX = SX\n",
    "Ci = inv_Sig\n",
    "CXX= SXX\n",
    "tau_2_hat = Y.T@Ci@Y/len(Y)\n",
    "tau_2_hat = tau_2_hat.squeeze()\n",
    "print(\"2*sqrt(tau_2_hat): \", 2*np.sqrt(tau_2_hat))\n",
    "#Calculate posterior mean and variance\n",
    "mup2 = CX@Ci@Y\n",
    "sigmap2 = tau_2_hat*(CXX - CX@Ci@CX.T)\n",
    "sdp2 = np.sqrt(np.diag(sigmap2))\n",
    "#Generate 100 predictions, YY for XX\n",
    "YY = rng.multivariate_normal(mup2[:,0], sigmap2, nsamples_test)\n",
    "#Calculate 5th and 95th percentiles for calculations\n",
    "q1 = mup2[:,0] + norm.ppf(0.05, loc=0, scale=sdp2)\n",
    "q2 = mup2[:,0] + norm.ppf(0.95, loc=0, scale=sdp2)\n",
    "\n",
    "#Plot Posterior distribution\n",
    "plt.figure(figsize = (7,6))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(direction=\"in\",top=True, right=True)\n",
    "plt.locator_params(axis='y', nbins=7)\n",
    "plt.locator_params(axis='x', nbins=5)\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "plt.plot(XX[:,0], YY.T, color = \"grey\")\n",
    "plt.plot(X[:,0], Y[:,0], marker = 'o',color='k', label = \"Training\", linestyle = \"None\")\n",
    "plt.plot(XX[:,0], 5*np.sin(XX), color = \"blue\", label = \"True Fxn\")\n",
    "plt.plot(XX[:,0], q1,  linestyle='dashed', color = \"red\", label = \"95th Percentile\")\n",
    "plt.plot(XX[:,0], q2,  linestyle='dashed', color = \"red\")\n",
    "plt.plot(XX[:,0], mup2[:,0], color = \"black\", label = \"GP Mean\")\n",
    "\n",
    "#Set plot details\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0)\n",
    "x_label = \"X-Values\"\n",
    "y_label = \"y-Values\"\n",
    "plt.xlabel(x_label, fontsize=16, fontweight='bold')\n",
    "plt.ylabel(y_label, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5dc5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to calculate score\n",
    "def score(Y, mu, Sigma, mah=False):\n",
    "    Ymmu = Y - mu\n",
    "    Sigmai = np.linalg.inv(Sigma)\n",
    "    mahdist = Ymmu.T@Sigmai@Ymmu\n",
    "    if mah:\n",
    "        return np.sqrt(mahdist)\n",
    "    else:\n",
    "        sign, logdet = np.linalg.slogdet(Sigma)\n",
    "        return -logdet - mahdist\n",
    "#         return -np.log(np.linalg.det(Sigma)) - mahdist   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9dd2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define scores using scaling factor of 1 and optimized scaling factor\n",
    "Ytrue = 5*np.sin(XX)\n",
    "mah = True\n",
    "scores = np.array([score(Ytrue, mup, sigmap, mah=mah), score(Ytrue, mup2, sigmap2, mah=mah)]).squeeze()\n",
    "df = pd.DataFrame(np.array([scores]), columns = [\"tau2=1\", \"tau2_hat\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14239fd",
   "metadata": {},
   "source": [
    "## 5.2.2 Noise and Nuggets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b766bd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define negative likelihood of nugget g as a function\n",
    "def nlg(g, X, Y):\n",
    "    n = len(Y)\n",
    "    K = covkernel(X,X,l=1.0)\n",
    "    Kjit = K + g * np.eye(K.shape[0])\n",
    "    Ki = np.linalg.inv(Kjit)\n",
    "    sign, logdet = np.linalg.slogdet(Kjit)\n",
    "    ldetK = logdet\n",
    "    ll = -(n/2)*np.log(Y.T@Ki@Y) - (1/2)*ldetK\n",
    "    return -ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ff5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimize g\n",
    "np.random.seed(3) #Set seed\n",
    "rng = np.random.default_rng() # sets up a \"random\" number generator\n",
    "\n",
    "eps = 1e-7 #Jitter factor\n",
    "#Define 2 repeats of X (because we are adding noise)\n",
    "Xnew = np.concatenate((X,X)) \n",
    "n = Xnew.shape[0] #Define number of samples\n",
    "Ynew = 5*np.sin(Xnew) + np.random.normal(size=(n,1)  ,loc = 0, scale = 1) #Define Y data\n",
    "#Set bounds on g between jitter and variance of Y\n",
    "bnds = np.array([eps, np.var(Ynew)]).reshape(1,-1) \n",
    "#optimize g\n",
    "solution = scipy.optimize.minimize(nlg, 1, args = (Xnew,Ynew), method='L-BFGS-B', bounds=bnds, tol=1e-10)\n",
    "print(solution)\n",
    "g = solution.x\n",
    "iterations = solution.nit\n",
    "g = g.flatten()\n",
    "print(\"Value of g: \", g)\n",
    "print(\"# of Iterations: \", iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25ce856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define covariance matrix and optimize scaling factor tau2hat\n",
    "K = covkernel(Xnew,Xnew,l=1.0)\n",
    "K += g * np.eye(K.shape[0])\n",
    "Ki = np.linalg.inv(K)\n",
    "tau2hat = Ynew.T@Ki@Ynew/n\n",
    "tau2hat = tau2hat.flatten()\n",
    "print(\"tau: \", np.sqrt(tau2hat))\n",
    "print(\"sigma: \", np.sqrt(tau2hat*g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f694daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Covariance matricies\n",
    "KX = covkernel(XX,Xnew,l=1.0)\n",
    "KXX = covkernel(XX,XX,l=1.0)\n",
    "KXX += g * np.eye(KXX.shape[0])\n",
    "\n",
    "#Calculate covariance matrix and GP mean\n",
    "mup = KX@Ki@Ynew\n",
    "sigmap = tau2hat*(KXX - KX@Ki@KX.T)\n",
    "sdp = np.sqrt(np.diag(sigmap))\n",
    "\n",
    "#Calculate 5th and 95th percentiles for calculations\n",
    "q1 = mup[:,0] + norm.ppf(0.05, loc=0, scale=sdp)\n",
    "q2 = mup[:,0] + norm.ppf(0.95, loc=0, scale=sdp)\n",
    "\n",
    "#Define covariance matrix that uses eps instead of a nugget to add jitter\n",
    "KXX2 = covkernel(XX,XX,l=1.0)\n",
    "KXX2 += eps * np.eye(KXX.shape[0])\n",
    "sigma_int = tau2hat*(KXX2 - KX@Ki@KX.T)\n",
    "\n",
    "# #Generate 100 predictions, YY for XX using the eps based covariance predictions\n",
    "YY = rng.multivariate_normal(mup[:,0], sigma_int, nsamples_test)\n",
    "\n",
    "# #Plot Posterior distribution\n",
    "plt.figure(figsize = (7,6))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(direction=\"in\",top=True, right=True)\n",
    "plt.locator_params(axis='y', nbins=7)\n",
    "plt.locator_params(axis='x', nbins=5)\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "plt.plot(XX[:,0], YY.T, color = \"grey\")\n",
    "plt.plot(XX[:,0], 5*np.sin(XX), color = \"blue\", label = \"True Fxn\")\n",
    "plt.plot(XX[:,0], q1,  linestyle='dashed', color = \"red\", label = \"95th Percentile\")\n",
    "plt.plot(XX[:,0], q2,  linestyle='dashed', color = \"red\")\n",
    "plt.plot(XX[:,0], mup[:,0], color = \"black\", label = \"GP Mean\")\n",
    "plt.scatter(Xnew[:,0], Ynew[:,0], marker = 'o',color='k', label = \"Training\", linestyle = \"None\", zorder =3)\n",
    "\n",
    "#Set plot details\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0)\n",
    "x_label = \"X-Values\"\n",
    "y_label = \"y-Values\"\n",
    "plt.xlabel(x_label, fontsize=16, fontweight='bold')\n",
    "plt.ylabel(y_label, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03ae4ec",
   "metadata": {},
   "source": [
    "## 5.2.3 Derivative-Based Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20413b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the gradient of the negative likelihood w.r.t nugget g\n",
    "def gnlg(g, X, Y):\n",
    "    n = len(Y)\n",
    "    K = covkernel(X,X,l=1.0)\n",
    "    Kjit = K + g * np.eye(K.shape[0])\n",
    "    Ki = np.linalg.inv(Kjit)\n",
    "    KiY = Ki@Y\n",
    "    dll = (n/2)*KiY.T@KiY/(Y.T@KiY)-(1/2)*np.sum(np.diag(Ki))\n",
    "    return -1*dll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ed7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimize parameter g using the gradient\n",
    "#Note. I'm not sure how to recreate what the example is doing in python. I don't think this is right\n",
    "bnds = np.array([eps, np.var(Ynew)]).reshape(1,-1)\n",
    "solution = scipy.optimize.minimize(nlg, 0.1, args = (Xnew,Ynew), method='L-BFGS-B', jac= gnlg, bounds=bnds, tol=1e-10 )\n",
    "print(solution)\n",
    "g = solution.x\n",
    "g = g.flatten()\n",
    "iterations = solution.nit\n",
    "print(\"Val nugget g: \", g)\n",
    "print(\"# of Iterations: \", iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d46688",
   "metadata": {},
   "source": [
    "## 5.2.4 Lengthscale: rate decay of correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46869538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a kernel function\n",
    "def covkernel(xi,xj,l=1.0):\n",
    "    # returns matrix of pairwise distances\n",
    "    dij = cdist(xi,xj,metric='euclidean') \n",
    "    return np.exp(-(1*dij**2)/l)\n",
    "\n",
    "#Define negative lieklihood function w.r.t g and \\ell\n",
    "def nl(par, X, Y):\n",
    "    ell, g = par\n",
    "    n = len(Y)\n",
    "    K = covkernel(X,X,l=ell)\n",
    "    Kjit = K + g * np.eye(K.shape[0])\n",
    "    Ki = np.linalg.inv(Kjit)\n",
    "    sign, logdet = np.linalg.slogdet(Kjit)\n",
    "    ldetK = logdet\n",
    "    ll = -(n/2)*np.log(Y.T@Ki@Y) - (1/2)*ldetK\n",
    "    return -ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac93701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define LHS sample\n",
    "#Generate X data from LHS\n",
    "nsamples =40\n",
    "ndim =2\n",
    "X = LHS_Design(nsamples, ndim, seed = 10)\n",
    "#Define 2 repeats of X (because we are adding noise)\n",
    "X2 = np.concatenate((X,X)) \n",
    "#Modify X data to correct scale\n",
    "X2[:,0] = (X2[:,0] - 0.5)*6 +1\n",
    "X2[:,1] = (X2[:,1] - 0.5)*6 +1\n",
    "#Generate y data\n",
    "np.random.seed(2) #Set seed\n",
    "noise = np.random.normal(size=(len(X2))  ,loc = 0, scale = 0.01)\n",
    "#Define Y data\n",
    "Y2 = X2[:,0]*np.exp(-X2[:,0]**2 -X2[:,1]**2) + noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c4a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnds = np.array([[eps, 10], #Set bounds on \\ell between jitter and 10\n",
    "                [eps, np.var(Y2)]])#Set bounds on g between jitter and variance of Y\n",
    "#optimize \\ell and g\n",
    "init = np.array([1, 0.1*np.var(Y2)])\n",
    "solution = scipy.optimize.minimize(nl, init, args = (X2,Y2), method='L-BFGS-B', bounds=bnds, tol=1e-10 )\n",
    "print(solution)\n",
    "ell, g = solution.x\n",
    "iterations = solution.nit\n",
    "ell = ell.flatten()\n",
    "g = g.flatten()\n",
    "print(\"Value of g: \", g)\n",
    "print(\"Value of \\ell: \", ell)\n",
    "print(\"# of Iterations: \", iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb54807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define gradient of nl w.r.t g and \\ell\n",
    "def gradnl(par, X, Y):\n",
    "    #Separate parameters\n",
    "    ell, g = par\n",
    "    #Calculate covariance quantities\n",
    "    n = len(Y)\n",
    "    K = covkernel(X,X,l=ell)\n",
    "    Kjit = K + g * np.eye(K.shape[0])\n",
    "    Ki = np.linalg.inv(Kjit)\n",
    "    D = cdist(X,X,metric='euclidean')\n",
    "    dotK = Kjit*D/(ell**2)\n",
    "    KiY = Ki@Y\n",
    "    dlltheta = (n/2)*KiY.T@dotK@KiY/(Y.T@KiY) - (1/2)*np.sum(np.diag(Ki@dotK))\n",
    "    dllg = (n/2) * KiY.T@KiY /(Y.T@KiY) - (1/2)*np.sum(np.diag(Ki))\n",
    "    grad = -np.array([dlltheta, dllg])\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14df59a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3) #Set seed\n",
    "bnds = np.array([[eps, 10], #Set bounds on \\ell between jitter and 10\n",
    "                [eps, np.var(Y2)]])#Set bounds on g between jitter and variance of Y\n",
    "#optimize \\ell and g\n",
    "init = np.array([0.1, 0.1*np.var(Y2)])\n",
    "solution = scipy.optimize.minimize(nl, init, args = (X2,Y2), method='L-BFGS-B', jac = gradnl, bounds=bnds, tol=1e-10 )\n",
    "print(solution)\n",
    "ell, g = solution.x\n",
    "iterations = solution.nit\n",
    "ell_opt = ell.flatten()\n",
    "g_opt = g.flatten()\n",
    "print(\"Value of g: \", g)\n",
    "print(\"Value of \\ell: \", ell)\n",
    "print(\"# of Iterations: \", iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3892441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build training quantities\n",
    "K = covkernel(X2,X2,l=ell_opt)\n",
    "K += g_opt * np.eye(K.shape[0])\n",
    "Ki = np.linalg.inv(K)\n",
    "tau2hat = Y2.T@Ki@Y2/(X2.shape[0])\n",
    "#Build predictive quantities\n",
    "gn =40\n",
    "#Define meshgrid for plotting \n",
    "xx = np.linspace(-2,4,gn)\n",
    "XX_mesh = np.array(np.meshgrid(xx,xx))\n",
    "XX = XX_mesh.reshape((2,gn**2)).T\n",
    "KXX = covkernel(XX,XX,l=ell_opt) \n",
    "KXX += g_opt * np.eye(KXX.shape[0])\n",
    "KX = covkernel(XX,X2,l=ell_opt)\n",
    "#Add kriging equations\n",
    "mup = KX@Ki@Y2\n",
    "Sigmap = tau2hat*(KXX-KX@Ki@KX.T)\n",
    "sdp = np.sqrt(np.diag(Sigmap))\n",
    "\n",
    "#Plot\n",
    "Xa, Xb = XX_mesh\n",
    "Z1 = mup.reshape(gn,gn)\n",
    "Z2 = sdp.reshape(gn,gn)\n",
    "z = [Z1,Z2]\n",
    "tot_lev = [60,60]\n",
    "#Set plot details for mean predictions\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = len(z), figsize = (14,6))\n",
    "ax = axes\n",
    "title = [\"Mean\", \"StDev\"]\n",
    "\n",
    "for i in range(len(z)):      \n",
    "    #Create a colormap and colorbar for each subplot\n",
    "    cs_fig = ax[i].contourf(Xa, Xb,z[i], levels = 900, cmap = \"jet\")\n",
    "    if np.amax(z[i]) < 1e-1 or np.amax(z[i]) > 1000:\n",
    "        cbar = plt.colorbar(cs_fig, ax = ax[i], format='%.2e')\n",
    "    else:\n",
    "        cbar = plt.colorbar(cs_fig, ax = ax[i], format = '%2.2f')\n",
    "\n",
    "    #Create a line contour for each colormap\n",
    "    cs2_fig = ax[i].contour(cs_fig, levels=cs_fig.levels[::tot_lev[i]], colors='k', alpha=0.7, linestyles='dashed', linewidths=3)\n",
    "    ax[i].clabel(cs2_fig,  levels=cs_fig.levels[::tot_lev[i]][1::2], fontsize=10, inline=1)\n",
    "\n",
    "    #plot training data X values\n",
    "    ax[i].scatter(X2[:,0],X2[:,1], color=\"green\",s=25, label = \"Training Data\", marker = \"o\")  \n",
    "\n",
    "    #Get legend information\n",
    "    if i == len(z)-1:\n",
    "        handles, labels = ax[i].get_legend_handles_labels()\n",
    "\n",
    "    #Plots axes such that they are scaled the same way (eg. circles look like circles) and name axes\n",
    "    ax[i].axis('scaled')  \n",
    "    ax[i].set_xlabel('$x_1$',weight='bold',fontsize=16)\n",
    "    ax[i].set_ylabel('$x_2$',weight='bold',fontsize=16)\n",
    "\n",
    "    #Plot title and set axis scale\n",
    "    ax[i].set_title(title[i], weight='bold',fontsize=16)\n",
    "    ax[i].set_xlim(left = np.amin(Xa), right = np.amax(Xa))\n",
    "    ax[i].set_ylim(bottom = np.amin(Xb), top = np.amax(Xb))      \n",
    "\n",
    "#Plots legend and title\n",
    "plt.tight_layout()\n",
    "fig.legend(handles, labels, loc=\"upper left\")  #bbox_to_anchor=(-0.01, 0.9), borderaxespad=0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ed6d4",
   "metadata": {},
   "source": [
    "## 5.2.5 Anisotropic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3344196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fried(n=50, m = 6):\n",
    "    assert m>5, \"Must have at least 5 columns\"\n",
    "    X = LHS_Design(n, m, seed = 10)\n",
    "    Ytrue = 10*np.sin(np.pi*X[:,0]*X[:,1]) + 20*(X[:,2] -0.5)**2 + 10*X[:,3] + 5*X[:,4]\n",
    "    Y = Ytrue + np.random.normal(size=(Ytrue.size)  ,loc = 0, scale = 1)\n",
    "    Y = Y.reshape(-1,1)\n",
    "    Ytrue = Ytrue.reshape(-1,1)\n",
    "    stack = np.hstack((X, Y, Ytrue))\n",
    "    dataframe = pd.DataFrame(data = stack)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 7\n",
    "n = 200\n",
    "nprime = 1000\n",
    "\n",
    "data = fried(n+nprime, m)\n",
    "X = data.to_numpy()[0:n,0:m]\n",
    "Y = data.to_numpy()[0:n,m:m+1].flatten()\n",
    "XX = data.to_numpy()[n:nprime, 0:m]\n",
    "YY = data.to_numpy()[n:nprime,-2].flatten()\n",
    "YYtrue = data.to_numpy()[n:nprime,-1].flatten()\n",
    "\n",
    "np.random.seed(3) #Set seed\n",
    "bnds = np.array([[eps, 10], #Set bounds on \\ell between jitter and 10\n",
    "                [eps, np.var(Y)]])#Set bounds on g between jitter and variance of Y\n",
    "#optimize \\ell and g\n",
    "init = np.array([1, 0.1*np.var(Y)])\n",
    "solution = scipy.optimize.minimize(nl, init, args = (X,Y), method='L-BFGS-B', jac = gradnl, bounds=bnds, tol=1e-10 )\n",
    "print(solution)\n",
    "ell, g = solution.x\n",
    "iterations = solution.nit\n",
    "ell_opt = ell.flatten()\n",
    "g_opt = g.flatten()\n",
    "print(\"Value of g: \", g)\n",
    "print(\"Value of \\ell: \", ell)\n",
    "print(\"# of Iterations: \", iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d3a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build training quantities\n",
    "K = covkernel(X,X,l=ell_opt)\n",
    "K += g_opt * np.eye(K.shape[0])\n",
    "Ki = np.linalg.inv(K)\n",
    "tau2hat = Y.T@Ki@Y/(X.shape[0])\n",
    "\n",
    "KXX = covkernel(XX,XX,l=ell_opt) \n",
    "KXX += g_opt * np.eye(KXX.shape[0])\n",
    "\n",
    "KX = covkernel(XX,X,l=ell_opt)\n",
    "\n",
    "#Add kriging equations\n",
    "mup = KX@Ki@Y\n",
    "Sigmap = tau2hat*(KXX-KX@Ki@KX.T)\n",
    "sdp = np.sqrt(np.diag(Sigmap))\n",
    "\n",
    "#Calculate RMSE\n",
    "rmse = np.sqrt(np.average((YYtrue - mup)**2))\n",
    "print(\"gpiso: \", round(rmse,3))\n",
    "\n",
    "#Note skipping MARS implementation because I can't find an install package for python that works with Python 3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def covkernel_sep(xi,xj,l):\n",
    "    dij2_l_sum = 0\n",
    "    for i in range(xi.shape[1]):\n",
    "        xi1 = xi[:,i].reshape(-1,1)\n",
    "        xj1 = xj[:,i].reshape(-1,1)\n",
    "        # returns matrix of pairwise distances\n",
    "        dij = cdist(xi1,xj1,metric='euclidean')\n",
    "        dij2_l_sum +=  dij**2/l[i]\n",
    "        \n",
    "    return np.exp(-dij2_l_sum)\n",
    "\n",
    "def nlsep(par, X, Y):\n",
    "    theta = par[0:X.shape[1]]\n",
    "    g = par[-1]\n",
    "    n = Y.shape[0]\n",
    "    K = covkernel_sep(X, X, theta)\n",
    "    K += g * np.eye(K.shape[0])\n",
    "    Ki = np.linalg.inv(K)\n",
    "    sign, logdet = np.linalg.slogdet(K)\n",
    "    ldetK = logdet\n",
    "    ll = -(n/2)*np.log(Y.T@Ki@Y) - (1/2)*ldetK\n",
    "    return -ll\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "np.random.seed(6) #Set seed\n",
    "ell_bounds = np.array([eps, 10])\n",
    "l_bounds = np.tile(ell_bounds,(X.shape[1],1))\n",
    "g_bounds = np.array([eps, np.var(Y)]).reshape(-1,2)\n",
    "bnds = np.concatenate((l_bounds, g_bounds), axis=0)\n",
    "\n",
    "#optimize \\ell and g\n",
    "hp_guess = np.ones((X.shape[1]))*0.1\n",
    "init = np.append(hp_guess, 0.1*np.var(Y))\n",
    "solution = scipy.optimize.minimize(nlsep, init, args = (X,Y), method='L-BFGS-B', bounds=bnds, tol=1e-10 )\n",
    "print(solution)\n",
    "hps = solution.x\n",
    "ell = hps[0:-1]\n",
    "g = hps[-1]\n",
    "iterations = solution.nit\n",
    "ell_opt = ell.flatten()\n",
    "g_opt = g.flatten()\n",
    "print(\"Value of g: \", g)\n",
    "print(\"Value of \\ell: \", ell)\n",
    "print(\"# of Iterations: \", iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e5414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradnlsep(par, X, Y):\n",
    "    theta = par[0:X.shape[1]]\n",
    "    g = par[-1]\n",
    "    n = Y.shape[0]\n",
    "    K = covkernel_sep(X, X, theta)\n",
    "    K += g * np.eye(K.shape[0])\n",
    "    Ki = np.linalg.inv(K)\n",
    "    KiY = Ki@Y\n",
    "    \n",
    "    #loop over theta components\n",
    "    dlltheta = np.zeros((len(theta)))\n",
    "    for k in range(len(theta)):\n",
    "        xi1 = X[:,k].reshape(-1,1)\n",
    "        xj1 = X[:,k].reshape(-1,1)\n",
    "        dij = cdist(xi1,xj1,metric='euclidean')\n",
    "        dotK = K * dij/theta[k]**2\n",
    "        dlltheta[k] = (n/2)*KiY.T@dotK@KiY /(Y.T@KiY) - (1/2)*np.sum(np.diag(Ki@dotK))\n",
    "        \n",
    "    #for g\n",
    "    dllg = (n/2)*KiY.T@KiY/(Y.T@KiY) - (1/2)*np.sum(np.diag(Ki))\n",
    "    dllg_mat = np.array([dllg]).reshape(1,-1)\n",
    "    grad = -np.append(dlltheta, dllg_mat)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d64ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "np.random.seed(3) #Set seed\n",
    "ell_bounds = np.array([eps, 10])\n",
    "l_bounds = np.tile(ell_bounds,(X.shape[1],1))\n",
    "g_bounds = np.array([eps, np.var(Y)]).reshape(-1,2)\n",
    "bnds = np.concatenate((l_bounds, g_bounds), axis=0)\n",
    "\n",
    "#optimize \\ell and g\n",
    "hp_guess = np.ones((X.shape[1]))*1\n",
    "init = np.append(hp_guess, 0.1*np.var(Y))\n",
    "solution = scipy.optimize.minimize(nlsep, init, args = (X,Y), method='L-BFGS-B', jac = gradnlsep, bounds=bnds, tol=1e-10 )\n",
    "print(solution)\n",
    "hps = solution.x\n",
    "ell = hps[0:-1]\n",
    "g = hps[-1]\n",
    "iterations = solution.nit\n",
    "ell_opt = ell.flatten()\n",
    "g_opt = g.flatten()\n",
    "print(\"Value of g: \", g)\n",
    "print(\"Value of \\ell: \", ell)\n",
    "print(\"# of Iterations: \", iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61484f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build training quantities\n",
    "K = covkernel_sep(X, X, ell_opt)\n",
    "K += g_opt * np.eye(K.shape[0])\n",
    "Ki = np.linalg.inv(K)\n",
    "tau2hat = Y.T@Ki@Y/(X.shape[0])\n",
    "KXX = covkernel_sep(XX, XX, ell_opt)\n",
    "KXX += g_opt * np.eye(KXX.shape[0])\n",
    "KX = covkernel_sep(XX, X, ell_opt)\n",
    "#Add kriging equations\n",
    "mup2 = KX@Ki@Y\n",
    "Sigmap2 = tau2hat*(KXX-KX@Ki@KX.T)\n",
    "sdp2 = np.sqrt(np.diag(Sigmap2))\n",
    "#Calculate RMSE\n",
    "rmse2 = np.sqrt(np.average((YYtrue - mup2)**2))\n",
    "print(\"gpiso: \", round(rmse,3))\n",
    "print(\"gpsep: \", round(rmse2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349b4b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gpiso score: \", score(YY, mup, Sigmap))\n",
    "print(\"gpsep score: \", score(YY, mup2, Sigmap2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2292ea",
   "metadata": {},
   "source": [
    "## 5.2.6 Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570be8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply scikit learn library\n",
    "%time\n",
    "#Define kernel with g and vector theta\n",
    "kernel = ConstantKernel(constant_value=1e-3)*RBF(length_scale = np.ones(X.shape[1])) + WhiteKernel(noise_level=1)\n",
    "#Define and fit the model\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=6, n_restarts_optimizer =20, optimizer = \"fmin_l_bfgs_b\")\n",
    "fit_process = gpr.fit(X, Y)\n",
    "#Back out trained hps\n",
    "opt_kern_params = fit_process.kernel_\n",
    "outputscl_final = opt_kern_params.k1.k1.constant_value\n",
    "lenscl_final = opt_kern_params.k1.k2.length_scale\n",
    "noise_final = opt_kern_params.k2.noise_level\n",
    "\n",
    "#Print hps\n",
    "print(\"g: \", outputscl_final)\n",
    "print(\"\\ell: \", lenscl_final)\n",
    "\n",
    "#Find rmse values and prediction values\n",
    "results = gpr.predict(XX, return_cov=True)\n",
    "\n",
    "mup3 = results[0]\n",
    "Sigmap3 = results[1]\n",
    "\n",
    "print(\"RMSE: \",np.sqrt(np.average((YYtrue - mup3)**2)))\n",
    "print(\"scikit learn score: \", score(YY, mup3, Sigmap3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d15e6c",
   "metadata": {},
   "source": [
    "## 5.2.7 A bakeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d043b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "R = 30 #30 MC iterations\n",
    "scores = np.zeros((3,R,3)) #Methods x MC iters x RMSE, score, time\n",
    "#loop over R\n",
    "for r in range(R):\n",
    "    #Train-test partition and application of f(x) on both\n",
    "    data = fried(2*n, m)\n",
    "    train = data.to_numpy()[:n,:]\n",
    "    test = data.to_numpy()[n:2*n,:]\n",
    "    #Extract elements from training and testing set\n",
    "    X = train[:,:m]\n",
    "    Y = train[:,-2].flatten()\n",
    "    XX = test[:,:m]\n",
    "    YY = test[:,-2].flatten()\n",
    "    YYtrue = test[:,-1].flatten()\n",
    "    #Isotropic GP fit and predict by hand\n",
    "    tic = time.process_time()\n",
    "    bnds = np.array([[eps, 10], #Set bounds on \\ell between jitter and 10\n",
    "                [eps, np.var(Y)]])#Set bounds on g between jitter and variance of Y\n",
    "    init = np.array([0.1, 0.1*np.var(Y)])\n",
    "    solution = scipy.optimize.minimize(nl, init, args = (X,Y), method='L-BFGS-B', jac = gradnl, bounds=bnds, tol=1e-10 )\n",
    "    ell, g = solution.x\n",
    "    iterations = solution.nit\n",
    "    ell_opt_iso = ell.flatten()\n",
    "    g_opt_iso = g.flatten()\n",
    "    K = covkernel(X,X,l=ell_opt_iso)\n",
    "    K += g_opt_iso * np.eye(K.shape[0])\n",
    "    Ki = np.linalg.inv(K)\n",
    "    tau2hat = Y.T@Ki@Y/(X.shape[0])\n",
    "    KXX = covkernel(XX,XX,l=ell_opt_iso) \n",
    "    KXX += g_opt_iso * np.eye(KXX.shape[0])\n",
    "    KX = covkernel(XX,X,l=ell_opt_iso) \n",
    "    mup = KX@Ki@Y\n",
    "    Sigmap = tau2hat*(KXX-KX@Ki@KX.T)\n",
    "    toc = time.process_time()   \n",
    "    #Isotropic GP fit and predict by hand\n",
    "    scores[0, r, 0] = np.sqrt(np.average((YYtrue - mup)**2))\n",
    "    scores[0, r, 1] = score(YY, mup, Sigmap)\n",
    "    scores[0, r, 2] = toc - tic  \n",
    "    #Seperable GP fit and predict by hand\n",
    "    tic = time.process_time()\n",
    "    ell_bounds = np.array([eps, 10])\n",
    "    l_bounds = np.tile(ell_bounds,(X.shape[1],1))\n",
    "    g_bounds = np.array([eps, np.var(Y)]).reshape(-1,2)\n",
    "    bnds = np.concatenate((l_bounds, g_bounds), axis=0)\n",
    "    hp_guess = np.ones((X.shape[1]))*1\n",
    "    init = np.append(hp_guess, 0.1*np.var(Y))\n",
    "    solution = scipy.optimize.minimize(nlsep, init, args = (X,Y), method='L-BFGS-B', jac = gradnlsep, bounds=bnds, tol=1e-10 )\n",
    "    hps = solution.x\n",
    "    ell = hps[0:-1]\n",
    "    g = hps[-1]\n",
    "    iterations = solution.nit\n",
    "    ell_opt_sep = ell.flatten()\n",
    "    g_opt_sep = g.flatten()\n",
    "    K = covkernel_sep(X,X,ell_opt_sep)\n",
    "    K += g_opt_sep * np.eye(K.shape[0])\n",
    "    Ki = np.linalg.inv(K)\n",
    "    tau2hat = Y.T@Ki@Y/(X.shape[0])\n",
    "    KXX = covkernel_sep(XX,XX,ell_opt_sep) \n",
    "    KXX += g_opt_sep * np.eye(KXX.shape[0])\n",
    "    KX = covkernel_sep(XX,X,ell_opt_sep)\n",
    "    mup2 = KX@Ki@Y\n",
    "    Sigmap2 = tau2hat*(KXX-KX@Ki@KX.T)\n",
    "    toc = time.process_time()  \n",
    "    #Calculation of metrics for seperable GP by hand\n",
    "    scores[1, r, 0] = np.sqrt(np.average((YYtrue - mup2)**2))\n",
    "    scores[1, r, 1] = score(YY, mup2, Sigmap2)\n",
    "    scores[1, r, 2] = toc - tic \n",
    "    #Scikit learn based seperable GP\n",
    "    tic = time.process_time()\n",
    "    kernel = ConstantKernel(constant_value=0.008)*RBF(length_scale = np.ones(X.shape[1])) + WhiteKernel(noise_level=1)\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel, random_state=1, n_restarts_optimizer =1, optimizer = \"fmin_l_bfgs_b\")\n",
    "    fit_process = gpr.fit(X, Y)\n",
    "    opt_kern_params = fit_process.kernel_\n",
    "    g_opt_skl = opt_kern_params.k1.k1.constant_value\n",
    "    ell_opt_skl = opt_kern_params.k1.k2.length_scale\n",
    "    noise_skl = opt_kern_params.k2.noise_level\n",
    "    results = gpr.predict(XX, return_cov=True)\n",
    "    mup3 = results[0]\n",
    "    Sigmap3 = results[1]\n",
    "    toc = time.process_time()\n",
    "    #Calculation of metrics for scikit learn GP by hand\n",
    "    scores[2, r, 0] = np.sqrt(np.average((YYtrue - mup2)**2))\n",
    "    scores[2, r, 1] = score(YY, mup2, Sigmap2)\n",
    "    scores[2, r, 2] = toc - tic    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe88fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create boxplots of data\n",
    "data1 = scores[:,:,0]\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('RMSE')\n",
    "ax1.boxplot(data1.T)\n",
    "\n",
    "data2 = scores[:,:,1]\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_title('Scores')\n",
    "ax2.boxplot(data2.T)\n",
    "\n",
    "data3 = scores[:,:,2]\n",
    "fig3, ax3 = plt.subplots()\n",
    "ax3.set_title('Time')\n",
    "ax3.boxplot(data3.T)\n",
    "plt.show()\n",
    "\n",
    "#1 = isotropic, 2 = seperable, 3 = scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76c4a5",
   "metadata": {},
   "source": [
    "# 5.3 Some interpretation and perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d6106",
   "metadata": {},
   "source": [
    "## 5.3.3 Stationary Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf0faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define matern kernel\n",
    "def matern(r, nu, theta):\n",
    "    rat = r*np.sqrt(2*nu/theta)\n",
    "    C = ((2**(1-nu))/scipy.special.gamma(nu))*(rat**nu)*scipy.special.kv(nu, rat)\n",
    "    C[np.isnan(C)] = 1\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66201cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.linspace(eps,3,100)\n",
    "\n",
    "# #Plot Matern functions w/ different nus\n",
    "plt.figure(figsize = (7,6))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(direction=\"in\",top=True, right=True)\n",
    "plt.locator_params(axis='y', nbins=7)\n",
    "plt.locator_params(axis='x', nbins=5)\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "plt.plot(r, matern(r, 0.5, 1), color = \"black\", label = \"nu=1/2\")\n",
    "plt.plot(r, matern(r, 2, 1),  linestyle='dashed', color = \"red\", label = \"nu=2\")\n",
    "plt.plot(r, matern(r, 10, 1),  linestyle='dashed', color = \"green\", label = \"nu=10\")\n",
    "\n",
    "#Set plot details\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0)\n",
    "x_label = \"r\"\n",
    "y_label = \"k(r,nu)\"\n",
    "plt.xlabel(x_label, fontsize=16, fontweight='bold')\n",
    "plt.ylabel(y_label, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1932dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(eps,10,100).reshape(-1,1)\n",
    "R = cdist(X,X,metric='euclidean')\n",
    "K_05 = matern(R, 0.5, 1)\n",
    "K_2 = matern(R, 2, 1)\n",
    "K_10 = matern(R, 10, 1)\n",
    "mean = np.zeros((len(X)))\n",
    "\n",
    "p_05 = rng.multivariate_normal(mean, K_05, 3)\n",
    "p_2 = rng.multivariate_normal(mean, K_2, 3)\n",
    "p_10 = rng.multivariate_normal(mean, K_10, 3)\n",
    "to_plot = [p_05.T, p_2.T, p_10.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691226f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set plot details for mean predictions\n",
    "title = [\"nu=1/2\", \"nu=2\", \"nu=10\"]\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = len(to_plot), figsize = (15,5))\n",
    "#Plots legend and title\n",
    "plt.tight_layout()\n",
    "\n",
    "for i in range(len(to_plot)):\n",
    "#Get legend information\n",
    "    ax[i].plot(X, to_plot[i])\n",
    "#     if i == len(to_plot)-1:\n",
    "#         handles, labels = ax[i].get_legend_handles_labels()\n",
    "\n",
    "    #Plots axes such that they are scaled the same way (eg. circles look like circles) and name axes  \n",
    "    ax[i].set_xlabel('$x$',weight='bold',fontsize=16)\n",
    "    ax[i].set_ylabel('$y$',weight='bold',fontsize=16)\n",
    "\n",
    "    #Plot title and set axis scale\n",
    "    ax[i].set_title(title[i], weight='bold',fontsize=16)\n",
    "    ax[i].set_xlim(left = np.amin(X), right = np.amax(X))\n",
    "    ax[i].set_ylim(bottom = np.amin(to_plot), top = np.amax(to_plot)) \n",
    "#     ax[i].axis('square')\n",
    "\n",
    "# fig.legend(handles, labels, loc=\"upper left\")  #bbox_to_anchor=(-0.01, 0.9), borderaxespad=0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfa638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = rng.multivariate_normal(mean, K_2, 1).T.reshape(-1,1)\n",
    "dF = (F[1:] - F[:len(F)-1])/(X[1] - X[0])\n",
    "dF2 = (dF[1:] - dF[:len(dF)-1])/(X[1] - X[0])\n",
    "\n",
    "# #Plot Matern functions w/ different nus\n",
    "plt.figure(figsize = (7,6))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(direction=\"in\",top=True, right=True)\n",
    "plt.locator_params(axis='y', nbins=7)\n",
    "plt.locator_params(axis='x', nbins=5)\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "plt.plot(X, F, color = \"black\", label = \"F\")\n",
    "plt.plot(X[1:], dF,  linestyle='dashed', color = \"red\", label = \"dF\")\n",
    "plt.plot(X[2:],dF2,  linestyle='dashed', color = \"green\", label = \"dF2\")\n",
    "\n",
    "#Set plot details\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0)\n",
    "x_label = \"r\"\n",
    "y_label = \"k(r,nu)\"\n",
    "plt.xlabel(x_label, fontsize=16, fontweight='bold')\n",
    "plt.ylabel(y_label, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b558efda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerexp(r, alpha, theta):\n",
    "    C = np.exp(-(r/np.sqrt(theta))**(alpha))\n",
    "    C[np.isnan(C)] = 1\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25686114",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.linspace(eps,3,100)\n",
    "\n",
    "# #Plot Matern functions w/ different nus\n",
    "plt.figure(figsize = (7,6))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(direction=\"in\",top=True, right=True)\n",
    "plt.locator_params(axis='y', nbins=7)\n",
    "plt.locator_params(axis='x', nbins=5)\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "plt.plot(r, powerexp(r, 1.5, 1), color = \"black\", label = \"alpha=1.5\")\n",
    "plt.plot(r, powerexp(r, 1.9, 1),  linestyle='dashed', color = \"red\", label = \"alpha=1.9\")\n",
    "plt.plot(r, powerexp(r, 2, 1),  linestyle='dashed', color = \"green\", label = \"alpha=2\")\n",
    "\n",
    "#Set plot details\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0)\n",
    "x_label = \"r\"\n",
    "y_label = \"k(r,alpha)\"\n",
    "plt.xlabel(x_label, fontsize=16, fontweight='bold')\n",
    "plt.ylabel(y_label, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d143aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ka_15 = powerexp(R, 1.5, 1)\n",
    "Ka_19 = powerexp(R, 1.9, 1)\n",
    "Ka_20 = powerexp(R, 2, 1)\n",
    "\n",
    "pa_15 = rng.multivariate_normal(mean, Ka_15, 3)\n",
    "pa_19 = rng.multivariate_normal(mean, Ka_19, 3)\n",
    "pa_20 = rng.multivariate_normal(mean, Ka_20, 3)\n",
    "to_plot = [pa_15.T, pa_19.T, pa_20.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9feeaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set plot details for mean predictions\n",
    "title = [\"alpha=1.5\", \"alpha=1.9\", \"alpha=2\"]\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = len(to_plot), figsize = (15,5))\n",
    "#Plots legend and title\n",
    "plt.tight_layout()\n",
    "\n",
    "for i in range(len(to_plot)):\n",
    "#Get legend information\n",
    "    ax[i].plot(X, to_plot[i])\n",
    "#     if i == len(to_plot)-1:\n",
    "#         handles, labels = ax[i].get_legend_handles_labels()\n",
    "\n",
    "    #Plots axes such that they are scaled the same way (eg. circles look like circles) and name axes  \n",
    "    ax[i].set_xlabel('$x$',weight='bold',fontsize=16)\n",
    "    ax[i].set_ylabel('$y$',weight='bold',fontsize=16)\n",
    "\n",
    "    #Plot title and set axis scale\n",
    "    ax[i].set_title(title[i], weight='bold',fontsize=16)\n",
    "    ax[i].set_xlim(left = np.amin(X), right = np.amax(X))\n",
    "    ax[i].set_ylim(bottom = np.amin(to_plot), top = np.amax(to_plot)) \n",
    "#     ax[i].axis('square')\n",
    "\n",
    "# fig.legend(handles, labels, loc=\"upper left\")  #bbox_to_anchor=(-0.01, 0.9), borderaxespad=0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d414785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratquad(r, alpha, theta):\n",
    "    C = (1 + r**2/(2*alpha*theta))**(-alpha)\n",
    "    C[np.isnan(C)] = 1\n",
    "    return C\n",
    "\n",
    "Krq_05 = ratquad(R, 1/2, 1)\n",
    "Krq_2 = ratquad(R, 2, 1)\n",
    "Krq_10 = ratquad(R, 10, 1)\n",
    "\n",
    "prq_05 = rng.multivariate_normal(mean, Krq_05, 3)\n",
    "prq_2 = rng.multivariate_normal(mean, Krq_2, 3)\n",
    "prq_10 = rng.multivariate_normal(mean, Krq_10, 3)\n",
    "to_plot = [prq_05.T, prq_2.T, prq_10.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b9c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.linspace(eps,3,100)\n",
    "\n",
    "# #Plot Matern functions w/ different nus\n",
    "plt.figure(figsize = (7,6))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(direction=\"in\",top=True, right=True)\n",
    "plt.locator_params(axis='y', nbins=7)\n",
    "plt.locator_params(axis='x', nbins=5)\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "plt.plot(r, ratquad(r, 0.5, 1), color = \"black\", label = \"alpha=0.5\")\n",
    "plt.plot(r, ratquad(r, 2, 1),  linestyle='dashed', color = \"red\", label = \"alpha=2\")\n",
    "plt.plot(r, ratquad(r, 10, 1),  linestyle='dashed', color = \"green\", label = \"alpha=10\")\n",
    "\n",
    "#Set plot details\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0)\n",
    "x_label = \"r\"\n",
    "y_label = \"k(r,alpha)\"\n",
    "plt.xlabel(x_label, fontsize=16, fontweight='bold')\n",
    "plt.ylabel(y_label, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3570c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set plot details for mean predictions\n",
    "title = [\"alpha=0.5\", \"alpha=2\", \"alpha=10\"]\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = len(to_plot), figsize = (15,5))\n",
    "#Plots legend and title\n",
    "plt.tight_layout()\n",
    "\n",
    "for i in range(len(to_plot)):\n",
    "#Get legend information\n",
    "    ax[i].plot(X, to_plot[i])\n",
    "#     if i == len(to_plot)-1:\n",
    "#         handles, labels = ax[i].get_legend_handles_labels()\n",
    "\n",
    "    #Plots axes such that they are scaled the same way (eg. circles look like circles) and name axes  \n",
    "    ax[i].set_xlabel('$x$',weight='bold',fontsize=16)\n",
    "    ax[i].set_ylabel('$y$',weight='bold',fontsize=16)\n",
    "\n",
    "    #Plot title and set axis scale\n",
    "    ax[i].set_title(title[i], weight='bold',fontsize=16)\n",
    "    ax[i].set_xlim(left = np.amin(X), right = np.amax(X))\n",
    "    ax[i].set_ylim(bottom = np.amin(to_plot), top = np.amax(to_plot)) \n",
    "#     ax[i].axis('square')\n",
    "\n",
    "# fig.legend(handles, labels, loc=\"upper left\")  #bbox_to_anchor=(-0.01, 0.9), borderaxespad=0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf903be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBF_func(r, theta):\n",
    "    C = np.exp(-r**2/(2*theta**2))\n",
    "    C[np.isnan(C)] = 1\n",
    "    return C \n",
    "\n",
    "r = np.linspace(eps,3,100)\n",
    "\n",
    "# #Plot Matern functions w/ different nus\n",
    "plt.figure(figsize = (7,6))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(direction=\"in\",top=True, right=True)\n",
    "plt.locator_params(axis='y', nbins=7)\n",
    "plt.locator_params(axis='x', nbins=5)\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "plt.plot(r, matern(r, 1.5, 1), color = \"black\", label = \"Matern 3/2\")\n",
    "plt.plot(r, ratquad(r, 0.5, 1), color = \"red\", label = \"Rat Quad\")\n",
    "# plt.plot(r, powerexp(r, 1.5, 1), color = \"green\", label = \"Power Exp\")\n",
    "plt.plot(r, RBF_func(r, 1), color = \"green\", label = \"RBF\")\n",
    "\n",
    "#Set plot details\n",
    "plt.legend(fontsize=20, loc = \"upper right\")\n",
    "x_label = \"Distance\"\n",
    "y_label = \"Correlation\"\n",
    "plt.xlabel(x_label, fontsize=24, fontweight='bold')\n",
    "plt.ylabel(y_label, fontsize=24, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c602eac",
   "metadata": {},
   "source": [
    "## 5.3.4 Signal to noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd9aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0,10,40).reshape(-1,1)\n",
    "ytrue = np.sin((np.pi*X/5)) + 0.2*np.cos(4*np.pi*X/5)\n",
    "y = ytrue + np.random.normal(size=(ytrue.size)  ,loc = 0, scale = 0.2).reshape(-1,1)\n",
    "g = np.linspace(0.001, 8, 100)\n",
    "theta = np.linspace(0.1,5,100)\n",
    "grid = np.array(np.meshgrid(g,theta)).T.reshape(-1,2)\n",
    "ll =np.zeros((len(grid)))\n",
    "xx = np.linspace(0,10,100).reshape(-1,1)\n",
    "pm = np.zeros((len(grid),len(xx)))\n",
    "psd = np.zeros((len(grid),len(xx)))\n",
    "for i in range(len(grid)):\n",
    "    kernel = ConstantKernel(constant_value=grid[i,0], constant_value_bounds=\"fixed\")*RBF(length_scale = grid[i,1], length_scale_bounds=\"fixed\") + WhiteKernel(noise_level=0.2)\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel, random_state=10, optimizer = None)\n",
    "    fit_process = gpr.fit(X, y)\n",
    "    results = gpr.predict(xx, return_std=True)\n",
    "    ll[i] = gpr.log_marginal_likelihood()\n",
    "    pm[i] = results[0]\n",
    "    psd[i] = results[1]\n",
    "l = np.exp(ll - np.max(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e0ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "l_mesh = l.reshape(len(g),len(theta))\n",
    "z = [l_mesh]\n",
    "tot_lev = [100]\n",
    "#Set plot details for mean predictions\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = len(z), figsize = (7,6))\n",
    "ax = axes\n",
    "title = [\"Marginal Log Likelihood\"]\n",
    "\n",
    "#Create a colormap and colorbar for each subplot\n",
    "cs_fig = ax.contourf(g, theta, z[0], levels = 900, cmap = \"autumn\")\n",
    "if np.amax(z[0]) < 1e-1 or np.amax(z[0]) > 1000:\n",
    "    cbar = plt.colorbar(cs_fig, ax = ax, format='%.2e')\n",
    "else:\n",
    "    cbar = plt.colorbar(cs_fig, ax = ax, format = '%2.2f')\n",
    "\n",
    "#Create a line contour for each colormap\n",
    "cs2_fig = ax.contour(cs_fig, levels=cs_fig.levels[::tot_lev[0]], colors='k', alpha=0.7, linestyles='dashed', linewidths=3)\n",
    "ax.clabel(cs2_fig,  levels=cs_fig.levels[::tot_lev[0]], fontsize=10, inline=1)\n",
    "\n",
    "#Get legend information\n",
    "# if i == len(z)-1:\n",
    "#     handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "#Plots axes such that they are scaled the same way (eg. circles look like circles) and name axes\n",
    "# ax.axis('square')  \n",
    "ax.set_ylabel('$\\\\theta$',weight='bold',fontsize=16)\n",
    "ax.set_xlabel('$g$',weight='bold',fontsize=16)\n",
    "\n",
    "#Plot title and set axis scale\n",
    "ax.set_title(title[0], weight='bold',fontsize=16)\n",
    "ax.set_xlim(left = np.amin(g), right = np.amax(g))\n",
    "ax.set_ylim(bottom = np.amin(theta), top = np.amax(theta))      \n",
    "\n",
    "#Plots legend and title\n",
    "plt.tight_layout()\n",
    "# fig.legend(handles, labels, loc=\"upper left\")  #bbox_to_anchor=(-0.01, 0.9), borderaxespad=0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8118f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = pm + norm.ppf(0.05, loc=0, scale=psd)\n",
    "q2 = pm + norm.ppf(0.95, loc=0, scale=psd)\n",
    "ytrue2 = np.sin((np.pi*xx/5)) + 0.2*np.cos(4*np.pi*xx/5)\n",
    "\n",
    "# #Plot Posterior distribution\n",
    "plt.figure(figsize = (7,6))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(direction=\"in\",top=True, right=True)\n",
    "plt.locator_params(axis='y', nbins=7)\n",
    "plt.locator_params(axis='x', nbins=5)\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "for i in range(len(l)):\n",
    "    alpha = l[i] / max(l) / 2\n",
    "    plt.plot(xx, q1[i].T,  linestyle='dashed', color = (1, 0, 0, alpha))\n",
    "    plt.plot(xx, q2[i].T,  linestyle='dashed', color = (1, 0, 0, alpha))\n",
    "    plt.plot(xx, pm[i].T, color = (0, 0, 0, alpha))\n",
    "plt.scatter(X, y, marker = \"o\", color = \"green\", label = \"Training\", zorder=3)\n",
    "\n",
    "#Set plot details\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0)\n",
    "x_label = \"X-Values\"\n",
    "y_label = \"y-Values\"\n",
    "plt.xlabel(x_label, fontsize=16, fontweight='bold')\n",
    "plt.ylabel(y_label, fontsize=16, fontweight='bold')\n",
    "plt.ylim(-2.5, 2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b2306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ell =10\n",
    "n =100\n",
    "X = np.linspace(0,10,n)\n",
    "omega = np.linspace(-1,11, ell)\n",
    "K = np.zeros((n,ell))\n",
    "for j in range(ell):\n",
    "    K[:,j] = scipy.stats.norm.pdf(X,  omega[j])\n",
    "Beta = np.random.normal(size =(ell,3))\n",
    "F = K@Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d279c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot Posterior distribution\n",
    "plt.figure(figsize = (7,6))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(direction=\"in\",top=True, right=True)\n",
    "plt.locator_params(axis='y', nbins=7)\n",
    "plt.locator_params(axis='x', nbins=5)\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "plt.plot(X,F[:,0], color = \"blue\")\n",
    "plt.plot(X,F[:,1], color = \"red\")\n",
    "plt.plot(X,F[:,2], color = \"green\")\n",
    "plt.vlines(x=omega, ymin=np.min(F), ymax=np.max(F), colors='black', ls=':', lw=2, label='vline_single - full height')\n",
    "\n",
    "#Set plot details\n",
    "# plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0)\n",
    "x_label = \"X\"\n",
    "y_label = \"f(x)\"\n",
    "plt.xlabel(x_label, fontsize=16, fontweight='bold')\n",
    "plt.ylabel(y_label, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f514a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,10,40)\n",
    "n =len(x)\n",
    "K = np.zeros((n,ell))\n",
    "for j in range(ell):\n",
    "    K[:,j] = scipy.stats.norm.pdf(x,  omega[j])\n",
    "print(K.shape)\n",
    "K_df = pd.DataFrame(data = K)\n",
    "# print(K_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfad5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit linear regression model given K and y\n",
    "model = LinearRegression(fit_intercept=False).fit(K_df, y)\n",
    "\n",
    "# print the coefficients\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952bad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(-1,11,100)\n",
    "KK = np.zeros((len(xx),ell))\n",
    "for j in range(ell):\n",
    "    KK[:,j] = scipy.stats.norm.pdf(xx,  omega[j])\n",
    "KK_df = pd.DataFrame(data = KK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed40388",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(KK)\n",
    "stdev = np.std(p)\n",
    "q1 = p + norm.ppf(0.05, loc=0, scale=stdev)\n",
    "q2 = p + norm.ppf(0.95, loc=0, scale=stdev)\n",
    "ytrue2 = np.sin((np.pi*xx/5)) + 0.2*np.cos(4*np.pi*xx/5)\n",
    "\n",
    "# #Plot Posterior distribution\n",
    "plt.figure(figsize = (7,6))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(direction=\"in\",top=True, right=True)\n",
    "plt.locator_params(axis='y', nbins=7)\n",
    "plt.locator_params(axis='x', nbins=7)\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "plt.plot(xx, q1,  linestyle='dashed', color = \"red\")\n",
    "plt.plot(xx, q2,  linestyle='dashed', color = \"red\")\n",
    "plt.plot(xx, p, color = \"black\")\n",
    "plt.scatter(x, y, marker = \"o\", color = \"green\", label = \"Training\", zorder=3)\n",
    "\n",
    "#Set plot details\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0)\n",
    "x_label = \"X-Values\"\n",
    "y_label = \"y-Values\"\n",
    "plt.xlabel(x_label, fontsize=16, fontweight='bold')\n",
    "plt.ylabel(y_label, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c7bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,10,40)\n",
    "n =len(x)\n",
    "K = np.zeros((n,ell))\n",
    "for j in range(ell):\n",
    "    K[:,j] = scipy.stats.norm.pdf(x,  omega[j], 0.5)\n",
    "K_df = pd.DataFrame(data = K)\n",
    "\n",
    "#Fit linear regression model given K and y\n",
    "model = LinearRegression(fit_intercept=False).fit(K_df, y)\n",
    "# print the coefficients\n",
    "# print(model.coef_)\n",
    "\n",
    "xx = np.linspace(-1,11,100)\n",
    "KK = np.zeros((len(xx),ell))\n",
    "for j in range(ell):\n",
    "    KK[:,j] = scipy.stats.norm.pdf(xx,  omega[j], 0.5)\n",
    "KK_df = pd.DataFrame(data = KK)\n",
    "\n",
    "p = model.predict(KK)\n",
    "stdev = np.std(p)\n",
    "q1 = p + norm.ppf(0.05, loc=0, scale=stdev)\n",
    "q2 = p + norm.ppf(0.95, loc=0, scale=stdev)\n",
    "ytrue2 = np.sin((np.pi*xx/5)) + 0.2*np.cos(4*np.pi*xx/5)\n",
    "\n",
    "# #Plot Posterior distribution\n",
    "plt.figure(figsize = (7,6))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(direction=\"in\",top=True, right=True)\n",
    "plt.locator_params(axis='y', nbins=7)\n",
    "plt.locator_params(axis='x', nbins=7)\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "plt.plot(xx, q1,  linestyle='dashed', color = \"red\")\n",
    "plt.plot(xx, q2,  linestyle='dashed', color = \"red\")\n",
    "plt.plot(xx, p, color = \"black\")\n",
    "plt.scatter(x, y, marker = \"o\", color = \"green\", label = \"Training\", zorder=3)\n",
    "\n",
    "#Set plot details\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0)\n",
    "x_label = \"X-Values\"\n",
    "y_label = \"y-Values\"\n",
    "plt.xlabel(x_label, fontsize=16, fontweight='bold')\n",
    "plt.ylabel(y_label, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339bbb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,10,40)\n",
    "n =len(x)\n",
    "K = np.zeros((n,ell))\n",
    "for j in range(ell):\n",
    "    K[:,j] = scipy.stats.norm.pdf(x,  omega[j], 0.1)\n",
    "K_df = pd.DataFrame(data = K)\n",
    "\n",
    "#Fit linear regression model given K and y\n",
    "model = LinearRegression(fit_intercept=False).fit(K_df, y)\n",
    "# print the coefficients\n",
    "# print(model.coef_)\n",
    "\n",
    "xx = np.linspace(-1,11,100)\n",
    "KK = np.zeros((len(xx),ell))\n",
    "for j in range(ell):\n",
    "    KK[:,j] = scipy.stats.norm.pdf(xx,  omega[j], 0.1)\n",
    "KK_df = pd.DataFrame(data = KK)\n",
    "\n",
    "p = model.predict(KK)\n",
    "stdev = np.std(p)\n",
    "q1 = p + norm.ppf(0.05, loc=0, scale=stdev)\n",
    "q2 = p + norm.ppf(0.95, loc=0, scale=stdev)\n",
    "ytrue2 = np.sin((np.pi*xx/5)) + 0.2*np.cos(4*np.pi*xx/5)\n",
    "\n",
    "# #Plot Posterior distribution\n",
    "plt.figure(figsize = (7,6))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(direction=\"in\",top=True, right=True)\n",
    "plt.locator_params(axis='y', nbins=7)\n",
    "plt.locator_params(axis='x', nbins=7)\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "plt.plot(xx, q1,  linestyle='dashed', color = \"red\")\n",
    "plt.plot(xx, q2,  linestyle='dashed', color = \"red\")\n",
    "plt.plot(xx, p, color = \"black\")\n",
    "plt.scatter(x, y, marker = \"o\", color = \"green\", label = \"Training\", zorder=3)\n",
    "\n",
    "#Set plot details\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0)\n",
    "x_label = \"X-Values\"\n",
    "y_label = \"y-Values\"\n",
    "plt.xlabel(x_label, fontsize=16, fontweight='bold')\n",
    "plt.ylabel(y_label, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set number of experimental data/ set rng\n",
    "np.random.seed(10) #Set seed\n",
    "rng = np.random.default_rng() # sets up a \"random\" number generator\n",
    "ndim= 2\n",
    "nsamples = 40\n",
    "gn =40\n",
    "\n",
    "#Generate X data from LHS\n",
    "X = LHS_Design(nsamples, ndim, seed = 10)\n",
    "#Modify X data to correct scale\n",
    "X[:,0] = (X[:,0] - 0.5)*6 +1\n",
    "X[:,1] = (X[:,1] - 0.5)*6 +1\n",
    "\n",
    "#Generate omega data\n",
    "ell =20\n",
    "omega = LHS_Design(ell, 2, seed = 8)\n",
    "omega[:,0] = (omega[:,0] - 0.5)*6 +1\n",
    "omega[:,1] = (omega[:,1] - 0.5)*6 +1\n",
    "\n",
    "#Generate y data\n",
    "y = X[:,0]*np.exp(-X[:,0]**2 -X[:,1]**2)\n",
    "\n",
    "#Generate K matrix\n",
    "n = len(X)\n",
    "K = np.zeros((n,ell))\n",
    "for j in range(ell):\n",
    "    K[:,j] = scipy.stats.norm.pdf(X[:,0],  omega[j,0])*scipy.stats.norm.pdf(X[:,1],  omega[j,1])\n",
    "# print(K.shape)\n",
    "K_df = pd.DataFrame(data = K)\n",
    "\n",
    "#Fit model\n",
    "#Fit linear regression model given K and y\n",
    "model = LinearRegression(fit_intercept=False).fit(K_df, y)\n",
    "\n",
    "xx = np.linspace(-2,4,gn)\n",
    "XX_mesh = np.array(np.meshgrid(xx,xx))\n",
    "XX = XX_mesh.reshape((2,gn**2)).T\n",
    "yy = XX[:,0]*np.exp(-XX[:,0]**2 -XX[:,1]**2)\n",
    "# print(XX.shape)\n",
    "\n",
    "#Define KK\n",
    "KK = np.zeros((len(XX),ell))\n",
    "for j in range(ell):\n",
    "    KK[:,j] = scipy.stats.norm.pdf(XX[:,0], omega[j,0])*scipy.stats.norm.pdf(XX[:,1],  omega[j,1])\n",
    "KK_df = pd.DataFrame(data = KK)\n",
    "p = model.predict(KK)\n",
    "\n",
    "#Calculate error\n",
    "std_err = (yy - p)**2\n",
    "# beta_hat = np.linalg.inv(KK.T @ KK) @ KK.T @ yy\n",
    "# y_hat = model.predict(KK)\n",
    "# residuals = yy - y_hat\n",
    "# residual_sum_of_squares = residuals.T @ residuals\n",
    "# sigma_squared_hat = residual_sum_of_squares / (n - ndim)\n",
    "# var_beta_hat = np.linalg.inv(KK.T @ KK) * sigma_squared_hat\n",
    "# for dim_ in range(ndim):\n",
    "#     standard_error = var_beta_hat[dim_, dim_] ** 0.5\n",
    "\n",
    "# #Plot\n",
    "X1, X2 = XX_mesh\n",
    "Z1 = p.reshape(gn,gn)\n",
    "Z2 = std_err.reshape(gn,gn)\n",
    "z = [Z1,Z2]\n",
    "tot_lev = [60,60]\n",
    "#Set plot details for mean predictions\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = len(z), figsize = (14,6))\n",
    "ax = axes\n",
    "title = [\"Mean\", \"St Err\"]\n",
    "\n",
    "for i in range(len(z)):      \n",
    "    #Create a colormap and colorbar for each subplot\n",
    "    cs_fig = ax[i].contourf(X1, X2,z[i], levels = 900, cmap = \"autumn\")\n",
    "    if np.amax(z[i]) < 1e-1 or np.amax(z[i]) > 1000:\n",
    "        cbar = plt.colorbar(cs_fig, ax = ax[i], format='%.2e')\n",
    "    else:\n",
    "        cbar = plt.colorbar(cs_fig, ax = ax[i], format = '%2.2f')\n",
    "\n",
    "    #Create a line contour for each colormap\n",
    "    cs2_fig = ax[i].contour(cs_fig, levels=cs_fig.levels[::tot_lev[i]], colors='k', alpha=0.7, linestyles='dashed', linewidths=3)\n",
    "    ax[i].clabel(cs2_fig,  levels=cs_fig.levels[::tot_lev[i]][1::2], fontsize=10, inline=1)\n",
    "\n",
    "    #plot training data X values\n",
    "    ax[i].scatter(X[:,0],X[:,1], color=\"green\",s=25, label = \"Training Data\", marker = \"o\", facecolors='none')  \n",
    "    ax[i].scatter(omega[:,0],omega[:,1], color=\"green\",s=25, label = \"Omega\", marker = \"o\")\n",
    "\n",
    "    #Get legend information\n",
    "    if i == len(z)-1:\n",
    "        handles, labels = ax[i].get_legend_handles_labels()\n",
    "\n",
    "    #Plots axes such that they are scaled the same way (eg. circles look like circles) and name axes\n",
    "    ax[i].axis('scaled')  \n",
    "    ax[i].set_xlabel('$x_1$',weight='bold',fontsize=16)\n",
    "    ax[i].set_ylabel('$x_2$',weight='bold',fontsize=16)\n",
    "\n",
    "    #Plot title and set axis scale\n",
    "    ax[i].set_title(title[i], weight='bold',fontsize=16)\n",
    "    ax[i].set_xlim(left = np.amin(X1), right = np.amax(X1))\n",
    "    ax[i].set_ylim(bottom = np.amin(X2), top = np.amax(X2))      \n",
    "\n",
    "#Plots legend and title\n",
    "plt.tight_layout()\n",
    "fig.legend(handles, labels, loc=\"lower right\", bbox_to_anchor=(-0.01, 0.9), borderaxespad=0)  #bbox_to_anchor=(-0.01, 0.9), borderaxespad=0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a850200",
   "metadata": {},
   "source": [
    "## 5.4.2 Limits of Stantionarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf770fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0,20,100).reshape(-1,1)\n",
    "XX = np.linspace(0,20,100).reshape(-1,1)\n",
    "ytrue =  np.sin((np.pi*X/5)) + 0.2*np.cos(4*np.pi*X/5) *(X<=9.6).reshape(-1,1)\n",
    "lin = X>9.6\n",
    "ytrue[lin] = -1 + X[lin]/10\n",
    "y = ytrue + np.random.normal(size=(len(ytrue))  ,loc = 0, scale = 0.1).reshape(-1,1)\n",
    "\n",
    "kernel = ConstantKernel(constant_value=0.1*np.var(y))*Matern(length_scale = 0.1, nu =1.5) + WhiteKernel(noise_level=0.1)\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=10, n_restarts_optimizer =10)\n",
    "fit_process = gpr.fit(X, y)\n",
    "results = gpr.predict(XX, return_std=True)\n",
    "ll = gpr.log_marginal_likelihood()\n",
    "pm = results[0]\n",
    "psd = results[1]\n",
    "\n",
    "q1 = pm + norm.ppf(0.05, loc=0, scale=psd)\n",
    "q2 = pm + norm.ppf(0.95, loc=0, scale=psd)\n",
    "\n",
    "# #Plot Posterior distribution\n",
    "plt.figure(figsize = (7,6))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(direction=\"in\",top=True, right=True)\n",
    "plt.locator_params(axis='y', nbins=7)\n",
    "plt.locator_params(axis='x', nbins=7)\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "plt.plot(XX, q1,  linestyle='dashed', color = \"red\")\n",
    "plt.plot(XX, q2,  linestyle='dashed', color = \"red\")\n",
    "plt.plot(XX, pm, color = \"black\")\n",
    "plt.scatter(X, y, marker = \"o\", color = \"green\", label = \"Training\", zorder=3)\n",
    "\n",
    "#Set plot details\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0)\n",
    "x_label = \"X-Values\"\n",
    "y_label = \"y-Values\"\n",
    "plt.xlabel(x_label, fontsize=16, fontweight='bold')\n",
    "plt.ylabel(y_label, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aa5905",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = X<9.6\n",
    "right = X>9.6\n",
    "kernel_l = ConstantKernel(constant_value=0.1*np.var(y))*Matern(length_scale = 0.1, nu =1.5) + WhiteKernel(noise_level=0.1)\n",
    "gpr_l = GaussianProcessRegressor(kernel=kernel_l, random_state=10, n_restarts_optimizer =1)\n",
    "fit_process_l = gpr_l.fit(X[left].reshape(-1,1), y[left].reshape(-1,1))\n",
    "\n",
    "kernel_r = ConstantKernel(constant_value=0.1*np.var(y))*Matern(length_scale = 0.1, nu =1.5) + WhiteKernel(noise_level=0.1)\n",
    "gpr_r = GaussianProcessRegressor(kernel=kernel_r, random_state=10, n_restarts_optimizer =1)\n",
    "fit_process_r = gpr_r.fit(X[right].reshape(-1,1), y[right].reshape(-1,1))\n",
    "\n",
    "results_l = gpr_l.predict(X[left].reshape(-1,1), return_std=True)\n",
    "results_r = gpr_r.predict(X[right].reshape(-1,1), return_std=True)\n",
    "\n",
    "pm_l = results_l[0]\n",
    "pm_r = results_r[0]\n",
    "psd_l = results_l[1]\n",
    "psd_r = results_r[1]\n",
    "\n",
    "q1_l = pm_l + norm.ppf(0.05, loc=0, scale=psd_l)\n",
    "q2_l = pm_l + norm.ppf(0.95, loc=0, scale=psd_l)\n",
    "\n",
    "q1_r = pm_r + norm.ppf(0.05, loc=0, scale=psd_r)\n",
    "q2_r = pm_r + norm.ppf(0.95, loc=0, scale=psd_r)\n",
    "\n",
    "# #Plot Posterior distribution\n",
    "plt.figure(figsize = (7,6))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(direction=\"in\",top=True, right=True)\n",
    "plt.locator_params(axis='y', nbins=7)\n",
    "plt.locator_params(axis='x', nbins=7)\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "plt.plot(X[left], q1_l,  linestyle='dashed', color = \"red\")\n",
    "plt.plot(X[left], q2_l,  linestyle='dashed', color = \"red\")\n",
    "plt.plot(X[right], q1_r,  linestyle='dashed', color = \"red\")\n",
    "plt.plot(X[right], q2_r,  linestyle='dashed', color = \"red\")\n",
    "plt.plot(X[left], pm_l, color = \"black\")\n",
    "plt.plot(X[right], pm_r, color = \"black\")\n",
    "plt.scatter(X, y, marker = \"o\", color = \"green\", label = \"Training\", zorder=3)\n",
    "\n",
    "#Set plot details\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0)\n",
    "x_label = \"X-Values\"\n",
    "y_label = \"y-Values\"\n",
    "plt.xlabel(x_label, fontsize=16, fontweight='bold')\n",
    "plt.ylabel(y_label, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
