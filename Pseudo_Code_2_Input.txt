#Pseudo code for all 2-Input GP


Step 1) Generate Experimental and Training data (Completed)

NOTE: 
Number of dimensions of coordinate system: m = 1, 
Number of Experimental data points: n = 5, 
Number of input variables: q = 2
Number of training points generated by LHS: t=t

--Generate Experimental data and training input data--
# Set  x values (m x n), noise_mean (float), and noise_stdev (float)
# noise is drawn from a normal distribution, N(noise_mean, noise_stdev)
# Create experimental data using y = x -x^2 +x^3 + noise (1 x n)
# Generate training data inputs through LHS for Theta1 and Theta2 (q x t)

--Generate training data outputs--
-- 2-Input GP
# Generate training data outputs with 
    ysim = Theta1*x + Theta2*x^2 +x^3 (1 x t)
    Yexp from CSV
    Error = sum((ysim-yexp)**2)


Step 2) Read Experimental and Training Data from the CSV (Completed)

# Read entire file with Pandas and convert to an array
# Create arrays for training/testing inputs and outputs based on data in stored files
# First column is an index, last column is output, all interior columns are inputs


Step 3) Train GP (Completed)

# Define likelihood and GP model clas
# Train GP based on input training data z, and output training data y
# Switch to evaluation mode


Step 4) Generate meshgrid for Theta

#Generate arrays for Theta1 and Theta2
Theta1 = np.linspace(-2,2,# points) (1 x p1)
Theta2 = np.linspace(-2,2,# points) (1 x p2)

#Create meshgrid for theta
theta_mesh = meshgrid (2 p1 x p2 arrays)

#Separate meshgrid into theta1 and theta2 arrays
theta1_mesh = theta_mesh[0] (p1 x p2)
theta2_mesh = theta_mesh[1] (p1 x p2)


Step 5) Calculate Best Error and EI

#Define f(x+)


##WHAT IS f(X+)? Best training data sse sample


#Will compare the rigorous solution and approximation later (multidimensional integral over each experiment using a sparse grid)

#Create an array in which to store expected improvement values
EI = np.zeros((len(p1),len(p2))) (p1 x p2) #Important for step 6
sse = np.zeros((len(p1),len(p2))) #For plotting purposes
# Loop over theta 1
for i in range(len(p1)):
    #Loop over theta2
    for j in range(len(p2)):
    #Evaluate GP at a point p = [Theta1,Theta2,Xexp]
    p = np.array(theta1_mesh[i,j],theta2_mesh[i,j],Xexp[k]) (1 x q)
    GP_mean, GP_var = calc_GP_outputs(model, likelihood, p) (float)
    #Caclulate Best Error
    #Note: Negative sign is here because max(f(x)) = -min(-f(x))
    best_error = max(-train_sse)
    #Calculate expected improvement
    EI[i,j] = calc_ei_basic(best_error, -GP_mean, GP_Var) #GP Mean is negative because max(f(x)) = -min(-f(x))


Step 6) Find GP Predicted Values of Theta
## Find point with lowest Error

#Caclulate argmin of sse: The lowest sse = The optimal point
argmin = np.array(np.where(np.isclose(sse, np.amax(sse),atol=1e-10)==True)) (1 x q)

#Calculate correcsponding theta and x values and set Theta_Opt
Theta1_Opt = float(theta1_mesh[argmin[0],argmin[1]]) (float)
Theta2_Opt = float(theta2_mesh[argmin[0],argmin[1]]) (float)
Theta_GP_Opt = np.array([Theta1_Opt,Theta2_Opt],dtype=object) (1 x q)


Step 7) Complete BO loops

BO_iter = # of iterations
#Loop over BO iterations
    # If training data are numpy arrays, convert to tensors
    if torch.is_tensor(train_T) != True:
        train_p = torch.from_numpy(train_T) (q x t)
    if torch.is_tensor(train_sse) != True:
        train_sse = torch.from_numpy(train_sse) (1 x t)
    
    ## Repeat steps 3-6

    ##Find point with best EI
    #Calculate argmax of EI: Highest EI = Best point to sample
    argmax = np.array(np.where(np.isclose(EI, np.amax(EI),atol=np.amax(EI)*1e-6)==True))
    argmin = np.array(np.where(np.isclose(SSE, np.amax(SSE),np.amax(SSE)*1e-6)==True))

    #Calculate correcsponding theta and x values and set p_Best
    Theta1_Best = float(theta1_mesh[argmax[0],argmax[1]]) (float)
    Theta2_Best = float(theta2_mesh[argmax[0],argmax[1]]) (float)
    Theta_Best = np.array([Theta1_Best,Theta2_Best],dtype=object) (q x 1)
    
    #Calculate corresponding theta and x values and set p_Opt
    Theta1_Opt = float(theta1_mesh[argmin[0],argmin[1]]) (float)
    Theta2_Opt = float(theta2_mesh[argmin[0],argmin[1]]) (float)
    Theta_Opt = np.array([Theta1_Best,Theta2_Best],dtype=object) (q x 1)
    
    #Use argmax/argmin results as initial guesses for scipy.optimize.minimize
    theta_b, theta_o = find_opt_best_scipy(theta_mesh, train_y, Theta_Best,Theta_Opt, sse, ei,model, likelihood, explore_bias)
    
    #Calculate y based on theta_o
    y_GP_Opt = gen_y_Theta_GP(Xexp, theta_o, q, m)
    
    #Calculate total SSE for BO iteration
    Error_mag = np.sum((y_GP_Opt-Yexp)**2)  
    
    ##Plot graphs
    
    ##Append best values to training data 
    #Convert training data to numpy arrays to allow concatenation to work
    train_T = train_T.numpy() (q x t)
    train_sse = train_sse.numpy() (1 x t)

    #Call the expensive function and evaluate at Theta_Best
    sse_Best = create_sse_data(Theta_Best, Xexp, Yexp) (1 x 1)
  
    #Add Theta_Best to train_p and y_best to train_y
    train_p = np.concatenate((train_T, Theta_Best), axis=0) (q x t)
    train_sse = np.concatenate((train_sse, sse_Best),axis=0) (1 x t)
    
 
Step 8)
#Complete Restart Loops
#Read CSV

#Initialize matricies to hold all BO data for all iterations
Theta_matrix = np.zeros((restarts,BO_iters,q))
SSE_matrix = np.zeros((restarts,BO_iters)) 
    
restarts = #
#Loop over restart #
    #Separate and shuffle training/testing data
    train_data, test_data = test_train_split(all_data, shuffle_seed=shuffle_seed)
    train_p = train_data[:,1:(q+1)]
    train_y = train_data[:,-1]
    train_p = train_p[0:t]
    train_y = train_y[0:t]
    
    #Complete step 7
    
    #Add results to initialized matricies
    #Plot all SSE/theta results for each BO iteration for all restarts

#Find restart with lowest optimal SSE for solution
(Use argmin)

            
QUESTIONS:


Answered Questions:
In the pseudo code you drew on the whiteboard, best error is called as best_error[k] in the EI equation. (Best error is 1xn)
1) What is f_bar and what is f(x) and are they both (1xn)? (f_bar = Yexp, f(x) approx. GP_mean)
2) When you add in a sample from the meshgrid do you retrain the GP for each set and apply it? (No)
3) Should there be a sum in the calculation of best error loop? (No we assume they act independently and summing them together in EI)
4) What is f(x+) - The Best error which is max(-train_sse)       
            
            
            
            
            
            
            
            
