Date and Time:  28-Apr-2023 (11:05:00)
Date and Time Saved:  2023/04/28/11-05
Case Study:  2.2
Number of Training Thetas:  [20, 200, 600, 1000]
Number of Experimental Data Points:  30
GP Emulating Function Output (T) or SSE (F)?  True
Scaling of Objective Function?  obj
Bounds On X Cut (T) or Normal (F)?  True
Dense Grid for Xexp? True
Evaluating Near Test Point (T) or True Parameter Set (F)?  True
GP Training Package:  scikit_learn
GP Training Iterations (Gpytorch only):  300
GP Kernel Function:  Mat_52
GP Kernel lengthscale:  1
GP Training Restarts (when lengthscale/outputscale not set):  10
Training Data Noise st.dev:  0.01
Number of Values per Parameter:  101
Xexp Points Indecies Evaluated:  [2, 12, 23, 26]


All Data Path:  Input_CSVs/Train_Data/d=8/all_emul_data/t=600_cut_bounds_dense.csv
All Data Path:  Input_CSVs/Train_Data/d=8/all_emul_data/t=6000_cut_bounds_dense.csv
All Data Path:  Input_CSVs/Train_Data/d=8/all_emul_data/t=18000_cut_bounds_dense.csv
All Data Path:  Input_CSVs/Train_Data/d=8/all_emul_data/t=30000_cut_bounds_dense.csv
GP Kernel has outputscale?:  False
Training Thetas Used:  [20.0, 200.0, 600.0, 1000.0]
Theta Train for Sensitivity: [-1.234012 -0.609235 -6.084443 -1.11762  -1.434347 -0.337736 12.242987
 -0.577918]
Theta Train for Sensitivity: [ 0.47334  -1.660774 -0.464505  0.791295  1.698514  0.862347 10.645607
 -1.959774]
Theta Train for Sensitivity: [-1.17405  -1.657363 -9.801574 -1.555507  0.656785  0.016924 14.569803
 -0.183648]
Theta Train for Sensitivity: [-0.917405 -0.685736 -1.130157  1.924449 -1.374322 -0.378308 14.30698
  1.701635]


GP Kernel has outputscale?:  True
Training Thetas Used:  [20.0, 200.0, 600.0, 1000.0]
Theta Train for Sensitivity: [-1.234012 -0.609235 -6.084443 -1.11762  -1.434347 -0.337736 12.242987
 -0.577918]
Theta Train for Sensitivity: [ 0.47334  -1.660774 -0.464505  0.791295  1.698514  0.862347 10.645607
 -1.959774]
Theta Train for Sensitivity: [-1.17405  -1.657363 -9.801574 -1.555507  0.656785  0.016924 14.569803
 -0.183648]
Theta Train for Sensitivity: [-0.917405 -0.685736 -1.130157  1.924449 -1.374322 -0.378308 14.30698
  1.701635]


