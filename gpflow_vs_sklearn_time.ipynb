{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from scipy.stats import qmc\n",
    "import itertools\n",
    "from itertools import combinations_with_replacement, combinations, permutations\n",
    "import copy\n",
    "\n",
    "import bo_methods_lib\n",
    "# from bo_methods_lib.bo_methods_lib.bo_functions_generic import gen_theta_set, clean_1D_arrays\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Classes_New import * #Fix this later\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Class_fxns import * #Fix this later\n",
    "from bo_methods_lib.bo_methods_lib.analyze_data import * #Fix this later\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Classes_plotters import * #Fix this later\n",
    "import pympler\n",
    "import pickle\n",
    "\n",
    "from pympler import asizeof\n",
    "import gpflow\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024984087640608554\n"
     ]
    }
   ],
   "source": [
    "#Make Simulator and Training Data\n",
    "cs_name_val = 10\n",
    "noise_mean = 0\n",
    "noise_std = None\n",
    "seed = 1\n",
    "retrain_GP = 5\n",
    "normalize = True\n",
    "#Define method, ep_enum classes, indecies to consider, and kernel\n",
    "meth_name = Method_name_enum(3)\n",
    "method = GPBO_Methods(meth_name)\n",
    "gen_meth_theta = Gen_meth_enum(1)\n",
    "gen_meth_x = Gen_meth_enum(2)\n",
    "num_x_data = 5\n",
    "\n",
    "#Define Simulator Class (Export your Simulator Object Here)\n",
    "simulator = simulator_helper_test_fxns(cs_name_val, noise_mean, noise_std, seed)\n",
    "num_theta_data = len(simulator.indeces_to_consider)*10\n",
    "#Generate Exp Data (OR Add your own experimental data as a Data class object)\n",
    "set_seed = 1 #Set set_seed to 1 for data generation\n",
    "gen_meth_x = Gen_meth_enum(gen_meth_x)\n",
    "exp_data = simulator.gen_exp_data(num_x_data, gen_meth_x, set_seed)\n",
    "#Set simulator noise_std artifically as 5% of y_exp mean (So that noise will be set rather than trained)\n",
    "simulator.noise_std = np.abs(np.mean(exp_data.y_vals))*0.05\n",
    "print(simulator.noise_std)\n",
    "#Note at present, training data is always the same between jobs since we set the data generation seed to 1\n",
    "sim_data = simulator.gen_sim_data(num_theta_data, num_x_data, gen_meth_theta, gen_meth_x, 1.0, seed, False)\n",
    "val_data = simulator.gen_sim_data(10, 10, Gen_meth_enum(1), Gen_meth_enum(1), 1.0, seed + 1, False)\n",
    "# print(sim_data.theta_vals)\n",
    "\n",
    "noise_std = simulator.noise_std #Yexp_std is exactly the noise_std of the GP Kernel\n",
    "\n",
    "if method.emulator == True:\n",
    "    all_gp_data = sim_data\n",
    "    all_val_data = val_data\n",
    "    gp_object = Type_2_GP_Emulator(all_gp_data, all_val_data, None, None, None, Kernel_enum(1), None, noise_std, None, \n",
    "                                retrain_GP, seed, normalize, None, None, None, None)\n",
    "else:\n",
    "    all_gp_data = simulator.sim_data_to_sse_sim_data(method, sim_data, exp_data, 1.0, False)\n",
    "    print(len(all_gp_data.theta_vals), len(all_gp_data.x_vals), len(all_gp_data.y_vals))\n",
    "    all_val_data = simulator.sim_data_to_sse_sim_data(method, val_data, exp_data, 1.0, False)\n",
    "    print(len(all_val_data.theta_vals), len(all_val_data.x_vals), len(all_val_data.y_vals))\n",
    "    gp_object = Type_1_GP_Emulator(all_gp_data, all_val_data, None, None, None, Kernel_enum(1), None, noise_std, None, \n",
    "                                retrain_GP, seed, normalize, None, None, None, None)\n",
    "    \n",
    "train_data, test_data = gp_object.set_train_test_data(1.0, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up scalers\n",
    "if gp_object.normalize:\n",
    "    gp_object.scalerY.fit(gp_object.train_data.y_vals.reshape(-1,1))\n",
    "    sclr = float(gp_object.scalerY.scale_)\n",
    "    gp_object.scalerX = gp_object.scalerX.fit(gp_object.train_data_init)\n",
    "else:\n",
    "    sclr = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up for sklearn kernel\n",
    "if normalize:\n",
    "    points = gp_object.scalerX.transform(gp_object.train_data_init)\n",
    "else:\n",
    "    points = gp_object.train_data_init\n",
    "\n",
    "# Compute pairwise Euclidean distances between all points\n",
    "pairwise_distances = np.sqrt(np.sum((points[:, None] - points) ** 2, axis=-1))\n",
    "\n",
    "# Set the diagonal elements (self-distances) to infinity\n",
    "np.fill_diagonal(pairwise_distances, np.inf)\n",
    "# Mask out infinite values so that np.max will ignore them\n",
    "pairwise_distances = np.ma.masked_invalid(pairwise_distances)\n",
    "\n",
    "# Find the maximum and minimum distances between points\n",
    "max_distance = np.max(pairwise_distances)\n",
    "min_distance = np.min(pairwise_distances)\n",
    "\n",
    "#Set max lenscl as the smaller of the max distance or 100\n",
    "lenscl_1 = min(max_distance, 100)\n",
    "#Set min lenscl as the larger of the min distance or 0.01\n",
    "lenscl_2 = max(min_distance, 1e-2)\n",
    "#Ensure min lenscl < max lenscl and that bounds are floats\n",
    "min_lenscl = float(np.minimum(lenscl_1, lenscl_2))\n",
    "max_lenscl = float(np.maximum(lenscl_1, lenscl_2))\n",
    "lenscl_bnds = min_lenscl, max_lenscl\n",
    "\n",
    "#Set the noise guess or allow gp to tune the noise parameter\n",
    "if gp_object.noise_std is not None:\n",
    "    #If we know the noise, use it\n",
    "    noise_guess_org = (gp_object.noise_std/sclr)**2\n",
    "    #Set the noise as the higher of noise_guess and 1e-10\n",
    "    noise_guess = np.maximum(1e-10, noise_guess_org)\n",
    "    noise_kern = WhiteKernel(noise_level=noise_guess, noise_level_bounds= \"fixed\")\n",
    "else:\n",
    "    #Otherwise, set the guess as 5% the taining data mean\n",
    "    data_mean = np.abs(np.mean(gp_object.gp_sim_data.y_vals))\n",
    "    noise_guess = data_mean*0.05/sclr\n",
    "    #Set the min noise as 1% of the data mean or 1e-3 (minimum) and max as 10% or (1e-2 maximum)\n",
    "    noise_min = max(data_mean*0.01/sclr, 1e-3)\n",
    "    noise_max = noise_min*10 \n",
    "    #Ensure the guess is in the bounds, if not, use the mean of the max and min\n",
    "    if not noise_min <= noise_guess <= noise_max:\n",
    "        noise_guess = (noise_min+noise_max)/2\n",
    "    noise_kern = WhiteKernel(noise_level= noise_guess**2, noise_level_bounds= (noise_min**2, noise_max**2))\n",
    "\n",
    "#If normalizing data\n",
    "if gp_object.normalize:\n",
    "    #Scale will be the average \n",
    "    train_y_scl = gp_object.scalerY.transform(gp_object.train_data.y_vals.reshape(-1,1))\n",
    "    #Scaled bounds on C\n",
    "    mse = sum(train_y_scl.flatten()**2)/len(train_y_scl.flatten())\n",
    "    c_max = np.maximum(3, mse)\n",
    "    c_min = np.minimum(1e-2, mse)\n",
    "    c_bnds = (c_min,c_max)\n",
    "    if c_bnds[0] < 1.0 < c_bnds[1]:\n",
    "        c_guess = 1.0\n",
    "    else:\n",
    "        c_guess = (c_max + c_min) /2\n",
    "else:\n",
    "    #For unscaled data, this distance on the mean is dependent of the data\n",
    "    c_bnds = (1e-2,1e2)\n",
    "    c_guess = 1.0\n",
    "\n",
    "cont_kern = ConstantKernel(constant_value = c_guess, constant_value_bounds=c_bnds)\n",
    "\n",
    "if gp_object.kernel.value == 3: #RBF\n",
    "    kernel = cont_kern*RBF(length_scale_bounds=(lenscl_bnds)) + noise_kern\n",
    "elif gp_object.kernel.value == 2: #Matern 3/2\n",
    "    kernel = cont_kern*Matern(length_scale_bounds=(lenscl_bnds), nu=1.5) + noise_kern \n",
    "else: #Matern 5/2\n",
    "    kernel = cont_kern*Matern(length_scale_bounds=(lenscl_bnds), nu=2.5) + noise_kern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Sklearn and GPFLow Models\n",
    "#GPFLOW\n",
    "gpflow_model = gp_object.set_gp_model()\n",
    "sklearn_kern = kernel\n",
    "\n",
    "if isinstance(gp_object.lenscl, np.ndarray) and all(var is not None for var in gp_object.lenscl) and gp_object.outputscl != None:\n",
    "    optimizer = None\n",
    "elif isinstance(gp_object.lenscl, (float, int)) and gp_object.lenscl != None and gp_object.outputscl != None:\n",
    "    optimizer = None\n",
    "else:\n",
    "    optimizer = \"fmin_l_bfgs_b\"\n",
    "sklearn_model = GaussianProcessRegressor(kernel=kernel, alpha=1e-10, n_restarts_optimizer=gp_object.retrain_GP, \n",
    "                                            random_state = gp_object.seed, optimizer = optimizer, normalize_y = not gp_object.normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.1 s, sys: 3.14 s, total: 38.2 s\n",
      "Wall time: 6.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "choice = \"gpflow\"\n",
    "#Train gpflow model\n",
    "if choice == \"gpflow\":\n",
    "    gp_object.train_gp(gpflow_model)\n",
    "else:\n",
    "    #Train sklearn Model\n",
    "    #Train GP\n",
    "    #Preprocess Training data\n",
    "    if gp_object.normalize == True:\n",
    "        #Update scaler to be the fitted scaler. This scaler will change as the training data is updated\n",
    "        gp_object.scalerX = gp_object.scalerX.fit(gp_object.feature_train_data)\n",
    "        gp_object.scalerY = gp_object.scalerY.fit(gp_object.train_data.y_vals.reshape(-1,1))\n",
    "        #Scale training data if necessary\n",
    "        feature_train_data_scaled = gp_object.scalerX.transform(gp_object.feature_train_data)\n",
    "        y_train_data_scaled = gp_object.scalerY.transform(gp_object.train_data.y_vals.reshape(-1,1))\n",
    "    else:\n",
    "        feature_train_data_scaled = gp_object.feature_train_data\n",
    "        y_train_data_scaled = gp_object.train_data.y_vals.reshape(-1,1)\n",
    "\n",
    "    #Fit GP Model\n",
    "    fit_gp_model = sklearn_model.fit(feature_train_data_scaled, y_train_data_scaled)\n",
    "\n",
    "    #Pull out kernel parameters after GP training\n",
    "    opt_kern_params = fit_gp_model.kernel_\n",
    "    outputscl_final = opt_kern_params.k1.k1.constant_value\n",
    "    lenscl_final = opt_kern_params.k1.k2.length_scale\n",
    "    noise_final = opt_kern_params.k2.noise_level\n",
    "\n",
    "    #Put hyperparameters in a list\n",
    "    trained_hyperparams = [lenscl_final, noise_final, outputscl_final] \n",
    "\n",
    "    #Assign self parameters\n",
    "    gp_object.trained_hyperparams = trained_hyperparams\n",
    "    gp_object.fit_gp_model = fit_gp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 550 ms, sys: 131 ms, total: 681 ms\n",
      "Wall time: 201 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Predict values\n",
    "if choice == \"gpflow\":\n",
    "    misc_gp_mean, misc_var_return = gp_object.eval_gp_mean_var_val(covar = False)\n",
    "else:\n",
    "    data = gp_object.featurize_data(all_val_data)\n",
    "    if len(data.shape) < 2:\n",
    "        data.reshape(1,-1)\n",
    "    #scale eval_point if necessary\n",
    "    if gp_object.normalize == True:\n",
    "        eval_points = gp_object.scalerX.transform(data)\n",
    "    else:\n",
    "        eval_points = data\n",
    "    \n",
    "    #Evaluate GP given parameter set theta and state point value\n",
    "    # print(self.fit_gp_model.kernel_)\n",
    "    gp_mean_scl, gp_covar_scl = gp_object.fit_gp_model.predict(eval_points, return_cov=True)\n",
    "\n",
    "    #Unscale gp_mean and gp_covariance\n",
    "    if gp_object.normalize == True:\n",
    "        gp_mean = gp_object.scalerY.inverse_transform(gp_mean_scl.reshape(-1,1)).flatten()\n",
    "        gp_covar = float(gp_object.scalerY.scale_**2) * gp_covar_scl  \n",
    "    else:\n",
    "        gp_mean = gp_mean_scl\n",
    "        gp_covar = gp_covar_scl\n",
    "    \n",
    "    gp_var = np.diag(gp_covar)\n",
    "    misc_gp_mean = gp_mean\n",
    "    misc_var_return = gp_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Toy_Problem_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
