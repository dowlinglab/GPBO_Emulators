{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from scipy.stats import qmc\n",
    "import itertools\n",
    "from itertools import combinations_with_replacement, combinations, permutations\n",
    "\n",
    "import bo_methods_lib\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Classes_New import * #Fix this later\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Class_fxns import * #Fix this later\n",
    "from bo_methods_lib.bo_methods_lib.analyze_data import * #Fix this later\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Classes_plotters import * #Fix this later\n",
    "import pympler\n",
    "import pickle\n",
    "import signac\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from textwrap import fill\n",
    "import tensorflow_probability as tfp\n",
    "import gpflow\n",
    "\n",
    "from pympler import asizeof\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 200\n",
    "\n",
    "#Ignore inconcistent version warning\n",
    "import warnings\n",
    "# from sklearn.exceptions import InconsistentVersionWarning\n",
    "# warnings.filterwarnings(action='ignore', category=InconsistentVersionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Simulator and Training Data\n",
    "cs_name_val = 2\n",
    "noise_mean = 0\n",
    "noise_std = None\n",
    "seed = 1\n",
    "#Define method, ep_enum classes, indecies to consider, and kernel\n",
    "meth_name = Method_name_enum(3)\n",
    "method = GPBO_Methods(meth_name)\n",
    "gen_meth_theta = Gen_meth_enum(1)\n",
    "gen_meth_x = Gen_meth_enum(2)\n",
    "num_x_data = 5\n",
    "\n",
    "#Define Simulator Class (Export your Simulator Object Here)\n",
    "simulator = simulator_helper_test_fxns(cs_name_val, noise_mean, noise_std, seed)\n",
    "num_theta_data = len(simulator.indeces_to_consider)*10\n",
    "#Generate Exp Data (OR Add your own experimental data as a Data class object)\n",
    "set_seed = 1 #Set set_seed to 1 for data generation\n",
    "gen_meth_x = Gen_meth_enum(gen_meth_x)\n",
    "exp_data = simulator.gen_exp_data(num_x_data, gen_meth_x, set_seed)\n",
    "#Set simulator noise_std artifically as 5% of y_exp mean (So that noise will be set rather than trained)\n",
    "simulator.noise_std = np.abs(np.mean(exp_data.y_vals))*0.05\n",
    "print(simulator.noise_std)\n",
    "#Note at present, training data is always the same between jobs since we set the data generation seed to 1\n",
    "sim_data = simulator.gen_sim_data(num_theta_data, num_x_data, gen_meth_theta, gen_meth_x, 1.0, seed, False)\n",
    "val_data = simulator.gen_sim_data(10, 10, Gen_meth_enum(1), Gen_meth_enum(1), 1.0, simulator.seed, False)\n",
    "# print(sim_data.theta_vals)\n",
    "all_gp_data = sim_data\n",
    "all_val_data = val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "retrain_GP = 5\n",
    "normalize = True\n",
    "noise_std = simulator.noise_std #Yexp_std is exactly the noise_std of the GP Kernel\n",
    "\n",
    "#Get GP object\n",
    "gp_object = Type_2_GP_Emulator(all_gp_data, all_val_data, None, None, None, Kernel_enum(1), None, noise_std, None, \n",
    "                                retrain_GP, seed, normalize, None, None, None, None)\n",
    "#Train on all sets\n",
    "train_data, test_data = gp_object.set_train_test_data(1.0, seed)\n",
    "new_gp_model = gp_object.set_gp_model()\n",
    "gp_object.train_gp()\n",
    "gpflow.utilities.print_summary(gp_object.fit_gp_model)\n",
    "hps_org = gp_object.trained_hyperparams\n",
    "print(hps_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Data Class which is Sim and Val Data together\n",
    "t_train_val = np.concatenate((sim_data.theta_vals, val_data.theta_vals))\n",
    "x_train_val = np.concatenate((sim_data.x_vals, val_data.x_vals))\n",
    "y_train_val = np.concatenate((sim_data.y_vals, val_data.y_vals))\n",
    "\n",
    "all_data = Data(t_train_val, x_train_val, y_train_val, None, None, None, None, None, \n",
    "all_gp_data.bounds_theta, all_gp_data.bounds_x, 1.0, seed)\n",
    "\n",
    "X = gp_object.featurize_data(all_data)\n",
    "y = all_data.y_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_LOO(all_data, gp_object):\n",
    "    n_samples = len(all_data.y_vals)\n",
    "\n",
    "    predict_mean = []\n",
    "    predict_std = []\n",
    "    train_mean = []\n",
    "    train_std = []\n",
    "    seed = gp_object.seed\n",
    "    hps_org = gp_object.trained_hyperparams\n",
    "\n",
    "    #For each sample in the full set\n",
    "    for i in range(n_samples):\n",
    "        #Get sample Data\n",
    "        t_leave_one = np.atleast_2d(all_data.theta_vals[i])\n",
    "        X_leave_one = np.atleast_2d(all_data.x_vals[i])\n",
    "        y_leave_one = np.atleast_1d(all_data.y_vals[i])\n",
    "        t_rest = np.delete(all_data.theta_vals,i,axis=0)\n",
    "        X_rest = np.delete(all_data.x_vals,i,axis=0)\n",
    "        y_rest = np.delete(all_data.y_vals,i,axis=0)\n",
    "        loo_data = Data(t_leave_one, X_leave_one, y_leave_one, None, None, None, None, None, \n",
    "                                all_data.bounds_theta, all_data.bounds_x, 1.0, seed)\n",
    "        \n",
    "        loo_data_train = Data(t_rest, X_rest, y_rest, None, None, None, None, None, \n",
    "                                all_data.bounds_theta, all_data.bounds_x, 1.0, seed)\n",
    "        \n",
    "        #Create GP object based on the above\n",
    "        gp_new = Type_2_GP_Emulator(loo_data_train, loo_data, None, loo_data_train, None, Kernel_enum(1), None, noise_std, None, \n",
    "                                    1, seed, normalize, None, None, None, None)\n",
    "        gp_new.scalerX = gp_object.scalerX\n",
    "        gp_new.scalerY = gp_object.scalerY\n",
    "        \n",
    "        #Create GP Model Based On Past Hyperparamaters\n",
    "        loo_ft = gp_new.featurize_data(loo_data)\n",
    "        loo_ft_trn = gp_new.featurize_data(loo_data_train)\n",
    "        looft_scl = gp_new.scalerX.transform(loo_ft_trn)\n",
    "        scl_looy = gp_new.scalerY.transform(y_rest.reshape(-1,1))\n",
    "        gp_new.train_data_init = loo_ft_trn\n",
    "        mat_kern = gpflow.kernels.Matern52(variance = hps_org[2], lengthscales=hps_org[0])\n",
    "        noise_kern = gpflow.kernels.White(variance=hps_org[1])\n",
    "        lik_noise_var = np.maximum(1.000001e-6, float(gp_object.fit_gp_model.likelihood.variance.numpy()))\n",
    "        kernel = mat_kern + noise_kern\n",
    "        new_gp_model = gpflow.models.GPR((looft_scl, scl_looy), kernel=kernel, noise_variance=lik_noise_var)\n",
    "        for param in new_gp_model.trainable_parameters:\n",
    "            gpflow.set_trainable(param, False)\n",
    "        gp_new.fit_gp_model = new_gp_model\n",
    "        gp_new.feature_train_data = loo_ft_trn\n",
    "\n",
    "        #Predict With New model\n",
    "        #Get data in vector form into array form\n",
    "        if len(loo_ft.shape) < 2:\n",
    "            loo_ft.reshape(1,-1)\n",
    "        #scale eval_point if necessary\n",
    "        if gp_new.normalize == True:\n",
    "            eval_points = gp_new.scalerX.transform(loo_ft)\n",
    "        else:\n",
    "            eval_points = loo_ft\n",
    "        \n",
    "        #Evaluate GP given parameter set theta and state point value\n",
    "        gp_mean_scl, gp_covar_scl = gp_new.fit_gp_model.predict_f(eval_points, full_cov=True)\n",
    "        #Remove dimensions of 1\n",
    "        gp_mean_scl = gp_mean_scl.numpy()\n",
    "        gp_covar_scl = np.squeeze(gp_covar_scl, axis = 0)\n",
    "\n",
    "        #Unscale gp_mean and gp_covariance\n",
    "        if gp_new.normalize == True:\n",
    "            gp_mean = gp_new.scalerY.inverse_transform(gp_mean_scl.reshape(-1,1)).flatten()\n",
    "            gp_covar = float(gp_new.scalerY.scale_**2) * gp_covar_scl  \n",
    "        else:\n",
    "            gp_mean = gp_mean_scl\n",
    "            gp_covar = gp_covar_scl\n",
    "        \n",
    "        y_loo_mean = gp_mean\n",
    "        y_loo_var = np.diag(gp_covar)\n",
    "        y_loo_std =  np.sqrt(y_loo_var)\n",
    "\n",
    "        #Check for feat_loo in train and test feat\n",
    "        is_row_train = np.any(np.all(gp_object.feature_train_data == loo_ft, axis=1))\n",
    "        if is_row_train:\n",
    "            train_mean.append(float(y_loo_mean))\n",
    "            train_std.append(float(y_loo_std))\n",
    "        else:\n",
    "            predict_mean.append(float(y_loo_mean))\n",
    "            predict_std.append(float(y_loo_std))\n",
    "\n",
    "    return np.array(predict_mean), np.array(predict_std), np.array(train_mean), np.array(train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_mean, predict_std, train_mean, train_std = perform_LOO(all_data, gp_object)\n",
    "# R2_score = r2_score(val_data.y_vals, predict_mean)\n",
    "print(predict_mean)\n",
    "print(predict_std)\n",
    "# print(R2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_info(kernel):\n",
    "    outputscl_final = float(kernel.kernels[0].variance.numpy())\n",
    "    lenscl_final = kernel.kernels[0].lengthscales.numpy()\n",
    "    noise_final = float(kernel.kernels[1].variance.numpy())\n",
    "\n",
    "    if isinstance(kernel.kernels[0], gpflow.kernels.RBF):\n",
    "        kern_type = \"RBF\"\n",
    "    elif isinstance(kernel.kernels[0], gpflow.kernels.Matern32):\n",
    "        kern_type = \"Mat32\"\n",
    "    elif isinstance(kernel.kernels[0], gpflow.kernels.Matern52):\n",
    "        kern_type = \"Mat52\"\n",
    "    else:\n",
    "        print(type(kernel))\n",
    "\n",
    "    if isinstance(lenscl_final, np.ndarray):\n",
    "        lenscl_str = '[' + ', '.join('{:.2g}'.format(x) for x in lenscl_final) + ']'\n",
    "    else:\n",
    "        lenscl_str = f\"{lenscl_final:.2g}\"\n",
    "    \n",
    "    info_str = \"tau=\" +'{:.3g}'.format(outputscl_final) + \", \"+ str(kern_type) +\"=\"+ lenscl_str + \", noise=\"'{:.2g}'.format(noise_final)\n",
    "    return info_str\n",
    "\n",
    "print(kernel_info(gp_object.fit_gp_model.kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'LOO-$R^2$ score: ' + str(np.round(R2_score,2))\n",
    "\n",
    "plt.errorbar(sim_data.y_vals, train_mean, 1.96*np.array(train_std), c='g', fmt = ' ', alpha = 0.3, zorder =1)\n",
    "plt.errorbar(val_data.y_vals, predict_mean, 1.96*np.array(predict_std), c='b', fmt = ' ', alpha = 0.3, zorder=2)\n",
    "plt.scatter(sim_data.y_vals, train_mean, 30,c='g', marker='D', label='LOO-Train', zorder= 3)\n",
    "plt.scatter(val_data.y_vals, predict_mean, 30,c='b', marker='D', label='LOO-Test', zorder = 4)\n",
    "\n",
    "plt.plot(y,y,'k--', label='parity line')\n",
    "plt.xlabel('Experimental y')\n",
    "plt.ylabel('Predicted y')\n",
    "plt.title(fill(\"CS \" + str(cs_name_val) + \": \" + kernel_info(gp_object.fit_gp_model.kernel), 60), fontdict={'size':15})\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize='x-large')\n",
    "# plt.text(1500, 100, text)\n",
    "# plt.text(1000, 0, '*Error Bars are 1.96 Standard Deviation')\n",
    "# plt.savefig('pic_Loo_R2_score')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Toy_Problem_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
