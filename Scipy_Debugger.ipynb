{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d566f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.stats import qmc\n",
    "from bo_methods_lib.bo_functions_generic import round_time, gen_theta_set,gen_x_set, find_train_doc_path, set_ep, clean_1D_arrays\n",
    "from bo_methods_lib.GP_Vs_True_Sensitivity import Muller_plotter\n",
    "from bo_methods_lib.GP_Vs_True_Param_Sens import mul_plot_param\n",
    "# from .CS1_create_data import gen_y_Theta_GP, calc_y_exp, create_y_data\n",
    "from bo_methods_lib.CS2_create_data import gen_y_Theta_GP, calc_y_exp, create_y_data\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, WhiteKernel, ConstantKernel\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e6f8dc",
   "metadata": {},
   "source": [
    "# Section 1) Setting Parameters/ Defining the Problem\n",
    "\n",
    "### Relavent Files: \n",
    "\n",
    "Training Data: Input_CSVs/Train_Data/d=8/all_emul_data/t=600_cut_bounds_dense.csv\n",
    "\n",
    "Experimental Data: Input_CSVs/Exp_Data/d=2/n=30_cut_bounds_dense.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875160c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Parameters\n",
    "CS = 2.2 #Case Study\n",
    "DateTime = None #Date and Time\n",
    "\n",
    "Bound_Cut = True #Defines whether original bounds are cut\n",
    "denseX = True\n",
    "eval_Train = True #Defines whether we are examining close to the true parameter set or the first training parameter set\n",
    "\n",
    "#Consatnts for the Muller Potential\n",
    "Constants = np.array([[-200,-100,-170,15],\n",
    "                      [-1,-1,-6.5,0.7],\n",
    "                      [0,0,11,0.6],\n",
    "                      [-10,-10,-6.5,0.7],\n",
    "                      [1,0,-0.5,-1],\n",
    "                      [0,0.5,1.5,1]])\n",
    "\n",
    "#Case Study 2.2 defined as the 8 parameter Muller potential\n",
    "if CS == 2.2:\n",
    "    Constants = np.array([[-200,-100,-170,15],\n",
    "                          [-1,-1,-6.5,0.7],\n",
    "                          [0,0,11,0.6],\n",
    "                          [-10,-10,-6.5,0.7],\n",
    "                          [1,0,-0.5,-1],\n",
    "                          [0,0.5,1.5,1]])\n",
    "\n",
    "    skip_param_types = 1 #This is what changes for subpoint\n",
    "    true_p = Constants[skip_param_types:skip_param_types+2].flatten()\n",
    "    param_dict = {0 : 'a_1', 1 : 'a_2', 2 : 'a_3', 3 : 'a_4',\n",
    "                  4 : 'b_1', 5 : 'b_2', 6 : 'b_3', 7 : 'b_4'}\n",
    "    exp_d = 2\n",
    "\n",
    "    if Bound_Cut == True:\n",
    "        bounds_x = np.array([[-1.0, 0.0],\n",
    "                            [   0.5, 1.5]])\n",
    "        if denseX == True:\n",
    "            n = 30\n",
    "        else:\n",
    "            n = 25 #Number of experimental data points to use\n",
    "    else:    \n",
    "        bounds_x = np.array([[-1.5, -0.5],\n",
    "                     [   1,    2]])\n",
    "        n = 27 #Number of experimental data points to use\n",
    "    bounds_p = np.array([[-2, -2, -10, -2, -2, -2,  5, -2],\n",
    "                   [ 2,  2,   0,  2,  2,  2, 15,  2]])\n",
    "    minima = np.array([[-0.558,1.442],\n",
    "                  [-0.050,0.467],\n",
    "                  [0.623,0.028]])\n",
    "\n",
    "    saddle = np.array([[-0.82,0.62],\n",
    "                  [0.22,0.30]])\n",
    "\n",
    "#Case Study 1 is the cubic equation toy problem. This code will not work with it. \n",
    "#This is just an example for when we add CS 2.3, 2.4 etc. which have more parameters\n",
    "else:\n",
    "    Constants = true_p = np.array([1,-1])\n",
    "    skip_param_types = 0\n",
    "    param_dict = {0 : '\\\\theta_1', 1 : '\\\\theta_2'}\n",
    "    exp_d = 1\n",
    "    n = 5\n",
    "    bounds_x = np.array([[-2], [2]])\n",
    "    bounds_p = np.array([[-2, -2],\n",
    "                         [ 2,  2]])\n",
    "\n",
    "package = \"scikit_learn\" #Package training the GP model\n",
    "t = 20 #Number of parameter training sets\n",
    "# percentiles = np.linspace(-1.0,1.0,41)\n",
    "percentiles = np.linspace(0,0,1) #Percent by which to deviate from a given parameter set\n",
    "value_num = 101 #Number of parameter values for each parameter to evaluate within the bounds\n",
    "d = len(true_p) #Dimensionality of parameter space\n",
    "kernel_func = \"Mat_52\" #Kernel function to use\n",
    "train_iter = 300 #Maximum GP training iterations\n",
    "initialize = 2 #Number of times to restart GP training\n",
    "noise_std = 0.01 #Noise associated with actual data\n",
    "set_lenscl = None #lengthscale of the kernel (None means it will be optimized)\n",
    "outputscl = True #Outputscale will be optimized\n",
    "verbose = False\n",
    "rand_seed = False #Whether or not a random seed it used for RNG events\n",
    "\n",
    "emulator = True #GP directly emulates function\n",
    "obj = \"obj\"\n",
    "\n",
    "save_figure = True #Whether to save figures\n",
    "save_figure = False\n",
    "save_csvs = True #Whether to save CSVs\n",
    "save_csvs = False\n",
    "\n",
    "#Naming convention to pull experimental and training data from CSVs\n",
    "if Bound_Cut == True:\n",
    "    cut_bounds = '_cut_bounds'\n",
    "else:\n",
    "    cut_bounds = \"\"\n",
    "if denseX == True:\n",
    "    dense = \"_dense\"\n",
    "else:\n",
    "    dense = \"\"\n",
    "\n",
    "#Pull Experimental data from CSV\n",
    "exp_data_doc = 'Input_CSVs/Exp_Data/d='+str(exp_d)+'/n='+str(n)+cut_bounds+dense+'.csv'\n",
    "exp_data = np.array(pd.read_csv(exp_data_doc, header=0,sep=\",\"))\n",
    "Xexp = exp_data[:,1:exp_d+1]\n",
    "Yexp = exp_data[:,-1]\n",
    "Xexp = clean_1D_arrays(Xexp)\n",
    "m = Xexp.shape[1]\n",
    "\n",
    "#Pull training data from CSV\n",
    "t_use = int(t*n)\n",
    "all_data_doc = find_train_doc_path(emulator, obj, d, t_use, Bound_Cut, denseX)\n",
    "all_data = np.array(pd.read_csv(all_data_doc, header=0,sep=\",\"))\n",
    "X_train = torch.tensor(all_data[:,1:-m+1]).float() #8 or 10 (emulator) parameters\n",
    "Y_train = train_y = torch.tensor(all_data[:,-1]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538d908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set meshgrid values and define a meshgrid over X_space\n",
    "p = 20\n",
    "X1 =  np.linspace(bounds_x[0,0],bounds_x[1,0],p) \n",
    "X2 =  np.linspace(bounds_x[0,1],bounds_x[1,1],p) \n",
    "X_mesh = np.array(np.meshgrid(X1, X2)) \n",
    "#Generate the same points as X_mesh in a 2D array\n",
    "X_space = gen_x_set(LHS = False, n_points = p, dimensions = exp_d, bounds = bounds_x) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71913abf",
   "metadata": {},
   "source": [
    "# Section 2) Building/ Training the Model\n",
    "\n",
    "### Relavent Files: \n",
    "\n",
    "Heat Maps Generated are saved to this location when save_CSVS = True: Test_Figs/Figures/GP_Vs_Sim_Comp_CB/CS_2.2/train_iter_300/TP_20/scikit_learn/Mat_52/len_scl_varies/Mul_Comp_Figs.png\n",
    "\n",
    "Plotting function for above in bo_methods_lib/GP_Vs_True_Sensitivity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f2de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GP_scikit(train_param, train_data, noise_std = 0, kern = \"Mat_52\", verbose=False, \n",
    "                    set_lenscl = None, outputscl = False, initialize = 1, rand_seed = False):\n",
    "    \"\"\"\n",
    "    Trains the GP model and finds hyperparameters with the scikit learn package\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        train_param: tensor or ndarray, The training parameter space data\n",
    "        train_data: tensor or ndarray, The training y data\n",
    "        iterations: float or int, number of training iterations to run. Default is 300\n",
    "        verbose: Set verbose to \"True\" to view the associated loss and hyperparameters for each training iteration. False by default\n",
    "        set_lenscl: float/None: Determines whether Hyperparameter values will be set. None by Default\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        lenscl_final: ndarray, List containing value of the lengthscale hyperparameter at the end of training\n",
    "    \"\"\"\n",
    "    if rand_seed == False:\n",
    "        random_state = 1\n",
    "    else:\n",
    "        random_state = None\n",
    "        \n",
    "    #Fix noise to noise_std when optimizing lengthscale or you want the noise to be 0\n",
    "    if noise_std == 0: \n",
    "        noise_level = 0\n",
    "    else:\n",
    "        noise_level = noise_std**2 \n",
    "    #Always fix kernel noise to 0.01 or 0\n",
    "    noise_kern = WhiteKernel(noise_level=noise_level, noise_level_bounds= \"fixed\") #bounds = \"fixed\"\n",
    "    \n",
    "    if kern == \"RBF\":\n",
    "        kernel = ConstantKernel(constant_value=1, constant_value_bounds = (1e-2,10))*RBF(length_scale_bounds=(1e-2, 1e2)) + noise_kern #RBF\n",
    "    elif kern == \"Mat_32\":\n",
    "        kernel = ConstantKernel(constant_value=1)*Matern(length_scale_bounds=(1e-05, 1e7), nu=1.5) + noise_kern #Matern 3/2\n",
    "    else:\n",
    "        kernel = ConstantKernel(constant_value=1, constant_value_bounds = (1e-5,1e10))*Matern(length_scale_bounds=(1e-05, 10000000.0), nu=2.5) + noise_kern#Matern 5/2\n",
    "\n",
    "    #If setting lengthscale, ensure lengthscale values are fixed, otherwise initialize them at 1\n",
    "    if set_lenscl != None:\n",
    "        lengthscale_val = np.ones(train_param.shape[1])*set_lenscl\n",
    "        kernel.k1.k2.length_scale_bounds = \"fixed\"\n",
    "    else:\n",
    "        lengthscale_val = np.ones(train_param.shape[1])\n",
    "    \n",
    "    #Set model lengthscale\n",
    "    kernel.k1.k2.length_scale = lengthscale_val\n",
    "    \n",
    "    #Set outputscl kernel to be optimized if necessary\n",
    "    if outputscl == False: \n",
    "            kernel.k1.k1.constant_value_bounds = \"fixed\"\n",
    "    \n",
    "    #Don't optimize anything if lengthscale and outputscale are being set\n",
    "    if set_lenscl != None and outputscl == False:\n",
    "        optimizer = None\n",
    "    else:\n",
    "        optimizer = \"fmin_l_bfgs_b\"\n",
    "    \n",
    "    #Define model\n",
    "    gaussian_process = GaussianProcessRegressor(kernel=kernel, alpha=noise_std**2, n_restarts_optimizer=initialize, \n",
    "                                                random_state = random_state, optimizer = optimizer)\n",
    "    #Train GP\n",
    "    fit_gp = gaussian_process.fit(train_param, train_data)\n",
    "    \n",
    "    #Pull out kernel parameters after GP training\n",
    "    opt_kern_params = fit_gp.kernel_\n",
    "    outputscl_final = opt_kern_params.k1.k1.constant_value\n",
    "    lenscl_final = opt_kern_params.k1.k2.length_scale\n",
    "    lenscl_noise_final = opt_kern_params.k2.noise_level\n",
    "    \n",
    "    #Print them nicely\n",
    "    lenscl_print = ['%.3e' % lenscl_final[i] for i in range(len(lenscl_final))]\n",
    "    lenscl_noise_print = '%.3e' % lenscl_noise_final\n",
    "    opscl_print = '%.3e' % outputscl_final\n",
    "    \n",
    "    if verbose == True:\n",
    "        if set_lenscl is not None:\n",
    "            print(\"Lengthscale Set To: \", lenscl_print)\n",
    "        else:\n",
    "            print(\"Lengthscale is optimized using MLE to \", lenscl_print) \n",
    "        print(\"Noise set to \", lenscl_noise_print)\n",
    "        print(\"Outputscale set to \", opscl_print, \"\\n\")\n",
    "            \n",
    "    return lenscl_final, lenscl_noise_final, outputscl_final, fit_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b7d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train GP w/out noise hyperparameter\n",
    "lenscl_final,lenscl_noise_final, opscl_final, gaussian_process = train_GP_scikit(X_train, Y_train, noise_std, kernel_func, verbose, \n",
    "                                                                    set_lenscl, outputscl, initialize, rand_seed = False)   \n",
    "\n",
    "lenscl_print = ['%.3e' % lenscl_final[i] for i in range(len(lenscl_final))]\n",
    "lenscl_noise_print =  '%.3e' % lenscl_noise_final\n",
    "opscl_print = '%.3e' % opscl_final\n",
    "print(\"Final Lengthscale: \", lenscl_print)\n",
    "print(\"Final Noise for lengthscale (blank if none)\", lenscl_noise_print)\n",
    "print(\"Outputscale (1 is none)\", opscl_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ef1a3",
   "metadata": {},
   "source": [
    "# Section 3: Comparing GP and Y Sim Predictions on a Heat Map\n",
    "\n",
    "### Relavent Files: \n",
    "\n",
    "Heat Maps Generated are saved to this location when save_CSVs = True: \n",
    "Test_Figs/Figures/GP_Vs_Sim_Comp_CB/CS_2.2/train_iter_300/TP_20/scikit_learn/Mat_52/len_scl_varies/Mul_Comp_Figs.png\n",
    "\n",
    "Plotting function for above in bo_methods_lib/GP_Vs_True_Sensitivity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d7937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_GP_scipy(theta_set, X_space, true_model_coefficients, model, skip_param_types=0, CS=1, Xspace_is_Xexp = False):\n",
    "    \"\"\" \n",
    "    Calculates the expected improvement of the emulator approach\n",
    "    Parameters\n",
    "    ----------\n",
    "        theta_set: ndarray (num_LHS_points x dimensions), list of theta combinations\n",
    "        X_space: ndarray, The points for X over which to evaluate the GP (p^2 x dim(x) or n x dim(x))\n",
    "        true_model_coefficients: ndarray, The array containing the true values of problem constants\n",
    "        model: bound method, The model that the GP is bound by (gpytorch ot scikitlearn method)\n",
    "        likelihood: bound method, The likelihood of the GP model. In this case, must be a Gaussian likelihood or None\n",
    "        skip_param_types: The offset of which parameter types (A - y0) that are being guessed. Default 0\n",
    "        CS: float, the number of the case study to be evaluated. Default is 1, other option is 2.2 \n",
    "        Xspace_is_Xexp: bool, whether X_space is is a set of meshgrid values or Xexp values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        GP_mean: ndaarray, Array of GP mean predictions at X_space and theta_set\n",
    "        GP_stdev: ndarray, Array of GP variances related to GP means at X_space and theta_set\n",
    "        y_sim: ndarray, simulated values at X_space and theta_set\n",
    "    \"\"\"\n",
    "    #Define dimensionality of X \n",
    "    X_space = clean_1D_arrays(X_space, True)\n",
    "    m = X_space.shape[1]\n",
    "    p_sq = X_space.shape[0]\n",
    "    p = int(np.sqrt(p_sq))\n",
    "    \n",
    "    #Set theta_set to only be parameter values\n",
    "    theta_set_params = theta_set\n",
    "    \n",
    "    #Define the length of theta_set and the number of parameters that will be regressed (q)\n",
    "    if len(theta_set_params.shape) > 1:\n",
    "        len_set, q = theta_set_params.shape[0], theta_set_params.shape[1]\n",
    "    else:\n",
    "        theta_set_params = clean_1D_arrays(theta_set_params, param_clean = True)\n",
    "        len_set, q = theta_set_params.shape[0], theta_set_params.shape[1]\n",
    "    \n",
    "    #Initialize values for saving data\n",
    "    GP_mean = np.zeros((p_sq))\n",
    "    GP_var = np.zeros((p_sq))\n",
    "    y_sim = np.zeros((p_sq))\n",
    "    \n",
    "    #Loop over experimental data \n",
    "    for k in range(p_sq):\n",
    "        ##Calculate Values\n",
    "        #Define a parameter set, point\n",
    "        point = list(theta_set_params[0])\n",
    "        #Append Xexp_k to theta_set to evaluate at theta, xexp_k\n",
    "        x_point_data = list(X_space[k]) #astype(np.float)\n",
    "        #Create point to be evaluated\n",
    "        point = point + x_point_data\n",
    "        eval_point = torch.from_numpy(np.array([point])).float()\n",
    "        #Evaluate GP given parameter set theta and state point value\n",
    "        model_mean, model_variance = model.predict(eval_point, return_std=True)\n",
    "        \n",
    "        #Save values of GP mean and variance\n",
    "        GP_mean[k] = model_mean\n",
    "        GP_var[k] = model_variance\n",
    "        \n",
    "        #Calculate y_sim\n",
    "        if CS == 1:\n",
    "            #Case study 1, the 2D problem takes different arguments for its function create_y_data than 2.2\n",
    "            y_sim[k] = create_y_data(eval_point)\n",
    "        else:\n",
    "            y_sim[k] = create_y_data(eval_point, true_model_coefficients, X_space, skip_param_types)\n",
    "    \n",
    "    #Define GP standard deviation   \n",
    "    GP_stdev = np.sqrt(GP_var)  \n",
    "        \n",
    "    if m > 1 and Xspace_is_Xexp == False:\n",
    "        #Turn GP_mean, GP_stdev, and y_sim back into meshgrid form\n",
    "        GP_stdev = np.array(GP_stdev).reshape((p, p))\n",
    "        GP_mean = np.array(GP_mean).reshape((p, p))\n",
    "        y_sim = np.array(y_sim).reshape((p, p))\n",
    "   \n",
    "    return GP_mean, GP_stdev, y_sim\n",
    "\n",
    "eval_Train = \"Other\"\n",
    "# eval_Train = True\n",
    "# eval_Train = False\n",
    "\n",
    "#Set parameter set to evaluate at\n",
    "if eval_Train == False:\n",
    "    eval_p_base = true_p #True parameter set\n",
    "elif eval_Train == True:\n",
    "    eval_theta_num = -1 #Train theta to use for plotting later (Use -5 to -1)\n",
    "    eval_p_base = X_train[eval_theta_num*n,:-exp_d] #Parameter set corresponding to first training point\n",
    "else:\n",
    "    p=1 #int(np.sqrt(5**d)) #5 points per dimension in original theta set \n",
    "    theta_mesh = gen_theta_set(LHS = True, n_points = p, dimensions = d, bounds = bounds_p, seed = 1)\n",
    "    eval_p_base = torch.tensor(theta_mesh.flatten()) #Parameter set corresponding to first training point\n",
    "print(\"Evaluation Parameter Set\", eval_p_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978cc38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate GP for true parameter set value over all of X parameter space\n",
    "GP_mean, GP_stdev, y_sim = eval_GP_scipy(eval_p_base, X_space, Constants, gaussian_process, skip_param_types, CS, False)\n",
    "GP_mean_exp, GP_stdev_exp, y_sim_exp = eval_GP_scipy(eval_p_base, Xexp, Constants, gaussian_process, skip_param_types, CS, True)\n",
    "\n",
    "#Plot GP Predictions and True value\n",
    "title = [\"Y Sim\", \"GP Mean\", \"GP St.Dev\"]\n",
    "Mul_title = [\"/Sim_val\", \"/GP_mean\", \"/GP_stdev\"]\n",
    "z = [y_sim.T, GP_mean.T, GP_stdev.T]\n",
    "MAPE = np.sum(100*abs((y_sim_exp.flatten() - GP_mean_exp.flatten())/y_sim_exp.flatten()))\n",
    "print(\"MAPE: \", MAPE)\n",
    "#Note: Pulled this plotting code from file bo_methods_lib/GP_Vs_True_Sensitivity.py\n",
    "Muller_plotter(X_mesh, z, minima, saddle, title, set_lenscl, train_iter, t, CS, Bound_Cut, \n",
    "               lenscl_final, lenscl_noise_final, kernel_func, DateTime, Xexp, \n",
    "               save_csvs, save_figure , Mul_title, package = package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca43963",
   "metadata": {},
   "source": [
    "# Section 4: Parameter Sensitivity Analysis\n",
    "\n",
    "Parameter vs Muller potential Plots Generated are saved to this location when save_CSVs = True: Test_Figs/Figures/GP_Vs_Sim_Comp_CB/CS_2.2/train_iter_300/TP_20/scikit_learn/Mat_52/len_scl_varies/X_val_num_Y-param_Z.png\n",
    "\n",
    "Plotting function for above in bo_methods_lib/GP_Vs_True_Param_Sens.py\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "\"param_Z\" is a marker for a1, a2, a3,... b4\n",
    "\n",
    "Y is a fill in for a numbered X space point (not necessarily corresponding to the one in Xexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b9415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define X_space points to test for predictions\n",
    "x_set_points = [1,6,12,17,20]\n",
    "print(\"Xexp Point numbers (starting from 1 not 0): \", np.array(x_set_points)+1)\n",
    "eval_theta_num = -1 #Use -5 to -1\n",
    "train_xspace_set = np.array([all_data[m + eval_theta_num*n,1:] for m in x_set_points])\n",
    "X_set= np.array([Xexp[m] for m in x_set_points])\n",
    "Xspace_is_Xexp = False\n",
    "\n",
    "def Compare_GP_True_Param_Sens(eval_p_base, X_set, value_num, bounds_p, Constants, model, \n",
    "                               skip_param_types, CS, Xspace_is_Xexp):\n",
    "    #Make eval point base a tensor\n",
    "    if torch.is_tensor(eval_p_base) == False:\n",
    "        eval_p_base = torch.tensor(eval_p_base)\n",
    "    q = len(eval_p_base)\n",
    "    #Create list to save evaluated arrays in and arrays to store GP mean/stdev and true predictions in\n",
    "    GP_mean_all = np.zeros((len(X_set), q, value_num) )\n",
    "    GP_stdev_all = np.zeros((len(X_set), q, value_num) )\n",
    "    y_sim_all = np.zeros((len(X_set), q, value_num) )\n",
    "\n",
    "    #Create list to save evaluated parameter sets and values in\n",
    "    eval_p_df = []\n",
    "    values_list = [] \n",
    "\n",
    "    ##Evaluate parameter sets at each Xspace value\n",
    "    #Loop over all parameters\n",
    "    for i in range(len(eval_p_base)):   \n",
    "        #Clone the base value\n",
    "        eval_p = eval_p_base.clone()\n",
    "        #Define upper and lower theta bounds\n",
    "        lower_theta = bounds_p[0,i]\n",
    "        upper_theta = bounds_p[1,i]\n",
    "        #Define Values to test\n",
    "        values = np.linspace(lower_theta, upper_theta, value_num) #Note: Default to 41 \n",
    "        values_list.append(values)\n",
    "        #Save each bound value as a number from 0 to len(percentiles)\n",
    "        val_num_map = np.linspace(0,len(values)-1, len(values))\n",
    "        #Define parameter sets to test   \n",
    "        for j in range(len(values)):   \n",
    "            # Evaluate at the original point for each parameter and swap a parameter value for a value within the bounds\n",
    "            new_eval_p = values[j]\n",
    "#             print(new_eval_p)\n",
    "            #Change the value to the value listed in the linspace\n",
    "            eval_p[i] = torch.tensor(float(new_eval_p))\n",
    "#             eval_p[i] = torch.tensor(float('%.2g' % float(new_eval_p)))\n",
    "            #Append evaluated value to this list only on 1st iteration of k\n",
    "            eval_p_df.append(list(eval_p.numpy()))\n",
    "            #Loop over Xspace Values: #Note. X_space defined as Xexp points we want to test\n",
    "            for k in range(len(X_set)):\n",
    "                #Evaluate the values\n",
    "                eval_components_Xset = eval_GP_scipy(eval_p, X_set[k], Constants, model, skip_param_types, \n",
    "                                                     CS, Xspace_is_Xexp = False)\n",
    "                #Get GP predictions and true values\n",
    "                GP_mean_Xset, GP_stdev_Xset, y_sim_Xset = eval_components_Xset\n",
    "                #Append GP mean, GP_stdev, and true values to arrays outside of loop\n",
    "                GP_mean_all[k,i,j], GP_stdev_all[k,i,j], y_sim_all[k,i,j] = GP_mean_Xset, GP_stdev_Xset, y_sim_Xset\n",
    "\n",
    "    return y_sim_all, GP_mean_all, GP_stdev_all, values_list, val_num_map\n",
    "\n",
    "Evals  = Compare_GP_True_Param_Sens(eval_p_base, X_set, value_num, bounds_p, Constants, gaussian_process, \n",
    "                                    skip_param_types, CS, Xspace_is_Xexp)\n",
    "\n",
    "y_sim_all, GP_mean_all, GP_stdev_all, values_list, val_num_map = Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot GP vs y_sim predictions for each Xexp Value and save data\n",
    "all_data_to_plot = [y_sim_all, GP_mean_all, GP_stdev_all] \n",
    "#Note: Plotting function pulled from\n",
    "mul_plot_param(all_data_to_plot, set_lenscl, train_iter, t, CS, Bound_Cut, X_set, x_set_points, \n",
    "               param_dict, values_list, val_num_map, lenscl_final, train_xspace_set, lenscl_noise_final, \n",
    "               opscl_final, kernel_func, DateTime, Xexp, save_csvs, save_figure, package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb55333",
   "metadata": {},
   "source": [
    "# Section 5: Specified Lengthscale GP Prediction Analysis\n",
    "\n",
    "Parameter vs Muller potential Plots Generated are saved to this location when save_CSVs = True: Test_Figs/Figures/GP_Vs_Sim_Comp_CB/CS_2.2/train_iter_300/TP_20/scikit_learn/Mat_52/len_scl_XXX/X_val_num_Y-param_Z.png\n",
    "\n",
    "Plotting function for above in bo_methods_lib/GP_Vs_True_Param_Sens.py\n",
    "\n",
    "Heat Maps Generated are saved to this location when save_CSVs = True: Test_Figs/Figures/GP_Vs_Sim_Comp_CB/CS_2.2/train_iter_300/TP_20/scikit_learn/Mat_52/len_scl_XXX/Mul_Comp_Figs.png\n",
    "\n",
    "Plotting function for above in bo_methods_lib/GP_Vs_True_Sensitivity.py\n",
    "\n",
    "A version of this experiment where multiple amounts of trainin data are used and hyperparameters are optimized can be found in /scratch365/mcarlozo/Toy_Problem/2023/05/09/13-05. Similarly, a copy of this experiment where lengthscales are fixed to 1 can be found in /scratch365/mcarlozo/Toy_Problem/2023/04/28/11-05\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "XXX is a fill in for the lengthscale. If this value is optimized, XXX = \"varies\"\n",
    "\n",
    "\"param_Z\" is a marker for a1, a2, a3,... b4\n",
    "\n",
    "Y is a fill in for a numbered X space point (not necessarily corresponding to the one in Xexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c5b1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lenscl_noises = True\n",
    "lengthscales = [0.1,1,10,None]\n",
    "lengthscales = [1]\n",
    "for lenscl in lengthscales:\n",
    "    model_params = train_GP_scikit(X_train, Y_train, noise_std, kernel_func, verbose, set_lenscl, outputscl, \n",
    "                                   initialize, rand_seed = False)\n",
    "    lenscl_final,lenscl_noise_final, opscl_final, gaussian_process = model_params\n",
    "    \n",
    "    #Evaluate GP for true parameter set values over all of parameter space bounds\n",
    "    Evals  = Compare_GP_True_Param_Sens(eval_p_base, X_set, value_num, bounds_p, Constants, gaussian_process, \n",
    "                                    skip_param_types, CS, Xspace_is_Xexp)\n",
    "\n",
    "    y_sim_all, GP_mean_all, GP_stdev_all, values_list, val_num_map = Evals\n",
    "\n",
    "    #Plot GP vs y_sim predictions for each Xexp Value and save data\n",
    "    all_data_to_plot = [y_sim_all, GP_mean_all, GP_stdev_all] \n",
    "\n",
    "    #Evaluate GP for true parameter set value over all of X parameter space\n",
    "    GP_mean, GP_stdev, y_sim = eval_GP_scipy(eval_p_base, X_space, Constants, gaussian_process, skip_param_types, \n",
    "                                             CS, False)\n",
    "\n",
    "    #Plot GP Predictions and True value\n",
    "    title = [\"Y Sim\", \"GP Mean\", \"GP St.Dev\"]\n",
    "    Mul_title = [\"/Sim_val\", \"/GP_mean\", \"/GP_stdev\"]\n",
    "    z = [y_sim.T, GP_mean.T, GP_stdev.T]\n",
    "    #Not sure how to find stdev of the lengthscale\n",
    "    #Note: Pulled this plotting code from file bo_methods_lib/GP_Vs_True_Sensitivity.py\n",
    "    Muller_plotter(X_mesh, z, minima, saddle, title, lenscl, train_iter, t, CS, Bound_Cut, \n",
    "                   lenscl_final, lenscl_noise_final, kernel_func, DateTime, Xexp, save_csvs, save_figure, Mul_title, package = package)\n",
    "\n",
    "    mul_plot_param(all_data_to_plot, set_lenscl, train_iter, t, CS, Bound_Cut, X_set, x_set_points, \n",
    "               param_dict, values_list, val_num_map, lenscl_final, train_xspace_set, lenscl_noise_final, \n",
    "               opscl_final, kernel_func, DateTime, Xexp, save_csvs, save_figure, package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac3d86a",
   "metadata": {},
   "source": [
    "# Section 6: Remaking Specific Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab38cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Parameters\n",
    "CS = 2.2 #Case Study\n",
    "DateTime = None #Date and Time\n",
    "Bound_Cut = True #Defines whether original bounds are cut\n",
    "denseX = True\n",
    "eval_Train = True #Defines whether we are examining close to the true parameter set or the first training parameter set\n",
    "#Consatnts for the Muller Potential\n",
    "Constants = np.array([[-200,-100,-170,15],\n",
    "                      [-1,-1,-6.5,0.7],\n",
    "                      [0,0,11,0.6],\n",
    "                      [-10,-10,-6.5,0.7],\n",
    "                      [1,0,-0.5,-1],\n",
    "                      [0,0.5,1.5,1]])\n",
    "skip_param_types = 1 #This is what changes for subpoint\n",
    "true_p = Constants[skip_param_types:skip_param_types+2].flatten()\n",
    "param_dict = {0 : 'a_1', 1 : 'a_2', 2 : 'a_3', 3 : 'a_4',\n",
    "              4 : 'b_1', 5 : 'b_2', 6 : 'b_3', 7 : 'b_4'}\n",
    "exp_d = 2\n",
    "if Bound_Cut == True:\n",
    "    bounds_x = np.array([[-1.0, 0.0],\n",
    "                        [   0.5, 1.5]])\n",
    "    if denseX == True:\n",
    "        n = 30\n",
    "    else:\n",
    "        n = 25 #Number of experimental data points to use\n",
    "else:    \n",
    "    bounds_x = np.array([[-1.5, -0.5],\n",
    "                 [   1,    2]])\n",
    "    n = 27 #Number of experimental data points to use\n",
    "bounds_p = np.array([[-2, -2, -10, -2, -2, -2,  5, -2],\n",
    "               [ 2,  2,   0,  2,  2,  2, 15,  2]])\n",
    "minima = np.array([[-0.558,1.442],\n",
    "              [-0.050,0.467],\n",
    "              [0.623,0.028]])\n",
    "saddle = np.array([[-0.82,0.62],\n",
    "                  [0.22,0.30]])\n",
    "package = \"scikit_learn\" #Package training the GP model\n",
    "t = 20 #Number of parameter training sets\n",
    "# percentiles = np.linspace(-1.0,1.0,41)\n",
    "percentiles = np.linspace(0,0,1) #Percent by which to deviate from a given parameter set\n",
    "value_num = 101 #Number of parameter values for each parameter to evaluate within the bounds\n",
    "d = len(true_p) #Dimensionality of parameter space\n",
    "kernel_func = \"Mat_52\" #Kernel function to use\n",
    "train_iter = 300 #Maximum GP training iterations\n",
    "initialize = 2 #Number of times to restart GP training\n",
    "noise_std = 0.01 #Noise associated with actual data\n",
    "set_lenscl = None #lengthscale of the kernel (None means it will be optimized)\n",
    "outputscl = True #Outputscale will be optimized\n",
    "verbose = False\n",
    "rand_seed = False #Whether or not a random seed it used for RNG events\n",
    "emulator = True #GP directly emulates function\n",
    "obj = \"obj\"\n",
    "save_figure = True #Whether to save figures\n",
    "save_figure = False\n",
    "save_csvs = True #Whether to save CSVs\n",
    "save_csvs = False\n",
    "#Naming convention to pull experimental and training data from CSVs\n",
    "if Bound_Cut == True:\n",
    "    cut_bounds = '_cut_bounds'\n",
    "else:\n",
    "    cut_bounds = \"\"\n",
    "if denseX == True:\n",
    "    dense = \"_dense\"\n",
    "else:\n",
    "    dense = \"\"\n",
    "#Pull Experimental data from CSV\n",
    "exp_data_doc = 'Input_CSVs/Exp_Data/d='+str(exp_d)+'/n='+str(n)+cut_bounds+dense+'.csv'\n",
    "exp_data = np.array(pd.read_csv(exp_data_doc, header=0,sep=\",\"))\n",
    "Xexp = exp_data[:,1:exp_d+1]\n",
    "Yexp = exp_data[:,-1]\n",
    "Xexp = clean_1D_arrays(Xexp)\n",
    "m = Xexp.shape[1]\n",
    "#Pull training data from CSV\n",
    "t_use = int(t*n)\n",
    "all_data_doc = find_train_doc_path(emulator, obj, d, t_use, Bound_Cut, denseX)\n",
    "all_data = np.array(pd.read_csv(all_data_doc, header=0,sep=\",\"))\n",
    "X_train = torch.tensor(all_data[:,1:-m+1]).float() #8 or 10 (emulator) parameters\n",
    "Y_train = train_y = torch.tensor(all_data[:,-1]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92adedd2",
   "metadata": {},
   "source": [
    "## Remake Plot for $\\ell = 0.1$ , Xexp Point 2, and Parameter b3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3843080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define X_space points to test for predictions\n",
    "ell = 0.1 #Set lengthscale\n",
    "x_set_points = [1] #Set Xexp point to evaluate at\n",
    "eval_p_base = X_train[eval_theta_num,:-exp_d] #Set eval_p_base to be the first training point\n",
    "param_to_plot = list(param_dict.values()).index(\"b_3\") #Only want data for b3\n",
    "\n",
    "print(\"Xexp Point numbers (starting from 1 not 0): \", np.array(x_set_points)+1)\n",
    "train_xspace_set = np.array([all_data[m + eval_theta_num*n,1:] for m in x_set_points])\n",
    "X_set= np.array([Xexp[m] for m in x_set_points])\n",
    "Xspace_is_Xexp = False\n",
    "\n",
    "model_params = train_GP_scikit(X_train, Y_train, noise_std, kernel_func, verbose, set_lenscl, outputscl, \n",
    "                                   initialize, rand_seed = False)\n",
    "lenscl_final,lenscl_noise_final, opscl_final, gaussian_process = model_params\n",
    "\n",
    "Evals  = Compare_GP_True_Param_Sens(eval_p_base, X_set, value_num, bounds_p, Constants, gaussian_process, \n",
    "                                    skip_param_types, CS, Xspace_is_Xexp)\n",
    "\n",
    "y_sim_all, GP_mean_all, GP_stdev_all, values_list, val_num_map = Evals\n",
    "\n",
    "#Plot GP vs y_sim predictions for each Xexp Value and save data\n",
    "all_data_to_plot = [y_sim_all, GP_mean_all, GP_stdev_all] \n",
    "#Note: Plotting function pulled from\n",
    "mul_plot_param(all_data_to_plot, set_lenscl, train_iter, t, CS, Bound_Cut, X_set, x_set_points, \n",
    "               param_dict, values_list, val_num_map, lenscl_final, train_xspace_set, lenscl_noise_final, \n",
    "               opscl_final, kernel_func, DateTime, Xexp, save_csvs, save_figure, package, param_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01867a",
   "metadata": {},
   "source": [
    "## Remake Plot for $\\ell = 10$ , Xexp Point 13, and Parameter b3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a6effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define X_space points to test for predictions\n",
    "ell = 10 #Set lengthscale\n",
    "x_set_points = [12] #Set Xexp point to evaluate at\n",
    "eval_p_base = X_train[eval_theta_num,:-exp_d] #Set eval_p_base to be the first training point\n",
    "param_to_plot = list(param_dict.values()).index(\"b_3\") #Only want data for b3\n",
    "\n",
    "print(\"Xexp Point numbers (starting from 1 not 0): \", np.array(x_set_points)+1)\n",
    "train_xspace_set = np.array([all_data[m + eval_theta_num*n,1:] for m in x_set_points])\n",
    "X_set= np.array([Xexp[m] for m in x_set_points])\n",
    "Xspace_is_Xexp = False\n",
    "\n",
    "model_params = train_GP_scikit(X_train, Y_train, noise_std, kernel_func, verbose, set_lenscl, outputscl, \n",
    "                                   initialize, rand_seed = False)\n",
    "lenscl_final,lenscl_noise_final, opscl_final, gaussian_process = model_params\n",
    "\n",
    "Evals  = Compare_GP_True_Param_Sens(eval_p_base, X_set, value_num, bounds_p, Constants, gaussian_process, \n",
    "                                    skip_param_types, CS, Xspace_is_Xexp)\n",
    "\n",
    "y_sim_all, GP_mean_all, GP_stdev_all, values_list, val_num_map = Evals\n",
    "\n",
    "#Plot GP vs y_sim predictions for each Xexp Value and save data\n",
    "all_data_to_plot = [y_sim_all, GP_mean_all, GP_stdev_all] \n",
    "#Note: Plotting function pulled from\n",
    "mul_plot_param(all_data_to_plot, set_lenscl, train_iter, t, CS, Bound_Cut, X_set, x_set_points, \n",
    "               param_dict, values_list, val_num_map, lenscl_final, train_xspace_set, lenscl_noise_final, \n",
    "               opscl_final, kernel_func, DateTime, Xexp, save_csvs, save_figure, package, param_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906eb722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
