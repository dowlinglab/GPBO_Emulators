{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1d566f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.stats import qmc\n",
    "from bo_methods_lib.bo_functions_generic import round_time, gen_theta_set,gen_x_set, find_train_doc_path, set_ep, clean_1D_arrays\n",
    "from bo_methods_lib.GP_Vs_True_Sensitivity import Muller_plotter\n",
    "from bo_methods_lib.GP_Vs_True_Param_Sens import mul_plot_param\n",
    "# from .CS1_create_data import gen_y_Theta_GP, calc_y_exp, create_y_data\n",
    "from bo_methods_lib.CS2_create_data import gen_y_Theta_GP, calc_y_exp, create_y_data\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, WhiteKernel\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e6f8dc",
   "metadata": {},
   "source": [
    "# Section 1) Setting Parameters/ Defining the Problem\n",
    "\n",
    "### Relavent Files: \n",
    "\n",
    "Training Data: Input_CSVs/Train_Data/d=8/all_emul_data/t=500_cut_bounds.csv\n",
    "\n",
    "Experimental Data: Input_CSVs/Exp_Data/d=2/n=25_cut_bounds.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "875160c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Parameters\n",
    "\n",
    "CS = 2.2 #Case Study\n",
    "DateTime = None #Date and Time\n",
    "\n",
    "Bound_Cut = True #Defines whether original bounds are cut\n",
    "eval_Train = True #Defines whether we are examining close to the true parameter set or the first training parameter set\n",
    "\n",
    "#Consatnts for the Muller Potential\n",
    "Constants = np.array([[-200,-100,-170,15],\n",
    "                      [-1,-1,-6.5,0.7],\n",
    "                      [0,0,11,0.6],\n",
    "                      [-10,-10,-6.5,0.7],\n",
    "                      [1,0,-0.5,-1],\n",
    "                      [0,0.5,1.5,1]])\n",
    "\n",
    "#Case Study 2.2 defined as the 8 parameter Muller potential\n",
    "if CS == 2.2:\n",
    "    skip_param_types = 1 #This is what changes for subpoint\n",
    "    true_p = Constants[skip_param_types:skip_param_types+2].flatten()\n",
    "    param_dict = {0 : 'a_1', 1 : 'a_2', 2 : 'a_3', 3 : 'a_4',\n",
    "                  4 : 'b_1', 5 : 'b_2', 6 : 'b_3', 7 : 'b_4'}\n",
    "    exp_d = 2 #Dimensioanlity of X data\n",
    "    if Bound_Cut == True:\n",
    "        bounds_x = np.array([[-1.0, 0.0], #Bounds on X space (after cutting)\n",
    "                            [   0.5, 1.5]])\n",
    "        n = 25 #Number of experimental data points to use\n",
    "    else:    \n",
    "        bounds_x = np.array([[-1.5, -0.5],  #Uncut bounds on X space\n",
    "                     [   1,    2]])\n",
    "        n = 27 #Number of experimental data points to use\n",
    "    bounds_p = np.array([[-2, -2, -10, -2, -2, -2,  5, -2], #Bounds on parameter space\n",
    "                   [ 2,  2,   0,  2,  2,  2, 15,  2]])\n",
    "    minima = np.array([[-0.558,1.442], #Minima of true Muller potential\n",
    "                              [-0.050,0.467],\n",
    "                              [0.623,0.028]])\n",
    "\n",
    "    saddle = np.array([[-0.82,0.62], #Saddle points of True Muller potential\n",
    "                              [0.22,0.30]])\n",
    "\n",
    "#Case Study 1 is the cubic equation toy problem\n",
    "else:\n",
    "    Constants = true_p = np.array([1,-1])\n",
    "    skip_param_types = 0\n",
    "    param_dict = {0 : '\\\\theta_1', 1 : '\\\\theta_2'}\n",
    "    exp_d = 1\n",
    "    n = 5\n",
    "    bounds_x = np.array([[-2], [2]])\n",
    "    bounds_p = np.array([[-2, -2],\n",
    "                         [ 2,  2]])\n",
    "\n",
    "package = \"scikit_learn\" #Package training the GP model\n",
    "t = 20 #Number of parameter training sets\n",
    "# percentiles = np.linspace(-1.0,1.0,41)\n",
    "percentiles = np.linspace(0,0,1) #Percent by which to deviate from a given parameter set\n",
    "value_num = 100 #Number of parameter values for each parameter to evaluate within the bounds\n",
    "d = len(true_p) #Dimensionality of parameter space\n",
    "kernel_func = \"Mat_52\" #Kernel function to use\n",
    "train_iter = 300 #Maximum GP training iterations\n",
    "initialize = 2 #Number of times to restart GP training\n",
    "noise_std = 0.1 #Noise assocuated with data\n",
    "lenscl_w_noise = False\n",
    "set_lenscl = None #lengthscale of the kernel (None means it will be optimized)\n",
    "verbose = False\n",
    "rand_seed = False #Whether or not a random seed it used for RNG events\n",
    "\n",
    "emulator = True #GP directly emulates function\n",
    "obj = \"obj\"\n",
    "\n",
    "save_figure = True #Whether to save figures\n",
    "# save_figure = False\n",
    "save_csvs = True #Whether to save CSVs\n",
    "# save_csvs = False\n",
    "\n",
    "#Naming convention to pull experimental and training data from CSVs\n",
    "if Bound_Cut == True:\n",
    "    cut_bounds = '_cut_bounds'\n",
    "else:\n",
    "    cut_bounds = \"\"\n",
    "\n",
    "#Pull Experimental data from CSV\n",
    "exp_data_doc = 'Input_CSVs/Exp_Data/d='+str(exp_d)+'/n='+str(n)+cut_bounds+'.csv'\n",
    "exp_data = np.array(pd.read_csv(exp_data_doc, header=0,sep=\",\"))\n",
    "Xexp = exp_data[:,1:exp_d+1]\n",
    "Yexp = exp_data[:,-1]\n",
    "Xexp = clean_1D_arrays(Xexp)\n",
    "m = Xexp.shape[1]\n",
    "\n",
    "#Pull training data from CSV\n",
    "t_use = int(t*n)\n",
    "all_data_doc = find_train_doc_path(emulator, obj, d, t_use, bound_cut = Bound_Cut)\n",
    "all_data = np.array(pd.read_csv(all_data_doc, header=0,sep=\",\"))\n",
    "X_train = torch.tensor(all_data[:,1:-m+1]).float() #8 or 10 (emulator) parameters\n",
    "Y_train = train_y = torch.tensor(all_data[:,-1]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538d908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set meshgrid values and define a meshgrid over X_space\n",
    "p = 20\n",
    "X1 =  np.linspace(bounds_x[0,0],bounds_x[1,0],p) \n",
    "X2 =  np.linspace(bounds_x[0,1],bounds_x[1,1],p) \n",
    "X_mesh = np.array(np.meshgrid(X1, X2)) \n",
    "#Generate the same points as X_mesh in a 2D array\n",
    "X_space = gen_x_set(LHS = False, n_points = p, dimensions = exp_d, bounds = bounds_x) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71913abf",
   "metadata": {},
   "source": [
    "# Section 2) Building/ Training the Model\n",
    "\n",
    "### Relavent Files: \n",
    "\n",
    "Heat Maps Generated: Test_Figs/Figures/GP_Vs_Sim_Comp_CB/CS_2.2/train_iter_300/TP_20/scikit_learn/Mat_52/len_scl_varies/Mul_Comp_Figs.png\n",
    "\n",
    "Plotting function for above in bo_methods_lib/GP_Vs_True_Sensitivity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c6122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GP_scikit(train_param, train_data, noise_std, kern = \"Mat_52\", verbose=False, \n",
    "                    set_lenscl = None, initialize = 1, rand_seed = False, noisy = False):\n",
    "    \"\"\"\n",
    "    Trains the GP model and finds hyperparameters with the scikit learn package\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        train_param: tensor or ndarray, The training parameter space data\n",
    "        train_data: tensor or ndarray, The training y data\n",
    "        iterations: float or int, number of training iterations to run. Default is 300\n",
    "        verbose: Set verbose to \"True\" to view the associated loss and hyperparameters for each training iteration. False by default\n",
    "        set_lenscl: float/None: Determines whether Hyperparameter values will be set. None by Default\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        lenscl_final: ndarray, List containing value of the lengthscale hyperparameter at the end of training\n",
    "    \"\"\"\n",
    "    if rand_seed == False:\n",
    "        random_state = 1\n",
    "    else:\n",
    "        random_state = None\n",
    "        \n",
    "    if noisy == False:\n",
    "        noise_level = 0\n",
    "    else:\n",
    "        noise_level = 1\n",
    "\n",
    "    noise_kern = WhiteKernel(noise_level=noise_level, noise_level_bounds=(1e-05, 10)) #bounds = \"fixed\"\n",
    "    \n",
    "    if kern == \"RBF\":\n",
    "        kernel = RBF(length_scale_bounds=(1e-2, 1e2)) + noise_kern # RBF\n",
    "    elif kern == \"Mat_32\":\n",
    "        kernel = Matern(length_scale_bounds=(1e-05, 10000000.0), nu=1.5) + noise_kern #Matern 3/2\n",
    "    else:\n",
    "        kernel = Matern(length_scale_bounds=(1e-05, 10000000.0), nu=2.5) + noise_kern#Matern 5/2\n",
    "\n",
    "    if set_lenscl != None:\n",
    "        lengthscale_val = np.ones(train_param.shape[1])*set_lenscl\n",
    "        kernel.k1.length_scale_bounds = \"fixed\"\n",
    "        kernel.k2.noise_level_bounds = \"fixed\" #Always fix kernel noise to 1 or 0\n",
    "        optimizer = None\n",
    "    else:\n",
    "        lengthscale_val = np.ones(train_param.shape[1])\n",
    "#         kernel.k2.noise_level_bounds = \"fixed\" #Always fix kernel noise to 1 or 0\n",
    "        optimizer = \"fmin_l_bfgs_b\"\n",
    "\n",
    "    kernel.k1.length_scale = lengthscale_val\n",
    "    gaussian_process = GaussianProcessRegressor(kernel=kernel, alpha=noise_std**2, n_restarts_optimizer=initialize, \n",
    "                                                random_state = random_state, optimizer = optimizer)\n",
    "    #Train GP\n",
    "    gaussian_process.fit(train_param, train_data)\n",
    "    \n",
    "    #Pull out kernel parameters after GP training\n",
    "    opt_kern_params = gaussian_process.kernel_\n",
    "    lenscl_final = opt_kern_params.k1.length_scale\n",
    "    lenscl_noise_final = opt_kern_params.k2.noise_level\n",
    "    \n",
    "    #Print them nicely\n",
    "    lenscl_print = ['%.3e' % lenscl_final[i] for i in range(len(lenscl_final))]\n",
    "    lenscl_noise_print = '%.3e' % lenscl_noise_final\n",
    "    \n",
    "    if verbose == True:\n",
    "        if set_lenscl is not None:\n",
    "            print(\"Lengthscale Set To: \", lenscl_print)\n",
    "            print(\"Noise Set To: \", lenscl_noise_print, \"\\n\")\n",
    "        else:\n",
    "            print(\"Lengthscale is optimized using MLE to \", lenscl_print) \n",
    "            print(\"Noise is optimized using MLE to \", lenscl_noise_print, \"\\n\")\n",
    "            \n",
    "    return lenscl_final, lenscl_noise_final, gaussian_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c6b7d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/crc.nd.edu/user/m/mcarlozo/.conda/envs/Toy_Problem_env/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:285: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(np.hstack(theta))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Lengthscale:  ['9.999e+06', '1.000e+07', '1.000e+07', '1.000e+07', '1.000e+07', '1.000e+07', '1.000e+07', '1.000e+07', '1.000e-05', '1.000e-05']\n",
      "Final Noise for lengthscale (blank if none) 1.000e+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/crc.nd.edu/user/m/mcarlozo/.conda/envs/Toy_Problem_env/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__length_scale is close to the specified upper bound 10000000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/afs/crc.nd.edu/user/m/mcarlozo/.conda/envs/Toy_Problem_env/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 1 of parameter k1__length_scale is close to the specified upper bound 10000000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/afs/crc.nd.edu/user/m/mcarlozo/.conda/envs/Toy_Problem_env/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 2 of parameter k1__length_scale is close to the specified upper bound 10000000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/afs/crc.nd.edu/user/m/mcarlozo/.conda/envs/Toy_Problem_env/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 3 of parameter k1__length_scale is close to the specified upper bound 10000000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/afs/crc.nd.edu/user/m/mcarlozo/.conda/envs/Toy_Problem_env/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 4 of parameter k1__length_scale is close to the specified upper bound 10000000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/afs/crc.nd.edu/user/m/mcarlozo/.conda/envs/Toy_Problem_env/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 5 of parameter k1__length_scale is close to the specified upper bound 10000000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/afs/crc.nd.edu/user/m/mcarlozo/.conda/envs/Toy_Problem_env/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 6 of parameter k1__length_scale is close to the specified upper bound 10000000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/afs/crc.nd.edu/user/m/mcarlozo/.conda/envs/Toy_Problem_env/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 7 of parameter k1__length_scale is close to the specified upper bound 10000000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/afs/crc.nd.edu/user/m/mcarlozo/.conda/envs/Toy_Problem_env/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 8 of parameter k1__length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/afs/crc.nd.edu/user/m/mcarlozo/.conda/envs/Toy_Problem_env/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 9 of parameter k1__length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/afs/crc.nd.edu/user/m/mcarlozo/.conda/envs/Toy_Problem_env/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified upper bound 10.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Train GP w/out noise hyperparameter\n",
    "lenscl_final,lenscl_noise_final, gaussian_process = train_GP_scikit(X_train, Y_train, noise_std, kernel_func, verbose, \n",
    "                                                                    set_lenscl, initialize, rand_seed = False, \n",
    "                                                                    noisy = lenscl_w_noise)   \n",
    "\n",
    "lenscl_print = ['%.3e' % lenscl_final[i] for i in range(len(lenscl_final))]\n",
    "lenscl_noise_print =  '%.3e' % lenscl_noise_final\n",
    "print(\"Final Lengthscale: \", lenscl_print)\n",
    "print(\"Final Noise for lengthscale (blank if none)\", lenscl_noise_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ef1a3",
   "metadata": {},
   "source": [
    "# Section 3: Comparing GP and Y Sim Predictions on a Heat Map\n",
    "\n",
    "### Relavent Files: \n",
    "\n",
    "Heat Maps Generated: Test_Figs/Figures/GP_Vs_Sim_Comp_CB/CS_2.2/train_iter_300/TP_20/scikit_learn/Mat_52/len_scl_varies/Mul_Comp_Figs.png\n",
    "\n",
    "Plotting function for above in bo_methods_lib/GP_Vs_True_Sensitivity.py\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "\"param_Z\" is a marker for a1, a2, a3,... b4\n",
    "\n",
    "Y is a fill in for a numbered X space point (not necessarily corresponding to the one in Xexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "289d7937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Parameter Set tensor([-1.1740, -1.6574, -9.8016, -1.5555,  0.6568,  0.0169, 14.5698, -0.1836])\n"
     ]
    }
   ],
   "source": [
    "def eval_GP_scipy(theta_set, X_space, true_model_coefficients, model, skip_param_types=0, CS=1, Xspace_is_Xexp = False):\n",
    "    \"\"\" \n",
    "    Calculates the expected improvement of the emulator approach\n",
    "    Parameters\n",
    "    ----------\n",
    "        theta_set: ndarray (num_LHS_points x dimensions), list of theta combinations\n",
    "        X_space: ndarray, The points for X over which to evaluate the GP (p^2 x dim(x) or n x dim(x))\n",
    "        true_model_coefficients: ndarray, The array containing the true values of problem constants\n",
    "        model: bound method, The model that the GP is bound by (gpytorch ot scikitlearn method)\n",
    "        likelihood: bound method, The likelihood of the GP model. In this case, must be a Gaussian likelihood or None\n",
    "        skip_param_types: The offset of which parameter types (A - y0) that are being guessed. Default 0\n",
    "        CS: float, the number of the case study to be evaluated. Default is 1, other option is 2.2 \n",
    "        Xspace_is_Xexp: bool, whether X_space is is a set of meshgrid values or Xexp values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        GP_mean: ndaarray, Array of GP mean predictions at X_space and theta_set\n",
    "        GP_stdev: ndarray, Array of GP variances related to GP means at X_space and theta_set\n",
    "        y_sim: ndarray, simulated values at X_space and theta_set\n",
    "    \"\"\"\n",
    "    #Define dimensionality of X \n",
    "    X_space = clean_1D_arrays(X_space, True)\n",
    "    m = X_space.shape[1]\n",
    "    p_sq = X_space.shape[0]\n",
    "    p = int(np.sqrt(p_sq))\n",
    "    \n",
    "    #Set theta_set to only be parameter values\n",
    "    theta_set_params = theta_set\n",
    "    \n",
    "    #Define the length of theta_set and the number of parameters that will be regressed (q)\n",
    "    if len(theta_set_params.shape) > 1:\n",
    "        len_set, q = theta_set_params.shape[0], theta_set_params.shape[1]\n",
    "    else:\n",
    "        theta_set_params = clean_1D_arrays(theta_set_params, param_clean = True)\n",
    "        len_set, q = theta_set_params.shape[0], theta_set_params.shape[1]\n",
    "    \n",
    "    #Initialize values for saving data\n",
    "    GP_mean = np.zeros((p_sq))\n",
    "    GP_var = np.zeros((p_sq))\n",
    "    y_sim = np.zeros((p_sq))\n",
    "    \n",
    "    #Loop over experimental data \n",
    "    for k in range(p_sq):\n",
    "        ##Calculate Values\n",
    "        #Define a parameter set, point\n",
    "        point = list(theta_set_params[0])\n",
    "        #Append Xexp_k to theta_set to evaluate at theta, xexp_k\n",
    "        x_point_data = list(X_space[k]) #astype(np.float)\n",
    "        #Create point to be evaluated\n",
    "        point = point + x_point_data\n",
    "        eval_point = torch.from_numpy(np.array([point])).float()\n",
    "        #Evaluate GP given parameter set theta and state point value\n",
    "        model_mean, model_variance = model.predict(eval_point, return_std=True)\n",
    "        \n",
    "        #Save values of GP mean and variance\n",
    "        GP_mean[k] = model_mean\n",
    "        GP_var[k] = model_variance\n",
    "        \n",
    "        #Calculate y_sim\n",
    "        if CS == 1:\n",
    "            #Case study 1, the 2D problem takes different arguments for its function create_y_data than 2.2\n",
    "            y_sim[k] = create_y_data(eval_point)\n",
    "        else:\n",
    "            y_sim[k] = create_y_data(eval_point, true_model_coefficients, X_space, skip_param_types)\n",
    "    \n",
    "    #Define GP standard deviation   \n",
    "    GP_stdev = np.sqrt(GP_var)  \n",
    "        \n",
    "    if m > 1 and Xspace_is_Xexp == False:\n",
    "        #Turn GP_mean, GP_stdev, and y_sim back into meshgrid form\n",
    "        GP_stdev = np.array(GP_stdev).reshape((p, p))\n",
    "        GP_mean = np.array(GP_mean).reshape((p, p))\n",
    "        y_sim = np.array(y_sim).reshape((p, p))\n",
    "   \n",
    "    return GP_mean, GP_stdev, y_sim\n",
    "\n",
    "#Set parameter set to evaluate at\n",
    "if eval_Train == False:\n",
    "    eval_p_base = true_p #True parameter set\n",
    "else:\n",
    "    eval_p_base = X_train[0,:-exp_d] #Parameter set corresponding to first training point\n",
    "print(\"Evaluation Parameter Set\", eval_p_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a19b88d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate GP for true parameter set value over all of X parameter space\n",
    "GP_mean, GP_stdev, y_sim = eval_GP_scipy(eval_p_base, X_space, Constants, gaussian_process, skip_param_types, CS, False)\n",
    "\n",
    "#Plot GP Predictions and True value\n",
    "title = [\"Y Sim\", \"GP Mean\", \"GP St.Dev\"]\n",
    "Mul_title = [\"/Sim_val\", \"/GP_mean\", \"/GP_stdev\"]\n",
    "z = [y_sim.T, GP_mean.T, GP_stdev.T]\n",
    "#Not sure how to find stdev of the lengthscale\n",
    "#Note: Pulled this plotting code from file bo_methods_lib/GP_Vs_True_Sensitivity.py\n",
    "Muller_plotter(X_mesh, z, minima, saddle, title, set_lenscl, train_iter, t, CS, Bound_Cut, \n",
    "               lenscl_final, lenscl_noise_final, kernel_func, DateTime, Xexp, \n",
    "               save_csvs, save_figure , Mul_title, package = package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca43963",
   "metadata": {},
   "source": [
    "# Section 4: Predictions Over Theta\n",
    "\n",
    "parameter vs Muller potential Plots Generated: Test_Figs/Figures/GP_Vs_Sim_Comp_CB/CS_2.2/train_iter_300/TP_20/scikit_learn/Mat_52/len_scl_varies/X_val_num_Y-param_Z.png\n",
    "\n",
    "Plotting function for above in bo_methods_lib/GP_Vs_True_Param_Sens.py\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "\"param_Z\" is a marker for a1, a2, a3,... b4\n",
    "\n",
    "Y is a fill in for a numbered X space point (not necessarily corresponding to the one in Xexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2b9415b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xexp Point numbers (starting from 1 not 0):  [ 2  7 13 18 21]\n"
     ]
    }
   ],
   "source": [
    "#Define X_space points to test for predictions\n",
    "x_set_points = [1,6,12,17,20]\n",
    "print(\"Xexp Point numbers (starting from 1 not 0): \", np.array(x_set_points)+1)\n",
    "X_set= np.array([Xexp[m] for m in x_set_points])\n",
    "Xspace_is_Xexp = False\n",
    "\n",
    "def Compare_GP_True_Param_Sens(eval_p_base, X_set, value_num, bounds_p, Constants, model, \n",
    "                               skip_param_types, CS, Xspace_is_Xexp):\n",
    "    #Make eval point base a tensor\n",
    "    if torch.is_tensor(eval_p_base) == False:\n",
    "        eval_p_base = torch.tensor(eval_p_base)\n",
    "    q = len(eval_p_base)\n",
    "    #Create list to save evaluated arrays in and arrays to store GP mean/stdev and true predictions in\n",
    "    GP_mean_all = np.zeros((len(X_set), q, value_num) )\n",
    "    GP_stdev_all = np.zeros((len(X_set), q, value_num) )\n",
    "    y_sim_all = np.zeros((len(X_set), q, value_num) )\n",
    "\n",
    "    #Create list to save evaluated parameter sets and values in\n",
    "    eval_p_df = []\n",
    "    values_list = [] \n",
    "\n",
    "    ##Evaluate parameter sets at each Xspace value\n",
    "    #Loop over all parameters\n",
    "    for i in range(len(eval_p_base)):   \n",
    "        #Clone the base value\n",
    "        eval_p = eval_p_base.clone()\n",
    "        #Define upper and lower theta bounds\n",
    "        lower_theta = bounds_p[0,i]\n",
    "        upper_theta = bounds_p[1,i]\n",
    "        #Define Values to test\n",
    "        values = np.linspace(lower_theta, upper_theta, value_num) #Note: Default to 41 \n",
    "        values_list.append(values)\n",
    "        #Save each bound value as a number from 0 to len(percentiles)\n",
    "        val_num_map = np.linspace(0,len(values)-1, len(values))\n",
    "        #Define parameter sets to test   \n",
    "        for j in range(len(values)):   \n",
    "            # Evaluate at the original point for each parameter and swap a parameter value for a value within the bounds\n",
    "            new_eval_p = values[j]\n",
    "            #Change the value to the exact point except for 1 variable that is rounded to 2 sig figs after modification by a percent\n",
    "            eval_p[i] = torch.tensor(float('%.2g' % float(new_eval_p)))\n",
    "            #Append evaluated value to this list only on 1st iteration of k\n",
    "            eval_p_df.append(list(eval_p.numpy()))\n",
    "            #Loop over Xspace Values: #Note. X_space defined as Xexp points we want to test\n",
    "            for k in range(len(X_set)):\n",
    "                #Evaluate the values\n",
    "                eval_components_Xset = eval_GP_scipy(eval_p, X_set[k], Constants, model, skip_param_types, \n",
    "                                                     CS, Xspace_is_Xexp = False)\n",
    "                #Get GP predictions and true values\n",
    "                GP_mean_Xset, GP_stdev_Xset, y_sim_Xset = eval_components_Xset\n",
    "                #Append GP mean, GP_stdev, and true values to arrays outside of loop\n",
    "                GP_mean_all[k,i,j], GP_stdev_all[k,i,j], y_sim_all[k,i,j] = GP_mean_Xset, GP_stdev_Xset, y_sim_Xset\n",
    "\n",
    "    return y_sim_all, GP_mean_all, GP_stdev_all, values_list, val_num_map\n",
    "\n",
    "Evals  = Compare_GP_True_Param_Sens(eval_p_base, X_set, value_num, bounds_p, Constants, gaussian_process, \n",
    "                                    skip_param_types, CS, Xspace_is_Xexp)\n",
    "\n",
    "y_sim_all, GP_mean_all, GP_stdev_all, values_list, val_num_map = Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcbfbdae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot GP vs y_sim predictions for each Xexp Value and save data\n",
    "all_data_to_plot = [y_sim_all, GP_mean_all, GP_stdev_all] \n",
    "#Note: Plotting function pulled from\n",
    "mul_plot_param(all_data_to_plot, set_lenscl, train_iter, t, CS, Bound_Cut, X_set, x_set_points, \n",
    "               param_dict, values_list, val_num_map, lenscl_final, lenscl_noise_final, kernel_func, DateTime, Xexp, save_csvs, \n",
    "               save_figure, package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb55333",
   "metadata": {},
   "source": [
    "# Section 5: Manual Lengthscale Sensitivity Analysis\n",
    "\n",
    "parameter vs Muller potential Plots Generated: Test_Figs/Figures/GP_Vs_Sim_Comp_CB/CS_2.2/train_iter_300/TP_20/scikit_learn/Mat_52/len_scl_XXX/X_val_num_Y-param_Z.png\n",
    "\n",
    "Plotting function for above in bo_methods_lib/GP_Vs_True_Param_Sens.py\n",
    "\n",
    "Heat Maps Generated: Test_Figs/Figures/GP_Vs_Sim_Comp_CB/CS_2.2/train_iter_300/TP_20/scikit_learn/Mat_52/len_scl_XXX/Mul_Comp_Figs.png\n",
    "\n",
    "Plotting function for above in bo_methods_lib/GP_Vs_True_Sensitivity.py\n",
    "\n",
    "200 Theta version of this experiments can be found in /cratch365/mcarlozo/Toy_Problem/2023/04/06. From here 18-35/* represents the kernel noise of 0 experiment, 18-50 is the kernel noise of 1 experiment, and 19-40 is the varying lengthscale and kernel of 1 experiment\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "XXX is a fill in for the lengthscale. If this value is optimized, XXX = \"varies\"\n",
    "\n",
    "\"param_Z\" is a marker for a1, a2, a3,... b4\n",
    "\n",
    "Y is a fill in for a numbered X space point (not necessarily corresponding to the one in Xexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e26ea8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengthscale is optimized using MLE to  ['3.608e-01', '4.103e+05', '1.000e+07', '2.352e+00', '1.000e+07', '1.000e+07', '2.957e+00', '1.000e+07', '1.686e-01', '1.632e-01']\n",
      "Noise is optimized using MLE to  0.000e+00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lenscl_noises = True\n",
    "lengthscales = [None]\n",
    "\n",
    "for lenscl in lengthscales:\n",
    "    train_GP = train_GP_scikit(X_train, Y_train, noise_std, kernel_func, verbose, lenscl, initialize, \n",
    "                               rand_seed = False, noisy = lenscl_w_noise) \n",
    "\n",
    "    lenscl_final, lenscl_noise_final, gaussian_process = train_GP\n",
    "    #Evaluate GP for true parameter set values over all of parameter space bounds\n",
    "    Evals  = Compare_GP_True_Param_Sens(eval_p_base, X_set, value_num, bounds_p, Constants, gaussian_process, \n",
    "                                    skip_param_types, CS, Xspace_is_Xexp)\n",
    "\n",
    "    y_sim_all, GP_mean_all, GP_stdev_all, values_list, val_num_map = Evals\n",
    "\n",
    "    #Plot GP vs y_sim predictions for each Xexp Value and save data\n",
    "    all_data_to_plot = [y_sim_all, GP_mean_all, GP_stdev_all] \n",
    "\n",
    "    #Evaluate GP for true parameter set value over all of X parameter space\n",
    "    GP_mean, GP_stdev, y_sim = eval_GP_scipy(eval_p_base, X_space, Constants, gaussian_process, skip_param_types, \n",
    "                                             CS, False)\n",
    "\n",
    "    #Plot GP Predictions and True value\n",
    "    title = [\"Y Sim\", \"GP Mean\", \"GP St.Dev\"]\n",
    "    Mul_title = [\"/Sim_val\", \"/GP_mean\", \"/GP_stdev\"]\n",
    "    z = [y_sim.T, GP_mean.T, GP_stdev.T]\n",
    "    #Not sure how to find stdev of the lengthscale\n",
    "    #Note: Pulled this plotting code from file bo_methods_lib/GP_Vs_True_Sensitivity.py\n",
    "    Muller_plotter(X_mesh, z, minima, saddle, title, lenscl, train_iter, t, CS, Bound_Cut, \n",
    "                   lenscl_final, lenscl_noise_final, kernel_func, DateTime, Xexp, save_csvs, save_figure, Mul_title, package = package)\n",
    "\n",
    "    mul_plot_param(all_data_to_plot, lenscl, train_iter, t, CS, Bound_Cut, X_set, x_set_points, \n",
    "                   param_dict, values_list, val_num_map, lenscl_final, lenscl_noise_final, kernel_func, DateTime, Xexp, save_csvs, \n",
    "                   save_figure, package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf6a9c8",
   "metadata": {},
   "source": [
    "# Section 6: Dense Grid x1 and x2 + LHS Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995efa15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972142f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
