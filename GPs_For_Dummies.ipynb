{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94212579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/crc.nd.edu/user/m/mcarlozo/.conda/envs/Toy_Problem_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import math\n",
    "import scipy.optimize as optimize\n",
    "import itertools\n",
    "from scipy.stats import norm\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.stats import qmc\n",
    "import scipy.special\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, WhiteKernel, ConstantKernel, DotProduct\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import bo_methods_lib\n",
    "from bo_methods_lib.bo_methods_lib.analyze_data import * #Fix this later\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Classes_New import * #Fix this later\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Class_fxns import * #Fix this later\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Classes_plotters import * #Fix this later\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0598ad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cs1_polynomial(true_model_coefficients, x):\n",
    "    \"\"\"\n",
    "    Calculates the value of y for case study 1\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    true_model_coefficients: ndarray, The array containing the true values of Theta1 and Theta2\n",
    "    x: ndarray, The list of xs that will be used to generate y\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    y_poly: ndarray, The noiseless values of y given theta_true and x\n",
    "    \"\"\"\n",
    "    \n",
    "    y_poly =  true_model_coefficients[0]*x + true_model_coefficients[1]*x**2 +x**3\n",
    "       \n",
    "    return y_poly\n",
    "\n",
    "# ## define function that includes nonlinear model\n",
    "# def model(theta,x):\n",
    "#     '''\n",
    "#     Toy model\n",
    "#     Arguments:\n",
    "#         theta: parameter vector\n",
    "#         x: independent variable vector\n",
    "#     Returns:\n",
    "#         y_model: dependent variable prediction\n",
    "#     '''\n",
    "#     y_model = theta[0]*x + theta[1]*x**2 + x**3\n",
    "#     return y_model\n",
    "\n",
    "# ##New Cell\n",
    "\n",
    "# # Create a function to optimize, in this case, least squares fitting\n",
    "# def regression_func(theta, x, y):\n",
    "#     '''\n",
    "#     Function to define regression function for least-squares fitting\n",
    "#     Arguments:\n",
    "#         theta: parameter vector\n",
    "#         x: independent variable vector\n",
    "#         y: dependent variable vector (measurements)\n",
    "#     Returns:\n",
    "#         e: residual vector\n",
    "#     '''\n",
    "    \n",
    "#     error = y - model(theta,x); #NOTE: Least squares will calculate sse based off this to minimize\n",
    "    \n",
    "#     return error\n",
    "# #New Cell\n",
    "\n",
    "# #Create a function to define the SSE for any Theta vector on a heat map.\n",
    "# def sse_func(xx, yy, x, y):\n",
    "#     '''\n",
    "#     Function to define define sum of squared error function for heat map\n",
    "#     Arguments:\n",
    "#         xx: An N X D array of all Theta1 values\n",
    "            \n",
    "#         yy: An D X N array of all Theta2 values\n",
    "#         theta: parameter vector\n",
    "#         x: independent variable vector (predicted x values including noise)\n",
    "#         y: dependent variable vector (predicted y values on Heat Map)\n",
    "#     Returns:\n",
    "#         sse: N x N sum of squared error matrix of all generated combination of xx and yy\n",
    "#     '''\n",
    "#     sse = np.zeros([len(xx),len(yy)])\n",
    "    \n",
    "#     for i in range(len(xx)):\n",
    "#         for j in range(len(yy)):\n",
    "#             theta = np.array([xx[i][j],yy[i][j]])\n",
    "#             sse[i][j] = sum((y - model(theta,x))**2) \n",
    "    \n",
    "#     return sse\n",
    "\n",
    "\n",
    "def lhs_sampling(num_points, bounds, seed):\n",
    "    \"\"\"\n",
    "    Design LHS Samples\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_points: int, number of points in LHS, should be greater than # of dimensions\n",
    "    bounds: ndarray, array containing upper and lower bounds of elements in LHS sample. Defaults of 0 and 1\n",
    "    seed: int, seed of random generation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lhs_data: ndarray, array of LHS sampling points with length (num_points) \n",
    "    \"\"\"\n",
    "    #Define number of dimensions\n",
    "    dimensions = bounds.shape[1]\n",
    "    #Define sampler\n",
    "    sampler = qmc.LatinHypercube(d=dimensions, seed = seed)\n",
    "    lhs_data = sampler.random(n=num_points)\n",
    "\n",
    "    #Generate LHS data given bounds\n",
    "    lhs_data = qmc.scale(lhs_data, bounds[0], bounds[1]) #Using this because I like that bounds can be different shapes\n",
    "\n",
    "    return lhs_data\n",
    "\n",
    "\n",
    "\n",
    "theta_names = ['theta_1', 'theta_2']\n",
    "bounds_x_l = [-2]\n",
    "bounds_x_u = [2]\n",
    "bounds_theta_l = [-2, -2]\n",
    "bounds_theta_u = [ 2,  2]\n",
    "theta_ref = np.array([1.0, -1.0])   \n",
    "noise_mean = 0\n",
    "noise_std = 0.1\n",
    "\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923ab248",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(int(seed))\n",
    "\n",
    "Xexp = np.linspace(-2,2,5)\n",
    "Yexp = calc_cs1_polynomial(theta_ref, Xexp)\n",
    "\n",
    "#Creates noise values with a certain stdev and mean from a normal distribution\n",
    "noise = np.random.normal(size=len(Yexp), loc = noise_mean, scale = noise_std)\n",
    "#Add noise to data\n",
    "Yexp = Yexp + noise\n",
    "\n",
    "# Evaluate model based on the assumed experimental values\n",
    "X = np.linspace(np.min(Xexp),np.max(Xexp),100)\n",
    "Y = calc_cs1_polynomial(theta_ref, X)\n",
    "\n",
    "#Set GP Model\n",
    "noise_kern = WhiteKernel(noise_level = noise_std**2, noise_level_bounds= \"fixed\") #bounds = \"fixed\"\n",
    "c_kern = ConstantKernel(constant_value= np.ones(1), constant_value_bounds = (1e-3, 1e3))\n",
    "Mat_kern = Matern(length_scale = 1, length_scale_bounds=(1e-3, 1e3), nu=2.5)\n",
    "# RBF_kern = RBF(length_scale_bounds=(1e-14, 1e14))\n",
    "# Dot_kern = DotProduct(sigma_0=1, sigma_0_bounds=(1e-14, 1e14))\n",
    "\n",
    "kernel = c_kern*Mat_kern + noise_kern\n",
    "gp_model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=16, random_state = seed,\n",
    "                                   optimizer = \"fmin_l_bfgs_b\")\n",
    "\n",
    "fit_gp_model = gp_model.fit(Xexp.reshape(-1,1), Yexp)\n",
    "Y_gp, stdev_gp = fit_gp_model.predict(X.reshape(-1,1), return_std=True)\n",
    "__, cov_2 = fit_gp_model.predict(X.reshape(-1,1), return_cov = True)\n",
    "print(fit_gp_model.kernel_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57023b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot covariance\n",
    "#Plot\n",
    "XX_mesh = np.array(np.meshgrid(X,X))\n",
    "X1, X2 = XX_mesh\n",
    "z = cov_2\n",
    "tot_lev = 150\n",
    "#Set plot details for mean predictions\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (6,6))\n",
    "ax = axes\n",
    "title = \"Covariance\"\n",
    "# title = [\"Mean\", \"StDev\"]\n",
    "     \n",
    "#Create a colormap and colorbar for each subplot\n",
    "cs_fig = ax.contourf(X1, X2,z, levels = 900, cmap = \"jet\")\n",
    "if np.amax(z) < 1e-1 or np.amax(z) > 1000:\n",
    "    cbar = plt.colorbar(cs_fig, ax = ax, format='%.2e')\n",
    "else:\n",
    "    cbar = plt.colorbar(cs_fig, ax = ax, format = '%2.2f')\n",
    "\n",
    "#Create a line contour for each colormap\n",
    "cs2_fig = ax.contour(cs_fig, levels=cs_fig.levels[::tot_lev], colors='k', alpha=0.7, linestyles='dashed', linewidths=3)\n",
    "ax.clabel(cs2_fig,  levels=cs_fig.levels[::tot_lev], fontsize=10, inline=1)\n",
    "ax.scatter(Xexp,Xexp, color=\"green\",s=25, label = \"Training Data\", marker = \"o\")  \n",
    "\n",
    "#Get legend information\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "#Plots axes such that they are scaled the same way (eg. circles look like circles) and name axes\n",
    "ax.axis('scaled')  \n",
    "ax.set_xlabel('$x_1$',weight='bold',fontsize=16)\n",
    "ax.set_ylabel('$x_2$',weight='bold',fontsize=16)\n",
    "\n",
    "#Plot title and set axis scale\n",
    "ax.set_title(title, weight='bold',fontsize=16)\n",
    "ax.set_xlim(left = np.amin(X1), right = np.amax(X1))\n",
    "ax.set_ylim(bottom = np.amin(X2), top = np.amax(X2))      \n",
    "\n",
    "#Plots legend and title\n",
    "plt.tight_layout()\n",
    "fig.legend(handles, labels, loc=\"upper left\")  #bbox_to_anchor=(-0.01, 0.9), borderaxespad=0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e58c61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = ([-np.inf, -np.inf], [np.inf, np.inf])\n",
    "theta0 = np.random.rand(2) * (2+2) -2\n",
    "Solution = optimize.least_squares(regression_func, theta0,bounds=bounds, method='trf',args=(Xexp, Yexp),verbose=2)\n",
    "\n",
    "theta = Solution.x\n",
    "print(\"theta = \",theta)\n",
    "X_pred = X\n",
    "Y_pred = model(theta, X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70652d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(X,Y,'b-',linewidth=2,label=r\"$y$\")\n",
    "plt.scatter(Xexp,Yexp, color = \"r\", s=20,label=r\"$y_{exp}$\")\n",
    "plt.plot(X,Y,'r-',linewidth=1,label=r\"$y_{true}$\")\n",
    "plt.plot(X_pred, Y_pred, linestyle = \"--\", color = \"g\", label=r\"$NLR$\")\n",
    "# plt.plot(X,Y_gp,'b',linestyle = \"--\",label=r\"$y_{gp}$\", linewidth = 3)\n",
    "# plt.fill_between(X, Y_gp-2*stdev_gp, Y_gp+2*stdev_gp, alpha=0.2, color = \"b\", label = \"95% CI\")\n",
    "# plt.title(r\"all trained\")\n",
    "plt.xlabel(r\"$x$\",fontsize=14)\n",
    "plt.ylabel(r'$y$',fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01501d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(X,Y,'b-',linewidth=2,label=r\"$y$\")\n",
    "plt.scatter(Xexp,Yexp, color = \"r\", s=20,label=r\"$y$\")\n",
    "plt.plot(X,Y,'r-',linewidth=1,label=r\"$f(\\mathbf{\\theta_{true}})$\")\n",
    "plt.plot(X_pred, Y_pred, linestyle = \"--\", color = \"g\", label= \"NLR \" + r\"$f(\\mathbf{\\theta})$\")\n",
    "# plt.plot(X,Y_gp,'b',linestyle = \"--\",label=r\"$y_{gp}$\", linewidth = 3)\n",
    "# plt.fill_between(X, Y_gp-2*stdev_gp, Y_gp+2*stdev_gp, alpha=0.2, color = \"b\", label = \"95% CI\")\n",
    "# plt.title(r\"all trained\")\n",
    "plt.xlabel(r\"$x$\",fontsize=14)\n",
    "plt.ylabel(r'$y$',fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be1d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_x_vs_y_given_theta2(data, exp_data, train_data, test_data, xbins, ybins, title, x_label, y_label, title_fontsize = 24, other_fontsize = 20, save_path = None):\n",
    "    \"\"\"\n",
    "    Plots x data vs y data for any given parameter set theta\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: Instance of Data,\n",
    "    \n",
    "    \"\"\"\n",
    "    subplots_needed = data.get_dim_x_vals()\n",
    "    fig, ax, num_subplots = create_subplots(subplots_needed)\n",
    "    \n",
    "    #Print the title and labels as appropriate\n",
    "    set_plot_titles(fig, title, x_label, y_label, title_fontsize, other_fontsize)\n",
    "        \n",
    "    #Loop over different hyperparameters (number of subplots)\n",
    "    for i in range(num_subplots):\n",
    "        #If you still have data to plot\n",
    "        if i < data.get_dim_x_vals():\n",
    "            #The index of the data is i, and one data type is in the last row of the data\n",
    "            X_space = data.x_vals[:,i]\n",
    "            if not len(train_data.y_vals) == len(train_data.x_vals):            \n",
    "                ax[i].plot(X_space, np.zeros(len(X_space)), label = r\"$y_{true}$\", color = \"red\")\n",
    "                y_NLR = regression_func(theta, exp_data.x_vals[:,i], exp_data.y_vals)\n",
    "                ax[i].plot(exp_data.x_vals[:,i], y_NLR, linestyle = \"--\", color = \"g\", label=r\"$NLR$\")    \n",
    "            else:\n",
    "                if exp_data is not None:\n",
    "                    ax[i].scatter(exp_data.x_vals[:,i], exp_data.y_vals, color = \"red\", marker = \"o\", label = r\"$y_{exp}$\")\n",
    "                ax[i].plot(X_space, data.y_vals, label = r\"$y_{true}$\", color = \"red\")            \n",
    "                ax[i].plot(X_space, model(theta, X_space), linestyle = \"--\", color = \"g\", label=r\"$NLR$\")   \n",
    "                \n",
    "            ax[i].plot(X_space, data.gp_mean, lw=2, label=r\"$y_{gp}$\", color = \"blue\")\n",
    "                \n",
    "#             if train_data is not None:\n",
    "#                 ax[i].scatter(train_data.x_vals[:,i], train_data.y_vals, color = \"green\",  s=150, marker = \"x\", label = \"Training\")\n",
    "#             if test_data is not None:\n",
    "#                 ax[i].scatter(test_data.x_vals[:,i], test_data.y_vals, color = \"red\", s=100, marker = \"x\", label = \"Testing\")\n",
    "            \n",
    "            ax[i].fill_between(\n",
    "                X_space,\n",
    "                data.gp_mean - 1.96 * np.sqrt(data.gp_var),\n",
    "                data.gp_mean + 1.96 * np.sqrt(data.gp_var),\n",
    "                alpha=0.3, label = \"95% CI\" )\n",
    "            subplot_details(ax[i], X_space, None, None, None, None , xbins, ybins, other_fontsize)\n",
    "        #Set axes off if it's an extra\n",
    "        else:\n",
    "            ax[i].set_axis_off()\n",
    "            \n",
    "        #Fetch handles and labels on last iteration\n",
    "        if i == num_subplots-1:\n",
    "            handles, labels = ax[i].get_legend_handles_labels()\n",
    "            \n",
    "    #Plots legend and title\n",
    "    plt.tight_layout()\n",
    "    fig.legend(handles, labels, loc= \"upper left\", fontsize = other_fontsize, bbox_to_anchor=(1.0, 0.95), borderaxespad=0)\n",
    "    \n",
    "    #save or show figure\n",
    "    if save_path is None:\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    else:\n",
    "        save_fig(save_path, ext='png', close=True, verbose=False)  \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d66651",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot xy data\n",
    "path_list = [\"2b0d1ee15911cd245f17a7d10a7abf3e\",\n",
    "             \"b0e257d6e76025909e364903b3b792ee\",\n",
    "             \"d0c8b17229671998f3ffdab06c79e1f9\",\n",
    "             \"fdc2673ac83f3d0cf5b3b969a3e80825\",\n",
    "             \"9042085b58a00b76d6c2ae0a20cfe9f3\"]\n",
    "\n",
    "df_best_path = \"workspace/\" + path_list[0] + \"/ep_study_best_all.csv\"\n",
    "df_best = pd.read_csv(df_best_path, header = 0, index_col = 0)\n",
    "run_num_list = list(map(int, df_best[\"Run Number\"].to_numpy() + 1))\n",
    "bo_iter_list = list(map(int, df_best[\"BO Iter\"].to_numpy() + 1))\n",
    "\n",
    "for i in range(len(path_list)):\n",
    "    file_path = \"workspace/\" + path_list[i] + \"/BO_Results.gz\"\n",
    "    \n",
    "    run_num = run_num_list[i]\n",
    "    bo_iter = bo_iter_list[i]\n",
    "    x_lin_pts = 50\n",
    "\n",
    "    title_fontsize = 24\n",
    "    other_fontsize = 20\n",
    "    save_path = None\n",
    "\n",
    "    xbins = 5\n",
    "    ybins = 5\n",
    "    title = None\n",
    "    x_label = r\"X\"\n",
    "    y_label = r\"$e(\\theta)$\"\n",
    "\n",
    "    theta_opt_data, exp_data, train_data, test_data = analyze_xy_plot(file_path, run_num, bo_iter, x_lin_pts)\n",
    "    plot_x_vs_y_given_theta2(theta_opt_data, exp_data, train_data, None, xbins, ybins, title, x_label, y_label, \n",
    "                            title_fontsize, other_fontsize, save_path)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b47298ef",
   "metadata": {},
   "source": [
    "def acquisition_EI(mean, variance, output, xi=0.01):\n",
    "    best = np.max(output)\n",
    "    mu, std = mean, variance\n",
    "    mu = mu.reshape(-1, 1)\n",
    "    std = std.reshape(-1, 1)\n",
    "    imp = best -mu - xi\n",
    "    Z = imp / std\n",
    "    ei = imp * norm.cdf(Z) + std * norm.pdf(Z)\n",
    "    ei[std == 0.0] = 0.0\n",
    "    return ei.reshape(-1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2152240",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def acquisition_EI(mean, variance, output, xi=0.01):\n",
    "    best = np.max(output)\n",
    "    mu, std = mean, variance\n",
    "    mu = mu.reshape(-1, 1)\n",
    "    std = std.reshape(-1, 1)\n",
    "    imp = best -mu - xi\n",
    "    Z = imp / std\n",
    "    ei = imp * norm.cdf(Z) + std * norm.pdf(Z)\n",
    "    ei[std == 0.0] = 0.0\n",
    "    return ei.reshape(-1)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "save_figure = False\n",
    "#Original z 0.0015 * x**5 - 0.055 * x**4 + 0.65 * x**3 - 2.8 * x**2 + 3.0 * x + 6.3\n",
    "# 目标函数                                                                                                                                                                  \n",
    "objective = np.vectorize(lambda x, sigma_n=0.01: 1*x -1*x**2 +x**3  + np.random.normal(0, sigma_n))\n",
    "\n",
    "# 采样函数 - GP-UCB                                                                                                                                                         \n",
    "GPUCB = np.vectorize(lambda mu, sigma, t, ld, delta=0.1: mu + (1 * 2 * np.log(ld * t**2 * np.pi**2 / (6 * delta)))**0.5 * sigma)\n",
    "\n",
    "# 超参数                                                                                                                                                                    \n",
    "mean, l, sigma_f, sigma_n = 5, 1, 1, 0.01\n",
    "\n",
    "# 迭代次数                                                                                                                                                                  \n",
    "max_iter = 3\n",
    "\n",
    "# SE协方差函数                                                                                                                                                              \n",
    "kernel = lambda r_2, l: np.exp(-r_2 / (2 * l**2))\n",
    "\n",
    "# 初始训练样本，以一维输入为例                                                                                                                                              \n",
    "X = np.linspace(-2,2,5)\n",
    "X = X.reshape(X.size, 1)\n",
    "Y = objective(X).flatten()\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "j = 0\n",
    "for i in range(max_iter):\n",
    "  \n",
    "    Xs = np.linspace(-2,2,100)\n",
    "    Xs = Xs.reshape(Xs.size, 1)\n",
    "\n",
    "    n, d = X.shape\n",
    "    t = np.repeat(X.reshape(n, 1, d), n, axis=1) - X\n",
    "    r_2 = np.sum(t**2, axis=2)\n",
    "    Kf = sigma_f**2 * kernel(r_2, l)\n",
    "    Ky = Kf + sigma_n**2 * np.identity(n)\n",
    "    Ky_inv = np.linalg.inv(Ky)\n",
    "\n",
    "    m = Xs.shape[0]\n",
    "    t = np.repeat(Xs.reshape(m, 1, d), n, axis=1) - X\n",
    "    r_2 = np.sum(t**2, axis=2).T\n",
    "    kf = sigma_f**2 * kernel(r_2, l)\n",
    "\n",
    "    mu = mean + kf.T @ Ky_inv @ (Y - mean)\n",
    "    sigma = np.sqrt(sigma_f**2 - np.sum(kf.T @ Ky_inv * kf.T, axis=1))\n",
    "    print(sigma)\n",
    "\n",
    "    # y_acf = GPUCB(mu, sigma, i + 1, n)\n",
    "    y_acf = acquisition_EI(mu, sigma, Y)\n",
    "\n",
    "    sample_x = np.around(Xs[np.argmax(y_acf)])\n",
    "    print(sample_x)\n",
    "\n",
    "\n",
    "    x_test = Xs.flatten()\n",
    "    y_obj = objective(x_test).flatten()\n",
    "\n",
    "    j = j+1\n",
    "    ax = plt.figure(figsize=(6, 4), dpi=200)\n",
    "#     ax = plt.subplot(max_iter, 2, j)\n",
    "    # ax.set_title('Iteration %d' % (i + 1), fontsize=20)\n",
    "#     plt.ylim(0, 14)\n",
    "    \n",
    "    if i == 0:\n",
    "      plt.plot(x_test, mu, c='tab:blue', lw=2, label='Predicted Mean')\n",
    "      plt.fill_between(x_test, mu + 1.96*sigma, mu - 1.96*sigma, alpha=0.2, color='tab:blue', lw=0, label='95% Prediction Interval')\n",
    "      plt.plot(x_test, y_obj, c='red', ls='--', lw=2, label = 'Target Function')\n",
    "      plt.scatter(X, Y, c='red', marker='o', s=80, label = 'Observations')\n",
    "      plt.xlabel(r'$\\mathit{z}$', fontsize=20)\n",
    "    \n",
    "    if i != 0:\n",
    "      plt.plot(x_test, mu, c='tab:blue', lw=2, label='Predicted Mean')\n",
    "      plt.fill_between(x_test, mu + 1.96*sigma, mu - 1.96*sigma, alpha=0.2, color='tab:blue', lw=0, label='95% Prediction Interval')\n",
    "      plt.plot(x_test, y_obj, c='red', ls='--', lw=2, label = 'Target Function')\n",
    "\n",
    "      text = 'New Observation ' + r'($\\bar{\\mathit{z}}_' + str(i) + r'$)'\n",
    "      plt.scatter(X[1:], Y[1:], c='red', marker='o', s=80, label = 'Observations')\n",
    "      plt.scatter(X[0], Y[0], marker='*', c='navy',s=150, label=text)\n",
    "      plt.xlabel(r'$\\mathit{z}$', fontsize=20)\n",
    "\n",
    "#     plt.ylabel('Iteration  ' + str(i+1), fontsize=20)\n",
    "    plt.ylabel(r'$f(\\mathit{z})$', fontsize=20)\n",
    "    \n",
    "    if i == max_iter-1:\n",
    "      plt.xlabel(r'$\\mathit{z}$', fontsize=20)\n",
    "#     if i == 0:\n",
    "#       plt.title('$f(x)$', fontsize=20)\n",
    "    plt.xticks(np.arange(-2, 2.1, 1.0), fontsize = 15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.legend(fontsize=12, loc='upper left')\n",
    "#     plt.grid()\n",
    "    plt.tight_layout()\n",
    "    if save_figure == True:\n",
    "        plt.savefig('BO_demo_min_Obj_z'+str(i+1))\n",
    "        plt.close()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    j = j + 1\n",
    "#     plt.subplot(max_iter, 2, j)\n",
    "    plt.figure(figsize=(6, 4), dpi=200)\n",
    "    # plt.ylim(-0.001, 0.10)\n",
    "    plt.ticklabel_format(axis='y', style='sci', scilimits=[1e-2, 1e2])\n",
    "    plt.plot(x_test, y_acf, c='darkgreen', lw=2, label='Acquisition Function')\n",
    "#     if i == 0:\n",
    "#       plt.title('Expected Improvement', fontsize=20)\n",
    "    plt.ylabel(r'$EI(\\mathit{z})$', fontsize=20)\n",
    "#     if i == 3:\n",
    "    plt.xlabel(r'$\\mathit{z}$', fontsize=20)\n",
    "\n",
    "    text = 'EI Maximum ' + r'($\\bar{\\mathit{z}}_' + str(i+1) + r'$)'\n",
    "    print(text)\n",
    "\n",
    "    plt.scatter(x_test[np.argmax(y_acf)], y_acf[np.argmax(y_acf)], marker='*', c='navy',s=150,  label=text)\n",
    "    plt.legend(fontsize=15, loc='lower center')\n",
    "    # plt.title('Acquisition Function')\n",
    "    plt.xticks(np.arange(-2, 2.1, 1.0), fontsize = 15)\n",
    "    plt.yticks(fontsize=15)\n",
    "#     plt.grid()\n",
    "\n",
    "    X = np.insert(X, 0, sample_x, axis=0)\n",
    "    Y = np.insert(Y, 0, objective(sample_x))\n",
    "    \n",
    "    print(x_test[np.argmax(y_acf)])\n",
    "    plt.tight_layout()\n",
    "    if save_figure == True:\n",
    "        plt.savefig('BO_demo_min_EI_z'+str(i+1))\n",
    "        plt.close()\n",
    "#     plt.show()\n",
    "    # plt.suptitle('Bayesian Optimization Example \\n $f(x) = 0.0015  x^5 - 0.055x^4 + 0.65 x^3 - 2.8 x^2 + 3.0 x + 6.3$', fontsize=25)\n",
    "# if save_figure == True:\n",
    "#     plt.savefig('BO_demo_1')\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
