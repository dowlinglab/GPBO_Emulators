{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from scipy.stats import qmc\n",
    "import itertools\n",
    "from itertools import combinations_with_replacement, combinations, permutations\n",
    "\n",
    "import bo_methods_lib\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Classes_New import * #Fix this later\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Class_fxns import * #Fix this later\n",
    "from bo_methods_lib.bo_methods_lib.analyze_data import * #Fix this later\n",
    "from bo_methods_lib.bo_methods_lib.GPBO_Classes_plotters import * #Fix this later\n",
    "import pympler\n",
    "import pickle\n",
    "import signac\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from textwrap import fill\n",
    "\n",
    "from pympler import asizeof\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 200\n",
    "\n",
    "#Ignore inconcistent version warning\n",
    "import warnings\n",
    "# from sklearn.exceptions import InconsistentVersionWarning\n",
    "# warnings.filterwarnings(action='ignore', category=InconsistentVersionWarning)\n",
    "import gpflow\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Simulator and Training Data\n",
    "cs_name_val = 1\n",
    "noise_mean = 0\n",
    "noise_std = None\n",
    "seed = 1\n",
    "#Define method, ep_enum classes, indecies to consider, and kernel\n",
    "meth_name = Method_name_enum(3)\n",
    "method = GPBO_Methods(meth_name)\n",
    "gen_meth_theta = Gen_meth_enum(1)\n",
    "gen_meth_x = Gen_meth_enum(2)\n",
    "num_x_data = 5\n",
    "\n",
    "#Define Simulator Class (Export your Simulator Object Here)\n",
    "simulator = simulator_helper_test_fxns(cs_name_val, noise_mean, noise_std, seed)\n",
    "num_theta_data = len(simulator.indeces_to_consider)*10\n",
    "#Generate Exp Data (OR Add your own experimental data as a Data class object)\n",
    "set_seed = 1 #Set set_seed to 1 for data generation\n",
    "gen_meth_x = Gen_meth_enum(gen_meth_x)\n",
    "exp_data = simulator.gen_exp_data(num_x_data, gen_meth_x, set_seed)\n",
    "#Set simulator noise_std artifically as 5% of y_exp mean (So that noise will be set rather than trained)\n",
    "simulator.noise_std = np.abs(np.mean(exp_data.y_vals))*0.05\n",
    "print(simulator.noise_std)\n",
    "#Note at present, training data is always the same between jobs since we set the data generation seed to 1\n",
    "sim_data = simulator.gen_sim_data(num_theta_data, num_x_data, gen_meth_theta, gen_meth_x, 1.0, seed, False)\n",
    "val_data = simulator.gen_sim_data(10, 10, Gen_meth_enum(1), Gen_meth_enum(1), 1.0, simulator.seed, False)\n",
    "# print(sim_data.theta_vals)\n",
    "all_gp_data = sim_data\n",
    "all_val_data = val_data\n",
    "\n",
    "if method.emulator == True:\n",
    "    all_gp_data = sim_data\n",
    "    all_val_data = val_data\n",
    "else:\n",
    "    all_gp_data = simulator.sim_data_to_sse_sim_data(method, sim_data, exp_data, 1.0, False)\n",
    "    print(len(all_gp_data.theta_vals), len(all_gp_data.x_vals), len(all_gp_data.y_vals))\n",
    "    all_val_data = simulator.sim_data_to_sse_sim_data(method, val_data, exp_data, 1.0, False)\n",
    "    print(len(all_val_data.theta_vals), len(all_val_data.x_vals), len(all_val_data.y_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test GPs\n",
    "#Make CS Params\n",
    "retrain_GP = 5\n",
    "normalize = True\n",
    "noise_std = simulator.noise_std #Yexp_std is exactly the noise_std of the GP Kernel\n",
    "print(noise_std)\n",
    "\n",
    "#Make Emulator\n",
    "#Evaluate GP Mean and Variance\n",
    "# gp_object = Type_1_GP_Emulator(all_gp_data, all_val_data, None, None, None, Kernel_enum(1), None, noise_std, None, \n",
    "#                                 retrain_GP, seed, normalize, None, None, None, None)\n",
    "gp_object = Type_2_GP_Emulator(all_gp_data, all_val_data, None, None, None, Kernel_enum(1), None, noise_std, None, \n",
    "                                retrain_GP, seed, normalize, None, None, None, None)\n",
    "#Choose training data\n",
    "train_data, test_data = gp_object.set_train_test_data(1.0, seed)\n",
    "gp_object.scalerY = gp_object.scalerY.fit(gp_object.train_data.y_vals.reshape(-1,1))\n",
    "gp_object.scalerX = gp_object.scalerX.fit(gp_object.feature_train_data)\n",
    "print(gp_object.scalerY.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl_feat_data = gp_object.scalerX.transform(gp_object.feature_train_data)\n",
    "scl_y_data = gp_object.scalerY.transform(gp_object.train_data.y_vals.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get lengthscale min and max differences\n",
    "#Transform X data\n",
    "points = gp_object.scalerX.transform(gp_object.train_data_init)\n",
    "# Compute pairwise differences for each column\n",
    "pairwise_diffs = np.abs(points[:, :, None] - points[:, :, None].transpose(0, 2, 1))\n",
    "# Compute Euclidean distances\n",
    "euclidean_distances = np.sqrt(np.sum(pairwise_diffs ** 2, axis=1))\n",
    "# Set diagonal elements (distance between the same point) to infinity\n",
    "np.fill_diagonal(euclidean_distances, np.inf)\n",
    "euclidean_distances = np.ma.masked_invalid(euclidean_distances)\n",
    "# Find the smallest/largest distance for each column\n",
    "min_distance = np.min(euclidean_distances, axis=0)\n",
    "max_distance = np.max(euclidean_distances, axis=0)\n",
    "# min_distance = np.min(euclidean_distances)\n",
    "# max_distance = np.max(euclidean_distances)\n",
    "d_guess = np.average((min_distance, max_distance), axis = 0)\n",
    "print(d_guess)\n",
    "print(min_distance)\n",
    "print(max_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = gp_object.scalerY.transform(sim_data.y_vals.reshape(-1,1))\n",
    "c_guess= sum(y.flatten()**2)/len(y)\n",
    "mat_kern = gpflow.kernels.Matern52(variance = c_guess, lengthscales=d_guess)\n",
    "noise_kern = gpflow.kernels.White(variance=float((noise_std/gp_object.scalerY.scale_)**2))\n",
    "kernel = mat_kern+noise_kern\n",
    "\n",
    "#Set scale parameter as Constant\n",
    "gp_model =gpflow.models.GPR((scl_feat_data, scl_y_data),\n",
    "        kernel=kernel, noise_variance = float((noise_std/gp_object.scalerY.scale_)**2)\n",
    "    )\n",
    "# gpflow.set_trainable(gp_model.likelihood.variance, False)\n",
    "if gp_object.outputscl is not None:\n",
    "    gpflow.set_trainable(mat_kern.variance, False)\n",
    "mat_kern.variance.prior = tfp.distributions.HalfCauchy(np.float64(1.0), np.float64(5.0))\n",
    "if gp_object.noise_std is not None:\n",
    "    gpflow.set_trainable(gp_model.kernel.kernels[1].variance, False)\n",
    "\n",
    "gpflow.utilities.print_summary(gp_model)\n",
    "#######################################################\n",
    "# import tensorflow_probability as tfp\n",
    "# y = gp_object.scalerY.transform(sim_data.y_vals.reshape(-1,1))\n",
    "# c_guess= sum(y.flatten()**2)/len(y)\n",
    "# mat_kern = gpflow.kernels.Matern52(variance = c_guess, lengthscales=max_distance)\n",
    "# per_kern = gpflow.kernels.Periodic(base_kernel = mat_kern, period = 1.0)\n",
    "# noise_kern = gpflow.kernels.White(variance = float((noise_std/gp_object.scalerY.scale_)**2))\n",
    "# kernel = per_kern+noise_kern\n",
    "\n",
    "# #Set scale parameter as Constant\n",
    "# gp_model =gpflow.models.GPR((scl_feat_data, scl_y_data),\n",
    "#         kernel=kernel, noise_variance = float((noise_std/gp_object.scalerY.scale_)**2)\n",
    "#     )\n",
    "# # gpflow.set_trainable(gp_model.likelihood.variance, False)\n",
    "# if gp_object.outputscl is not None:\n",
    "#     gpflow.set_trainable(mat_kern.variance, False)\n",
    "# if gp_object.noise_std is not None:\n",
    "#     gpflow.set_trainable(noise_kern.variance, False)\n",
    "\n",
    "# gpflow.utilities.print_summary(gp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train GP\n",
    "# optimizer = gpflow.optimizers.Scipy()\n",
    "# optimizer.minimize(gp_model.training_loss, gp_model.trainable_variables)\n",
    "# print(gp_model.trainable_parameters.numpy())\n",
    "# Number of retraining iterations\n",
    "print(gp_model.parameters[0].numpy())\n",
    "num_retrainings = 1\n",
    "\n",
    "# Train the model multiple times and keep track of the model with the lowest minimum training loss\n",
    "best_minimum_loss = float('inf')\n",
    "optimizer = gpflow.optimizers.Scipy()\n",
    "best_model = None\n",
    "\n",
    "for _ in range(num_retrainings):\n",
    "    # Define and train the GP model\n",
    "    optimizer.minimize(gp_model.training_loss, gp_model.trainable_variables)\n",
    "\n",
    "    # Compute the training loss of the model\n",
    "    training_loss = gp_model.training_loss().numpy()\n",
    "\n",
    "    # Check if this model has the best minimum training loss\n",
    "    if training_loss < best_minimum_loss:\n",
    "        best_minimum_loss = training_loss\n",
    "        best_model = gp_model\n",
    "\n",
    "gpflow.utilities.print_summary(gp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scld = gp_object.scalerX.transform(gp_object.feature_val_data)\n",
    "gp_mean, gp_var = gp_model.predict_f(X_scld)\n",
    "misc_gp_mean = gp_object.scalerY.inverse_transform(gp_mean.numpy()).flatten()\n",
    "misc_var_return = gp_var.numpy().flatten()*gp_object.scalerY.scale_**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_val_data.y_vals, all_val_data.y_vals, color='red', alpha=0.7)\n",
    "plt.scatter(all_val_data.y_vals, misc_gp_mean, color='blue', alpha=0.7)\n",
    "plt.errorbar(all_val_data.y_vals, misc_gp_mean, yerr = 1.96*np.sqrt(abs(misc_var_return)), alpha=0.3, fmt = 'o', color = \"blue\")\n",
    "plt.xlabel('True y Values')\n",
    "plt.ylabel('Predicted y Values')\n",
    "# plt.xlim([-10,10])\n",
    "plt.title('Parity Plot of CS' + str(cs_name_val))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Toy_Problem_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
