#Pseudo code for all 3-Input GP


Step 1) Generate Experimental and Training data (Completed)

NOTE: 
Number of dimensions of coordinate system: m = 1, 
Number of Experimental data points: n = 5, 
Number of input variables: q = 3
Number of training points generated by LHS: t=t

--Generate Experimental data and training input data--
# Set  x values (m x n), noise_mean (float), and noise_stdev (float)
# noise is drawn from a normal distribution, N(noise_mean, noise_stdev)
# Create experimental data using y = x -x^2 +x^3 + noise (1 x n)
# Generate training data inputs through LHS for Theta1,Theta2,and x (q x t)

--Generate training data outputs--
-- 3-Input GP
# Generate training data outputs with y = Theta1*x + Theta2*x^2 +x^3 (1 x t)


Step 2) Read Experimental and Training Data from the CSV (Completed)

# Read entire file with Pandas and convert to an array
# Create arrays for training/testing inputs and outputs based on data in stored files
# First column is an index, last column is output, all interior columns are inputs


Step 3) Train GP (Completed)

# Define likelihood and GP model clas
# Train GP based on input training data z, and output training data y
# Switch to evaluation mode


Step 4) Generate meshgrid for Theta

#Generate arrays for Theta1 and Theta2
Theta1 = np.linspace(-2,2,# points) (1 x p1)
Theta2 = np.linspace(-2,2,# points) (1 x p2)

#Create meshgrid for theta
theta_mesh = meshgrid (2 p1 x p2 arrays)

#Separate meshgrid into theta1 and theta2 arrays
theta1_mesh = theta_mesh[0] (p1 x p2)
theta2_mesh = theta_mesh[1] (p1 x p2)

Step 5) Calculate Best Error and EI

#Define f_bar and f(x)
f_bar = Yexp (1xn)
f(x) approximated by: GP mean evaluated at x - (1xn) (Note: This is an approximation)
#Will compare the rigorous solution and approximation later (multidimensional integral over each experiment using a sparse grid)

#Create an array in which to store expected improvement values
EI = np.zeros((p1,p2)) (p1 x p2)
Error = np.zeros((p1,p2))
# Loop over theta 1
for i in range(len(p1)):
    #Loop over theta2
    for j in range(len(p2)):
        #Create array to store error values
        error = np.zeros(n) (1xn)
        #Loop over Xexp
        for k in range(n):
            #Evaluate GP at a point p = [Theta1,Theta2,Xexp]
            point = [theta1_mesh[i,j],theta2_mesh[i,j],Xexp[k]]
            p = np.array([point]) (1 x q)
            GP_mean, GP_var = calc_GP_outputs(model, likelihood, p[0:1]) (float)
            f(x) = GP_mean (float)
            #Compute error for that point
            error_mag = -(f_bar - f(x))^2 (float)
            error[k] = error_mag
            Error += error_mag
        #Define best_error as the maximum value in the error array
        best_error = -max(error) (float)
        
        #Loop over Xexp
        for k in range(n):
            #Caclulate EI for each value n given the best error
            EI[i,j] += calc_ei(best_error, GP_mean, GP_Var, Y_exp[k])
return Error, EI

Step 6) Find GP Predicted Values of Theta
## Find point with lowest Error: Lowest error = Optimal value

#Caclulate argmin of Error_Point
argmin = np.array(np.where(np.isclose(Error, np.amax(Error),atol=1e-10)==True))

#Calculate correcsponding theta and x values and set p_Opt
Theta1_Opt = float(theta1_mesh[argmin[0],argmin[1]])
Theta2_Opt = float(theta2_mesh[argmin[0],argmin[1]])
Theta_GP_Opt = np.array([Theta1_Opt,Theta2_Opt],dtype=object)


Step 7) Complete BO loops

BO_iter = # of iterations
#Loop pver BO iterations
    # If training data are numpy arrays, convert to tensors
    if torch.is_tensor(train_p) != True: 
        train_p = torch.from_numpy(train_p) (q x t)
    if torch.is_tensor(train_y) != True:
        train_sse = torch.from_numpy(train_y) (1 x t)
    
    ## Repeat steps 3-6
    
    
    ##Find theta value with best EI
    #Calculate argmax of EI - highest EI = point to sample next
    argmax = np.array(np.where(np.isclose(EI_Point, np.amax(EI_Point),atol=1e-10)==True)) (1 x q)

    #Calculate correcsponding theta and x values and set p_Best
    Theta1_Best = float(theta1_mesh[argmax[1],argmax[2]]) (float)
    Theta2_Best = float(theta2_mesh[argmax[1],argmax[2]]) (float)
    
    ##Plot graphs
    
    ##Append best values to training data 
    #Convert training data to numpy arrays to allow concatenation
    train_p = train_p.numpy() (q x t)
    train_y = train_y.numpy() (1 x t)
    
    #Set noise and Theta True values (can do at beginning)
    noise_std = 0.1**2 (float)
    Theta_True = np.array([1,-1])
    #Loop over Xexp
    for i in range(n):
        ##Calculate y_Best and formal p_Best
        #Add 5 test points, same Theta1 and Theta2, but use all values of Xexp
        p_Best = np.array([Theta1_Best,Theta2_Best,Xexp[i]],dtype=object) (q x 1)
        y_Best = create_y_data(p_Best) (1 x 1)
        
        #Add Theta_Best to train_p and y_best to train_y
        train_p = np.concatenate((train_p, p_Best), axis=0) (q x t)
        train_y = np.concatenate((train_y, y_Best),axis=0) (1 x t)

QUESTIONS:


Answered Questions:
In the pseudo code you drew on the whiteboard, best error is called as best_error[k] in the EI equation. (Best error is 1xn)
1) What is f_bar and what is f(x) and are they both (1xn)? (f_bar = Yexp, f(x) approx. GP_mean)
2) When you add in a sample from the meshgrid do you retrain the GP for each set and apply it? (No)
3) Should there be a sum in the calculation of best error loop? (No we assume they act independently and summing them together in EI)
            
            
            
            
            
            
            
            
            
