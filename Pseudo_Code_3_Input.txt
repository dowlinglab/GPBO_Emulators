#Pseudo code for all 3-Input GP


Step 1) Generate Experimental and Training data (Completed)

NOTE: 
Number of dimensions of coordinate system: m = 1, 
Number of Experimental data points: n = 5, 
Number of input variables: q = 3
Number of training points generated by LHS: t=t

--Generate Experimental data and training input data--
# Set  x values (m x n), noise_mean (float), and noise_stdev (float)
# noise is drawn from a normal distribution, N(noise_mean, noise_stdev)
# Create experimental data using y = x -x^2 +x^3 + noise (1 x n)
# Generate training data inputs through LHS for Theta1,Theta2,and x (q x t)

--Generate training data outputs--
-- 3-Input GP
# Generate training data outputs with y = Theta1*x + Theta2*x^2 +x^3 (1 x t)


Step 2) Read Experimental and Training Data from the CSV (Completed)

# Read entire file with Pandas and convert to an array
# Create arrays for training/testing inputs and outputs based on data in stored files
# First column is an index, last column is output, all interior columns are inputs


Step 3) Train GP (Completed)

# Define likelihood and GP model clas
# Train GP based on input training data z, and output training data y
# Switch to evaluation mode


Step 4) Generate meshgrid for Theta

#Generate arrays for Theta1 and Theta2
Theta1 = np.linspace(-2,2,# points) (1 x p1)
Theta2 = np.linspace(-2,2,# points) (1 x p2)

#Create meshgrid for theta
theta_mesh = meshgrid (2 p1 x p2 arrays)

#Separate meshgrid into theta1 and theta2 arrays
theta1_mesh = theta_mesh[0] (p1 x p2)
theta2_mesh = theta_mesh[1] (p1 x p2)

Step 5) Calculate Best Error and EI

#Define f_bar and f(x)
f_bar = Yexp (1xn)
f(x) approximated by: GP mean evaluated at x - (1xn) (Note: This is an approximation)
#Will compare the rigorous solution and approximation later (multidimensional integral over each experiment using a sparse grid)

#Create an array in which to store expected improvement values
EI = np.zeros((p1,p2)) (p1 x p2)
Error = np.zeros((p1,p2))
# Loop over theta 1
for i in range(len(p1)):
    #Loop over theta2
    for j in range(len(p2)):
        #Create array to store error values
        error = np.zeros(n) (1xn)
        #Loop over Xexp
        for k in range(n):
            #Evaluate GP at a point p = [Theta1,Theta2,Xexp]
            p = np.array(theta1_mesh[i,j],theta2_mesh[i,j],Xexp[k]) (1 x q)
            GP_mean, GP_var = calc_GP_outputs(model, likelihood, p) (float)
            f(x) = GP_mean (float)
            #Compute error for that point
            error_mag = -(f_bar - f(x))^2 (float)
            error[k] = error_mag
            Error += error_mag
        #Define best_error as the maximum value in the error array
        best_error = -max(error) (float)
        
        #Loop over Xexp
        for k in range(n):
            #Caclulate EI for each value n given the best error
            EI[i,j] += calc_ei(best_error, GP_mean, GP_Var, Y_exp[k])
return Error and EI

Step 6) Find GP Predicted Values of Theta
## Find point with lowest Error

#Caclulate argmin of Error_Point
argmin = np.array(np.where(Error == np.amin(Error)))

#Calculate correcsponding theta and x values and set p_Opt
##DOES THIS STEP LOOK OK? I'M NOT REALLY SURE IF THIS IS RIGHT
Theta1_Opt = float(theta1_mesh[argmin[0],argmin[1]])
Theta2_Opt = float(theta2_mesh[argmin[0],argmin[1]])
Theta_GP_Opt = np.array([Theta1_Opt,Theta2_Opt],dtype=object)


Step 7) Complete BO loops

BO_iter = # of iterations
#Loop pver BO iterations
    ## Repeat steps 3-6  in a for loop

    #Initialize arrays to store EI and Error for each point
    EI_point = np.zeros((n,p1,p2)) (will be important for step 7)
    Error_Point = np.zeros((n,p1,p2))
    Points = []
    # Loop over theta 1
    for i in range(len(p1)):
        #Loop over theta2
        for j in range(len(p2)):
            #Create array to store error values
            error = np.zeros(n) (1xn)
            #Loop over Xexp
            for k in range(n):
                #Evaluate GP at a point p = [Theta1,Theta2,Xexp]
                p = np.array(theta1_mesh[i,j],theta2_mesh[i,j],Xexp[k]) (1 x q)
                Points = np.append(Points,p)
                GP_mean, GP_var = calc_GP_outputs(model, likelihood, p) (float)
                f(x) = GP_mean (float)
                #Compute error for that point
                error_mag = -(f_bar - f(x))^2 (float)
                error[k] = error_mag
                Error_Point[k,i,j] = error_mag
            #Define best_error as the maximum value in the error array
            best_error = -max(error) (float)

            #Loop over Xexp
            for k in range(n):
                #Caclulate EI for each value n given the best error
                EI_point[k,i,j] = calc_ei(best_error, GP_mean, GP_Var, Y_exp[k])
    return Points,EI_point, Error_Point

    ##Find point with best EI
    #Calculate argmax of EI
    argmax = np.array(np.where(EI_Point == np.amax(EI_Point))) (1 x q)

    #Calculate correcsponding theta and x values and set p_Best
    ##DOES THIS STEP LOOK OK? I'M NOT REALLY SURE IF THIS IS RIGHT
    Theta1_Best = float(theta1_mesh[argmin[1],argmin[2]]) (float)
    Theta2_Best = float(theta2_mesh[argmin[1],argmin[2]]) (float)
    x_Best = Xexp[argmin[0]] (float)
    p_Best = np.array([Theta1_Best,Theta2_Best,x_Best],dtype=object)

    ##Append best values to training data


QUESTIONS:


Answered Questions:
In the pseudo code you drew on the whiteboard, best error is called as best_error[k] in the EI equation. (Best error is 1xn)
1) What is f_bar and what is f(x) and are they both (1xn)? (f_bar = Yexp, f(x) approx. GP_mean)
2) When you add in a sample from the meshgrid do you retrain the GP for each set and apply it? (No)
3) Should there be a sum in the calculation of best error loop? (No we assume they act independently and summing them together in EI)
            
            
            
            
            
            
            
            
            
