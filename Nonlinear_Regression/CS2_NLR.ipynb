{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2539354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import scipy.optimize as optimize\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import itertools\n",
    "from itertools import combinations_with_replacement\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 200\n",
    "\n",
    "log_plot = True\n",
    "\n",
    "import bo_methods_lib\n",
    "\n",
    "#Note: Need to normalize all values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664c72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_sampling(num_points, bounds):\n",
    "        \"\"\"\n",
    "        Generates Grid sampled data\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        num_points: int, number of points in LHS, should be greater than # of dimensions\n",
    "        bounds: ndarray, array containing upper and lower bounds of elements in LHS sample. Defaults of 0 and 1\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        grid_data: ndarray, (num_points)**bounds.shape[1] grid sample of data\n",
    "        \n",
    "        \"\"\"\n",
    "        #Generate mesh_grid data for theta_set in 2D\n",
    "        #Define linspace for theta\n",
    "        params = np.linspace(0,1, num_points)\n",
    "        #Define dimensions of parameter\n",
    "        dimensions = bounds.shape[1]\n",
    "        #Generate the equivalent of all meshgrid points\n",
    "        df = pd.DataFrame(list(itertools.product(params, repeat=dimensions)))\n",
    "        df2 = df.drop_duplicates()\n",
    "        scaled_data = df2.to_numpy()\n",
    "        #Normalize to bounds \n",
    "        lower_bound = bounds[0]\n",
    "        upper_bound = bounds[1]\n",
    "        grid_data = scaled_data*(upper_bound - lower_bound) + lower_bound \n",
    "        return grid_data\n",
    "    \n",
    "def calc_muller(model_coefficients, x):\n",
    "    \"\"\"\n",
    "    Caclulates the Muller Potential\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        model_coefficients: ndarray, The array containing the values of Muller constants\n",
    "        x: ndarray, Values of X\n",
    "        noise: ndarray, Any noise associated with the model calculation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        y_mul: float, value of Muller potential\n",
    "    \"\"\"\n",
    "    #Reshape x to matrix form\n",
    "    #If array is not 2D, give it shape (len(array), 1)\n",
    "    if not len(x.shape) > 1:\n",
    "        x = x.reshape(-1,1)\n",
    "        \n",
    "    assert x.shape[0] == 2, \"Muller Potential x_data must be 2 dimensional\"\n",
    "    X1, X2 = x #Split x into 2 parts by splitting the rows\n",
    "    \n",
    "    #Separate all model parameters into their appropriate pieces\n",
    "    model_coefficients_reshape = model_coefficients.reshape(6, 4)\n",
    "        \n",
    "    #Calculate Muller Potential\n",
    "    A, a, b, c, x0, y0 = model_coefficients_reshape\n",
    "    term1 = a*(X1 - x0)**2\n",
    "    term2 = b*(X1 - x0)*(X2 - y0)\n",
    "    term3 = c*(X2 - y0)**2\n",
    "    y_mul = np.sum(A*np.exp(term1 + term2 + term3) )\n",
    "    \n",
    "    return y_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec834f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "CS = 2.2\n",
    "skip_params = 0\n",
    "\n",
    "#Change this to get less parameters\n",
    "# Create synthetic data assuming the following values for theta\n",
    "# a_guess = np.array([-150,-75,-150,10, #A guess\n",
    "#                     0.5,0.5,-7,1, #a guess\n",
    "#                     -1, 1, 10, 1,#b guess\n",
    "#                     -8, -8, -8, 0,#c guess\n",
    "#                     0.5, 0.5, 0, 0,#x0 guess\n",
    "#                     0.5, 0.5, 0.5, 0.5]) #y0 guess\n",
    "\n",
    "# a_guess = np.array([-150,-75,-150,10, #A guess\n",
    "#                     0.5,0.5,-7,1, #a guess\n",
    "#                     -1, 1, 10, 1,#b guess\n",
    "#                     -8, -8, -8, 0,#c guess\n",
    "#                     0.5, 0.5, 0, 0]) #x0 guess\n",
    "\n",
    "a_guess = np.array([-100, -90, -100, 5]) #A guess\n",
    "\n",
    "# a_guess = np.array([-1, 1, 10, 1]) #b guess\n",
    "\n",
    "Constants = np.array([[-200,-100,-170,15],\n",
    "                      [-1,-1,-6.5,0.7],\n",
    "                      [0,0,11,0.6],\n",
    "                      [-10,-10,-6.5,0.7],\n",
    "                      [1,0,-0.5,-1],\n",
    "                      [0,0.5,1.5,1]])\n",
    "\n",
    "num_param_guess = int(len(a_guess)/Constants.shape[1])\n",
    "##New Cell\n",
    "# Evaluate model and add noise based on assumed theta values\n",
    "# This generates experimental data points\n",
    "num_points = 5\n",
    "bounds_x = np.array([[-1.5, -0.5], \n",
    "                     [1, 2]])\n",
    "Xexp = grid_sampling(num_points, bounds_x)\n",
    "Yexp = [calc_muller(Constants, Xexp[i]) for i in range(len(Xexp)) ]\n",
    "print(Xexp.shape)\n",
    "print(a_guess[0])\n",
    "print(Constants.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cfd1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##New Cell\n",
    "\n",
    "# Evaluate model based on the assumed experimental values\n",
    "#Create Meshgrid for X1 and X2 and evaluate Y\n",
    "len_mesh_data = 20\n",
    "x1 = np.linspace(np.min(Xexp[:,0]),np.max(Xexp[:,0]),len_mesh_data)\n",
    "x2 = np.linspace(np.min(Xexp[:,1]),np.max(Xexp[:,1]),len_mesh_data)\n",
    "X1, X2 = np.meshgrid(x1,x2)\n",
    "X_mesh = np.meshgrid(x1,x2)\n",
    "# #Creates an array for Y that will be filled with the for loop\n",
    "# #Initialize y_sim\n",
    "Y = [] #len_data \n",
    "\n",
    "# #Find evey combination of X1/X2 to find the SSE for each combination\n",
    "#Set constants\n",
    "A, a, b, c, x0, y0 = Constants\n",
    "\n",
    "#Calculate y_sim\n",
    "#Define X1 and X2 (Need a better way do do this without for loops)\n",
    "#Loop over combinations of X1 X2\n",
    "for i in range(len_mesh_data):\n",
    "    for j in range(len_mesh_data):\n",
    "        Term1 = a*(X1[i,j] - x0)**2\n",
    "        Term2 = b*(X1[i,j] - x0)*(X2[i,j] - y0)\n",
    "        Term3 = c*(X2[i,j] - y0)**2\n",
    "        Y.append( np.sum( A*np.exp(Term1 + Term2 + Term3) ) )\n",
    "        \n",
    "#Reshape to correct dimension (Is there an easier wat to do this?)        \n",
    "Y = np.array(Y).reshape(len_mesh_data,-1)\n",
    "# print(Y)\n",
    "# Compare the experiments to the true model\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X1, X2, Y, 100, cmap='Reds')\n",
    "ax.plot(1000,1000,1000, label = \"$y_{exp}$\", color = 'red')\n",
    "ax.scatter(1000,1000,1000, label = \"Exp Data\", color = 'green', edgecolors = \"k\")\n",
    "ax.scatter3D(Xexp[:,0], Xexp[:,1], Yexp, c=Yexp, cmap='Greens', edgecolors = \"k\")\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.0, 1, 0.2),borderaxespad=0, loc = \"lower right\")\n",
    "\n",
    "ax.minorticks_on() # turn on minor ticks\n",
    "ax.tick_params(direction=\"in\",top=True, right=True) \n",
    "ax.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "\n",
    "\n",
    "ax.zaxis.set_tick_params(labelsize=12)\n",
    "ax.yaxis.set_tick_params(labelsize=12)\n",
    "ax.xaxis.set_tick_params(labelsize=12)\n",
    "ax.grid(False)\n",
    "\n",
    "ax.set_xlim((np.amin(X1),np.amax(X1)))\n",
    "ax.set_ylim((np.amin(X2),np.amax(X2)))\n",
    "\n",
    "ax.set_xlabel('X1', fontsize=16,fontweight='bold')\n",
    "ax.set_ylabel('X2', fontsize=16,fontweight='bold')\n",
    "ax.set_zlabel('Muller Potential',fontsize=16,fontweight='bold');\n",
    "\n",
    "ax.locator_params(axis='y', nbins=5)\n",
    "ax.locator_params(axis='x', nbins=5)\n",
    "# ax.locator_params(axis='z', nbins=5)\n",
    "# plt.title(\"Plotting True Model and Synthetic Data\")\n",
    "plt.show()\n",
    "\n",
    "# Y_scaled = np.array(Y_scaled).reshape(len_mesh_data,-1)\n",
    "# Y_bounds = np.array([np.min(Y_scaled), np.max(Y_scaled)])\n",
    "# Y = values_scaled_to_real(Y_scaled, Y_bounds)\n",
    "# Y = Y.reshape(len_mesh_data,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68370dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##New Cell\n",
    "\n",
    "## define function that includes nonlinear model\n",
    "def model(a_guess, Constants, x, skip_params = 0):\n",
    "    '''\n",
    "        \"\"\"\n",
    "    Creates Muller potential values given a guess for \"a\"\n",
    "    Parameters\n",
    "    ----------\n",
    "        a_guess: ndarray, guess value for a\n",
    "        Constants: ndarray, The array containing the true values of Muller constants\n",
    "        x: ndarray, Independent variable data (exp or pred)\n",
    "    Returns\n",
    "    -------\n",
    "        y_model: ndarray, The simulated Muller potential given the guess\n",
    "    '''\n",
    "    #Assert statements check that the types defined in the doctring are satisfied\n",
    "    \n",
    "    #Converts parameters to numpy arrays if they are tensors\n",
    "    if torch.is_tensor(a_guess)==True:\n",
    "        a_guess = a_guess.numpy()\n",
    "        \n",
    "    if isinstance(a_guess, pd.DataFrame):\n",
    "        a_guess = a_guess.to_numpy()\n",
    "    \n",
    "    #Initialize y_sim, set len_data and dim_x\n",
    "    len_x_shape = len(x.shape) #Will tell us whether we're looking at Xexp or Xmesh\n",
    "        \n",
    "    if len_x_shape < 3:\n",
    "        len_x_data = x.shape[0]\n",
    "        y_model = np.zeros(len_x_data)\n",
    "        \n",
    "    else:\n",
    "        len_x_data = x.shape[1]\n",
    "        y_model = np.zeros((len_x_data, len_x_data))\n",
    "    \n",
    "    #Set dig out values of a from train_p\n",
    "    #Set constants to change the a row to the index of the first loop\n",
    "    Constants_local = np.copy(Constants)\n",
    "    dim_guess = int(len(a_guess)/Constants.shape[1])\n",
    "    a_guess = a_guess.reshape(dim_guess,-1)\n",
    "    \n",
    "    for i in range(dim_guess):\n",
    "        Constants_local[i+skip_params] = a_guess[i]#Since we've chosen A, a, b, c, x0, and y0\n",
    "        \n",
    "#     print(Constants_local)\n",
    "#     print(Constants_local)\n",
    "    A, a, b, c, x0, y0 = Constants_local\n",
    "#     print(a,A)\n",
    "\n",
    "    #Iterates over Xexp to find the y for each combination\n",
    "    for i in range(len_x_data):\n",
    "        #Calculate y_sim\n",
    "        if len_x_shape < 3:\n",
    "            X1, X2 = x[i,0], x[i,1]\n",
    "            Term1 = a*(X1 - x0)**2\n",
    "            Term2 = b*(X1 - x0)*(X2 - y0)\n",
    "            Term3 = c*(X2 - y0)**2\n",
    "            y_model[i] = np.sum(A*np.exp(Term1 + Term2 + Term3) )\n",
    "        else:\n",
    "        #loop over all i and j\n",
    "            X1, X2 = x\n",
    "            for i in range(len_x_data):\n",
    "                for j in range(len_x_data):\n",
    "                    Term1 = a*(X1[i,j] - x0)**2\n",
    "                    Term2 = b*(X1[i,j] - x0)*(X2[i,j] - y0)\n",
    "                    Term3 = c*(X2[i,j] - y0)**2\n",
    "                    y_model[i,j] = ( np.sum( A*np.exp(Term1 + Term2 + Term3) ) )\n",
    "   \n",
    "    if not len_x_shape < 3:\n",
    "        y_model = y_model.reshape(len_x_data, -1)\n",
    "    \n",
    "    return y_model\n",
    "\n",
    "# print(model(a_guess,Constants,Xexp, skip_params))\n",
    "\n",
    "##New Cell\n",
    "\n",
    "# Create a function to optimize, in this case, least squares fitting\n",
    "def regression_func(a_guess, Constants, x, y, skip_params = 0):\n",
    "    '''\n",
    "    Function to define regression function for least-squares fitting\n",
    "    Arguments:\n",
    "        a_guess: ndarray, guess value for a\n",
    "        Constants: ndarray, The array containing the true values of Muller constants\n",
    "        x: ndarray, experimental X data (Inependent Variable)\n",
    "        y: ndarray, experimental Y data (Dependent Variable)\n",
    "    Returns:\n",
    "        e: residual vector\n",
    "    '''\n",
    "    \n",
    "    error = y - model(a_guess, Constants, x, skip_params); #NOTE: Least squares will calculate sse based off this to minimize\n",
    "    \n",
    "    return error\n",
    "\n",
    "# print(regression_func(a_guess, Constants, Xexp, Yexp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e88830",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## specify initial guess\n",
    "a0 = a_guess\n",
    "#Define # of dimensions\n",
    "try:\n",
    "    d = a_guess.shape[0]*a_guess.shape[1]\n",
    "except:\n",
    "    d = a_guess.shape[0]\n",
    "## specify bounds\n",
    "# first array: lower bounds\n",
    "# second array: upper bounds\n",
    "lower = np.repeat(-np.inf, d)\n",
    "upper = np.repeat(np.inf, d)\n",
    "bounds = (lower, upper) \n",
    "\n",
    "## use least squares optimizer in scipy\n",
    "# argument 1: function that takes theta as input, returns residual\n",
    "# argument 2: initial guess for theta\n",
    "# optional arguments 'bounds': bounds for theta\n",
    "# optional arugment 'args': additional arguments to pass to residual function\n",
    "# optional argument 'method': select the numerical method\n",
    "#   if you want to consider bounds, choose 'trf'\n",
    "#   if you do not want to consider bounds, try either 'lm' or 'trf'\n",
    "Solution = optimize.least_squares(regression_func, a0 ,bounds=bounds, method='trf',args=(Constants, Xexp, Yexp, skip_params),verbose=2)\n",
    "\n",
    "a_model = Solution.x\n",
    "a_model_soln = a_model.reshape(num_param_guess,-1)\n",
    "print(\"a = \",a_model_soln)\n",
    "print(\"Constants\", Constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c596480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1, a2, a3, a4 = np.meshgrid(a1_lin, a2_lin, a3_lin, a4_lin)\n",
    "# a_guesses = np.meshgrid(a1_lin, a2_lin, a3_lin, a4_lin)\n",
    "\n",
    "# P_inds = np.array([0,3])\n",
    "# print(sse_func(a_model, Xexp, Yexp, P_inds, a1_lin, a2_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cabbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Cell\n",
    "X_pred = np.array(np.meshgrid(x1,x2))\n",
    "Y_pred = model(a_model, Constants, X_pred, skip_params)\n",
    "\n",
    "\n",
    "# create plot and compare predictions and experiments\n",
    "fig = plt.figure(figsize = (6.4,4))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X1, X2, Y_pred, 100, cmap='Blues')\n",
    "ax.contour3D(X1, X2, Y, 100, cmap='Reds')\n",
    "ax.scatter3D(Xexp[:,0], Xexp[:,1], Yexp, c=Yexp, cmap='Greens', edgecolors = \"k\")\n",
    "ax.plot(1000,1000,1000, label = \"$y_{sim}$\", color = 'blue')\n",
    "ax.plot(1000,1000,1000, label = \"$y_{exp}$\", color = 'red')\n",
    "ax.scatter(1000,1000,1000, label = \"Exp Data\", color = 'green', edgecolors = \"k\")\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.0, 1, 0.2),borderaxespad=0, loc = \"lower right\")\n",
    "\n",
    "ax.minorticks_on() # turn on minor ticks\n",
    "ax.tick_params(direction=\"in\",top=True, right=True) \n",
    "ax.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "\n",
    "ax.locator_params(axis='y', nbins=5)\n",
    "ax.locator_params(axis='x', nbins=5)\n",
    "ax.locator_params(axis='z', nbins=5)\n",
    "\n",
    "ax.zaxis.set_tick_params(labelsize=12)\n",
    "ax.yaxis.set_tick_params(labelsize=12)\n",
    "ax.xaxis.set_tick_params(labelsize=12)\n",
    "ax.grid(False)\n",
    "\n",
    "ax.set_xlim((np.amin(X1),np.amax(X1)))\n",
    "ax.set_zlim(np.amin(Y),np.amax(Y))\n",
    "ax.set_xlim(np.amin(X1),np.amax(X1))\n",
    "ax.set_ylim(np.amin(X2),np.amax(X2))\n",
    "ax.set_xlabel('X1', fontsize=16,fontweight='bold')\n",
    "ax.set_ylabel('X2', fontsize=16,fontweight='bold')\n",
    "ax.set_zlabel('Muller Potential', fontsize=16,fontweight='bold')\n",
    "ax.zaxis._axinfo['label']['space_factor'] = 2\n",
    "# plt.title(\"Predictions vs Synthetic Data\")\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig(\"Figures/sim_true_comp.png\",dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4ce34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot error\n",
    "Y_pred_of_exp = model(a_model, Constants, Xexp, skip_params)\n",
    "error = (Yexp - Y_pred_of_exp)\n",
    "print(\"SSE = \", np.sum(error**2))\n",
    "plt.plot(Y_pred_of_exp,error,\"b.\",markersize=20, label = \"Error\")\n",
    "\n",
    "plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0, loc = \"lower right\")\n",
    "\n",
    "plt.minorticks_on() # turn on minor ticks\n",
    "plt.tick_params(direction=\"in\",top=True, right=True) \n",
    "plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "\n",
    "plt.locator_params(axis='y', nbins=5)\n",
    "plt.locator_params(axis='x', nbins=5)\n",
    "\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.grid(False)\n",
    "\n",
    "# plt.title(\"Residuals\")\n",
    "plt.xlabel('Predicted Potential', fontsize=16,fontweight='bold')\n",
    "plt.ylabel('Residuals', fontsize=16,fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4991e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sse_func(a_model, x, y, P_inds, P1_vals, P2_vals, skip_params):\n",
    "def sse_func(a_model, x, y, meshgrid, P_inds, skip_params):\n",
    "    '''\n",
    "    Function to define define sum of squared error function for heat map\n",
    "    Arguments:\n",
    "        xx: An N X D array of all a_1 values\n",
    "        yy: An D X N array of all a_2 values\n",
    "        x: independent variable vector (predicted x values including noise)\n",
    "        y: dependent variable vector (predicted y values on Heat Map)\n",
    "    Returns:\n",
    "        sse: N x N sum of squared error matrix of all generated combination of xx and yy\n",
    "    '''\n",
    "#     #Meshgrid\n",
    "#     P1_mesh, P2_mesh = np.meshgrid(P1_vals,P2_vals)\n",
    "    xx, yy = meshgrid\n",
    "    #Copy Center Point\n",
    "    a_guess_local = np.copy(a_model)\n",
    "    #Initialize SSE Maxtrix\n",
    "    sse = np.zeros((xx.shape))\n",
    "#     print(P_inds)\n",
    "#     print(a_guess_local)\n",
    "    #Calculate SSE\n",
    "    for i in range(len(xx)):\n",
    "        for j in range(len(yy)):\n",
    "#             a_guess_local[P_inds[0]+ 4*skip_params] = xx[i,j]\n",
    "#             a_guess_local[P_inds[1]+ 4*skip_params] = yy[i,j]\n",
    "            a_guess_local[P_inds[0] - 4*skip_params] = xx[i,j]\n",
    "            a_guess_local[P_inds[1] - 4*skip_params] = yy[i,j]\n",
    "#             print(a_guess_local)\n",
    "            sse[i,j] = sum((y - model(a_guess_local, Constants, x, skip_params))**2) \n",
    "    \n",
    "    return sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8653a5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#New Cell\n",
    "# log_plot = False\n",
    "log_plot = True\n",
    "# save_figure = True\n",
    "save_figure = False\n",
    "\n",
    "# generate predictions\n",
    "X_pred = np.array(X_mesh)\n",
    "Y_pred = model(a_guess, Constants, X_pred, skip_params)\n",
    "Constants_2 = Constants.flatten()\n",
    "\n",
    "#Modify these to have less parameters\n",
    "#Generate Guesses for a1-a4\n",
    "a1_lin = a2_lin = np.linspace(-2,0,10)\n",
    "a3_lin = np.linspace(-10,2,10)\n",
    "a4_lin = b1_lin = b2_lin = b4_lin = c4_lin = np.linspace(-2,2,10)\n",
    "A1_lin = A2_lin = A3_lin = np.linspace(-250,50,10)\n",
    "A4_lin = np.linspace(10,20,10)\n",
    "b3_lin = np.linspace(8,12,10)\n",
    "b4_lin = np.linspace(-2,2,10)\n",
    "c1_lin = np.linspace(-12,-8,10)\n",
    "c2_lin = np.linspace(-12,-8,10)\n",
    "c3_lin = np.linspace(-10,-6,10)\n",
    "\n",
    "x0_1_lin = x0_2_lin = x0_3_lin = x0_4_lin = np.linspace(-2,2,10)\n",
    "y0_1_lin = y0_2_lin = y0_3_lin = y0_4_lin = np.linspace(-2,2,10)\n",
    "\n",
    "# param_dict = {0: \"A1\", 1: \"A2\", 2: \"A3\", 3: \"A4\",\n",
    "#               4: \"a1\", 5: \"a2\", 6: \"a3\", 7: \"a4\",\n",
    "#               8: \"b1\", 9: \"b2\", 10:\"b3\", 11:\"b4\",\n",
    "#               12:\"c1\", 13:\"c2\", 14:\"c3\", 15:\"c4\",\n",
    "#               16:\"x0_1\", 17:\"x0_2\", 18:\"x0_3\", 19:\"x0_4\",\n",
    "#               20:\"y0_1\", 21:\"y0_2\", 22:\"y0_3\", 23:\"y0_4\"}\n",
    "\n",
    "# param_dict_vals = {0: A1_lin, 1: A2_lin, 2: A3_lin, 3: A4_lin,\n",
    "#               4: a1_lin, 5: a2_lin, 6: a3_lin, 7: a4_lin,\n",
    "#               8: b1_lin, 9: b2_lin, 10:b3_lin, 11:b4_lin,\n",
    "#               12:c1_lin, 13:c2_lin, 14:c3_lin, 15:c4_lin,\n",
    "#               16:x0_1_lin, 17:x0_2_lin, 18:x0_3_lin, 19:x0_4_lin,\n",
    "#               20:y0_1_lin, 21:y0_2_lin, 22:y0_3_lin, 23:y0_4_lin}\n",
    "\n",
    "# param_dict = {0: \"A1\", 1: \"A2\", 2: \"A3\", 3: \"A4\",\n",
    "#               4: \"a1\", 5: \"a2\", 6: \"a3\", 7: \"a4\",\n",
    "#               8: \"b1\", 9: \"b2\", 10:\"b3\", 11:\"b4\",\n",
    "#               12:\"c1\", 13:\"c2\", 14:\"c3\", 15:\"c4\",\n",
    "#               16:\"x0_1\", 17:\"x0_2\", 18:\"x0_3\", 19:\"x0_4\"}\n",
    "\n",
    "# param_dict_vals = {0: A1_lin, 1: A2_lin, 2: A3_lin, 3: A4_lin,\n",
    "#               4: a1_lin, 5: a2_lin, 6: a3_lin, 7: a4_lin,\n",
    "#               8: b1_lin, 9: b2_lin, 10:b3_lin, 11:b4_lin,\n",
    "#               12:c1_lin, 13:c2_lin, 14:c3_lin, 15:c4_lin,\n",
    "#               16:x0_1_lin, 17:x0_2_lin, 18:x0_3_lin, 19:x0_4_lin}\n",
    "\n",
    "param_dict = {0: \"A1\", 1: \"A2\", 2: \"A3\", 3: \"A4\",\n",
    "              4: \"a1\", 5: \"a2\", 6: \"a3\", 7: \"a4\",\n",
    "              8: \"b1\", 9: \"b2\", 10:\"b3\", 11:\"b4\",\n",
    "              12:\"c1\", 13:\"c2\", 14:\"c3\", 15:\"c4\",\n",
    "              16:\"x0_1\", 17:\"x0_2\", 18:\"x0_3\", 19:\"x0_4\"}\n",
    "\n",
    "param_dict_vals = {0: A1_lin, 1: A2_lin, 2: A3_lin, 3: A4_lin,\n",
    "              4: a1_lin, 5: a2_lin, 6: a3_lin, 7: a4_lin,\n",
    "              8: b1_lin, 9: b2_lin, 10:b3_lin, 11:b4_lin,\n",
    "              12:c1_lin, 13:c2_lin, 14:c3_lin, 15:c4_lin,\n",
    "              16:x0_1_lin, 17:x0_2_lin, 18:x0_3_lin, 19:x0_4_lin}\n",
    "\n",
    "skip_params = 1\n",
    "dim_list = np.linspace(0,d-1,d)\n",
    "mesh_combos = np.array(list(combinations(dim_list, 2)), dtype = int)\n",
    "# print(mesh_combos)\n",
    "#Loop over combinations of axes and create heatmaps\n",
    "for i in range(len(mesh_combos)):\n",
    "    indecies = mesh_combos[i] +4*skip_params\n",
    "#     print( [ param_dict[indecies[0]], param_dict[indecies[1]] ])\n",
    "    param_vals_list = [ param_dict_vals[indecies[0]], param_dict_vals[indecies[1]] ]\n",
    "    P1_vals, P2_vals = param_vals_list[0], param_vals_list[1]\n",
    "    theta_mesh = np.array(np.meshgrid(P1_vals, P2_vals))\n",
    "#     print(theta_mesh)\n",
    "    zz = sse_func(a_model, Xexp, Yexp, theta_mesh, indecies, skip_params)\n",
    "#     print(zz)\n",
    "\n",
    "    if log_plot == True:\n",
    "        zz = np.log(zz)\n",
    "    #Better way to do this?\n",
    "    plt.figure(figsize = (6,6))\n",
    "\n",
    "    plt.contourf(P1_vals,P2_vals, zz, cmap = \"autumn\", levels = 100)\n",
    "#             plt.colorbar()\n",
    "\n",
    "    cs = plt.contourf(P1_vals,P2_vals, zz, cmap = \"autumn\", levels = 100)\n",
    "            # plot color bar\n",
    "    if np.amax(zz) < 1e-1:\n",
    "        cbar = plt.colorbar(cs, format='%.2e')\n",
    "    else:\n",
    "        cbar = plt.colorbar(cs, format = '%2.2f')\n",
    "\n",
    "        # plot title in color bar\n",
    "        cbar.ax.set_ylabel(r'$\\mathbf{log(e(\\theta))}$', fontsize=16, fontweight='bold')\n",
    "    #     print(p_GP_opt[0],p_GP_opt[1])\n",
    "\n",
    "        # set font size in color bar\n",
    "        cbar.ax.tick_params(labelsize=16)\n",
    "\n",
    "        # Plot equipotential line\n",
    "        cs2 = plt.contour(cs, levels=cs.levels[::20], colors='k', alpha=0.7, linestyles='dashed', linewidths=3)\n",
    "\n",
    "        #Plot heatmap label\n",
    "\n",
    "        if np.amax(zz) < 1e-1:\n",
    "            plt.clabel(cs2, fmt='%.2e', colors='k', fontsize=16)\n",
    "        else:\n",
    "            plt.clabel(cs2, fmt='%2.2f', colors='k', fontsize=16)\n",
    "\n",
    "        plt.axis()\n",
    "        \n",
    "#         print(Constants_2[indecies[0]+4*skip_params])\n",
    "        plt.scatter(Constants_2[indecies[0]],Constants_2[indecies[1]], color=\"blue\", s=100, \n",
    "                    label = \"True Optimal Value\", marker = (5,1)) #k +1 since we chose a&b\n",
    "        plt.scatter(a_model[indecies[0]- 4*skip_params],a_model[indecies[1]- 4*skip_params], color=\"white\",s=50, \n",
    "                    marker = \".\", edgecolors= \"k\", linewidth=0.3, label = \"NLR Optimal Value\")\n",
    "#         print(a_model[indecies[0]+4*skip_params], Constants_2[indecies[0]+4*skip_params])\n",
    "#         print(a_model[indecies[1]+4*skip_params], Constants_2[indecies[1]+4*skip_params])\n",
    "        # plt.grid()\n",
    "        plt.legend(fontsize=10,bbox_to_anchor=(0, 1.05, 1, 0.2),borderaxespad=0, loc = \"lower right\")\n",
    "        plt.xlabel(param_dict[indecies[0]],fontsize=16,fontweight='bold')\n",
    "        plt.ylabel(param_dict[indecies[1]],fontsize=16,fontweight='bold')\n",
    "\n",
    "        plt.xlim((np.amin(P1_vals), np.amax(P1_vals)))\n",
    "        plt.ylim((np.amin(P2_vals), np.amax(P2_vals)))\n",
    "\n",
    "        plt.minorticks_on() # turn on minor ticks\n",
    "        plt.xticks(fontsize=16)\n",
    "        plt.yticks(fontsize=16)\n",
    "        plt.tick_params(which=\"minor\",direction=\"in\",top=True, right=True)\n",
    "        plt.locator_params(axis='y', nbins=5)\n",
    "        plt.locator_params(axis='x', nbins=5)\n",
    "\n",
    "        if save_figure == True:\n",
    "#             plt.title('NLR ln(SSE)', weight='bold',fontsize = 16)\n",
    "            path = \"2023/NLR/CS_\" + str(CS) +\"/log_sse/\" + param_dict[indecies[0]] + \"-\" + param_dict[indecies[1]]\n",
    "            save_fig(path, ext='png', close=True, verbose=False)\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c44cfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jacobian and Uncertainty Analysis\n",
    "print(\"Jacobian =\\n\")\n",
    "Jacobian = Solution.jac\n",
    "print(Jacobian) #Jacobian is fine for simple cases\n",
    "#Normalize Jacobian\n",
    "# print(\"Normalized Jacobian =\\n\")\n",
    "# for i in range(len(a_model)):\n",
    "#     Jacobian[i,:] = Jacobian[i,:]*a_model\n",
    "# print(Jacobian)\n",
    "\n",
    "#OR normalize error instead of FIM to solve the problem (2 ways to do this)\n",
    "# error_normalized = (error - np.average(error)) / np.average(error)\n",
    "# error_normalized = (error - np.amin(error)) / (np.amax(error)- np.amin(error))\n",
    "# print(\"Normalized Error =  \\n\", error_normalized)\n",
    "\n",
    "sigre = (error.T @ error)/(len(error) - 2)\n",
    "# sigre = (error_normalized.T @ error_normalized)/(len(error_normalized) - 2)\n",
    "# Sigma_theta2 = sigre * np.linalg.inv(Solution.jac.T @ Solution.jac)\n",
    "Sigma_theta2 = sigre * np.linalg.inv(Jacobian.T @ Jacobian)\n",
    "print(\"Covariance matrix:\\n\",Sigma_theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bcac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as linalg\n",
    "val, vec = linalg.eig(Sigma_theta2)\n",
    "print(\"Eigenvalues = \\n\",val)\n",
    "print(\"Eigenvectors = \\n\",vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b49dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIM = (1/sigre) * Jacobian.T @ Jacobian\n",
    "print(\"FIM = \\n\", FIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a5af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linalg.det(FIM))\n",
    "print(np.linalg.det(Sigma_theta2))\n",
    "print(sigre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa3f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = Jacobian.T @ Jacobian\n",
    "print(np.linalg.det(Test))\n",
    "print(Test)\n",
    "val, vec = linalg.eig(Test)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01baa6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a142a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
