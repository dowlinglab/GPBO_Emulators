{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa1bbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt\n",
    "from bo_functions import calc_best_error\n",
    "from bo_functions import calc_ei_basic\n",
    "from bo_functions import y_plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36cb54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data is 10^2 points in [-2,2] inclusive regularly spaced\n",
    "Theta1 = np.linspace(-2,2,10) #1x10\n",
    "Theta2 = np.linspace(-2,2,10) #1x10\n",
    "\n",
    "#Creates a mesh for training data\n",
    "train_mesh = np.array(np.meshgrid(Theta1, Theta2)) #2 Uniform Arrays 10x10 (.T turns this into 10 10x2 arrays)\n",
    "x = torch.tensor(np.linspace(-2,2,5)) #1x5\n",
    "\n",
    "#Lists every combination of training Theta1 and Theta2\n",
    "train_T = torch.tensor(train_mesh.T.reshape(-1, 2))#100x2\n",
    "\n",
    "#Set noise parameters and true value of Theta to generate training data\n",
    "noise_mean = 0\n",
    "noise_std = 0.1**2\n",
    "Theta_True = torch.tensor([1,-1]) #1x2\n",
    "\n",
    "#Creates noise values with a certain stdev and mean from a normal distribution\n",
    "noise = torch.tensor(np.random.normal(size=len(x),loc = noise_mean, scale = noise_std)) #1x5\n",
    "\n",
    "# True function is y=T1*x + T2*x^2 + x^3 with Gaussian noise\n",
    "y_true =  Theta_True[0]*x + Theta_True[1]*x**2 +x**3 + noise #1x5\n",
    "\n",
    "#Creates an array for train_y that will be filled with the for loop\n",
    "train_y = torch.tensor(np.zeros(len(train_T))) #1 x 100\n",
    "\n",
    "#Iterates over evey combination of theta to find the SSE for each combination\n",
    "for i in range(len(train_T)):\n",
    "    theta_1 = train_T[i,0] #100x1 \n",
    "    theta_2 = train_T[i,1] #100x1\n",
    "    y_exp = theta_1*x + theta_2*x**2 +x**3 + noise #100 x5\n",
    "    train_y[i] = sum((y_true - y_exp)**2) # A number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ff1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "#This class is the Exact GP model and we are defining the class as ExactGPModel\n",
    "class ExactGPModel(gpytorch.models.ExactGP): #Exact GP does not add noise\n",
    "    \"\"\"\n",
    "    The base class for any Gaussian process latent function to be used in conjunction\n",
    "    with exact inference.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    torch.Tensor train_inputs: (size n x d) The training features :math:`\\mathbf X`.\n",
    "    \n",
    "    torch.Tensor train_targets: (size n) The training targets :math:`\\mathbf y`.\n",
    "    \n",
    "    ~gpytorch.likelihoods.GaussianLikelihood likelihood: The Gaussian likelihood that defines\n",
    "        the observational distribution. Since we're using exact inference, the likelihood must be Gaussian.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    The :meth:`__init__` function takes training data and a likelihood and computes the objects of mean and covariance \n",
    "    for the forward method\n",
    "\n",
    "    The :meth:`forward` function should describe how to compute the prior latent distribution\n",
    "    on a given input. Typically, this will involve a mean and kernel function.\n",
    "    The result must be a :obj:`~gpytorch.distributions.MultivariateNormal`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Calling this model will return the posterior of the latent Gaussian process when conditioned\n",
    "    on the training data. The output will be a :obj:`~gpytorch.distributions.MultivariateNormal`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_T, train_y, likelihood):\n",
    "        \"\"\"\n",
    "        Initializes the model\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        self : A class,The model itself. In this case, gpytorch.models.ExactGP\n",
    "        train_T : tensor, The inputs of the training data\n",
    "        train_y : tensor, the output of the training data\n",
    "        likelihood : bound method, the lieklihood of the model. In this case, it must be Gaussian\n",
    "        \n",
    "        \"\"\"\n",
    "        #Initializes the GP model with train_Y, train_y, and the likelihood\n",
    "        ##Calls the __init__ method of parent class\n",
    "        super(ExactGPModel, self).__init__(train_T, train_y, likelihood)\n",
    "        #Defines a constant prior mean on the GP. Used in the forward method\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        #Defines prior covariance matrix of GP to a scaled RFB Kernel. Used in the forward method\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        A forward method that takes in some (n×d) data, x, and returns a MultivariateNormal with the prior mean and \n",
    "        covariance evaluated at x. In other words, we return the vector μ(x) and the n×n matrix Kxx representing the \n",
    "        prior mean and covariance matrix of the GP.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        self : A class,The model itself. In this case, gpytorch.models.ExactGP\n",
    "        x : tensor, first input when class is called\n",
    "        \n",
    "        Returns:\n",
    "        Vector μ(x)\n",
    "        \n",
    "        \"\"\"\n",
    "        #Defines the mean of the GP based off of x\n",
    "        mean_x = self.mean_module(x) #1x100\n",
    "        #Defines the covariance matrix based off of x\n",
    "        covar_x = self.covar_module(x) #100 x 100 covariance matrix\n",
    "        #Constructs a multivariate normal random variable, based on mean and covariance. \n",
    "            #Can be multivariate, or a batch of multivariate normals\n",
    "            #Returns multivariate normal distibution gives the mean and covariance of the GP        \n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x) #Multivariate dist based on 1x100 tensor\n",
    "\n",
    "# initialize likelihood and model\n",
    "##Assumes a homoskedastic noise model p(y | f) = f + noise\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "#Defines our model in terms of the class parameters above\n",
    "model = ExactGPModel(train_T, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b4bbb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal model hyperparameters\n",
    "training_iter = 300\n",
    "\n",
    "#Puts the model in training mode\n",
    "model.train()\n",
    "\n",
    "#Puts the likelihood in training mode\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "    #algorithm for first-order gradient-based optimization of stochastic objective functions\n",
    "    # The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. \n",
    "    #The hyper-parameters have intuitive interpretations and typically require little tuning.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  #Needs GaussianLikelihood parameters, and a learning rate\n",
    "    #lr default is 0.001\n",
    "\n",
    "# Calculate\"Loss\" for GPs\n",
    "\n",
    "#The marginal log likelihood (the evidence: quantifies joint probability of the data under the prior)\n",
    "#returns an exact MLL for an exact Gaussian process with Gaussian likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model) #Takes a Gaussian likelihood and a model, a bound Method\n",
    "#iterates a give number of times\n",
    "for i in range(training_iter): #0-299\n",
    "    # Zero gradients from previous iteration - Prevents past gradients from influencing the next iteration\n",
    "    optimizer.zero_grad() \n",
    "    # Output from model\n",
    "    output = model(train_T) # A multivariate norm of a 1 x 100 tensor\n",
    "    # Calc loss and backprop gradients\n",
    "    #Minimizing -logMLL lets us fit hyperparameters\n",
    "    loss = -mll(output, train_y) #A number (tensor)\n",
    "    #computes dloss/dx for every parameter x which has requires_grad=True. \n",
    "    #These are accumulated into x.grad for every parameter x\n",
    "    loss.backward()\n",
    "#     print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "#         i + 1, training_iter, loss.item(),\n",
    "#         model.covar_module.base_kernel.lengthscale.item(),\n",
    "#          model.likelihood.noise.item()\n",
    "#     ))\n",
    "    #optimizer.step updates the value of x using the gradient x.grad. For example, the SGD optimizer performs:\n",
    "    #x += -lr * x.grad\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f5c9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "#Puts model in evaluation mode\n",
    "model.eval()\n",
    "#Puts likelihood in evaluation mode\n",
    "likelihood.eval()\n",
    "\n",
    "#Define Testing Data\n",
    "test_Theta1 =  np.linspace(-1,1,5) #1x5\n",
    "test_Theta2 =  np.linspace(-1,1,5) #1x5\n",
    "test_mesh = np.array(np.meshgrid(test_Theta1, test_Theta2)) #2 Uniform 5x5 arrays\n",
    "test_T = torch.tensor(test_mesh.T.reshape(-1, 2)) #25 x 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41599894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GP predicts that Theta1 = 1.0 and Theta2 = -1.0\n"
     ]
    }
   ],
   "source": [
    "#Code for EI\n",
    "\n",
    "with gpytorch.settings.fast_pred_var(), torch.no_grad():\n",
    "#torch.no_grad() \n",
    "    #Disabling gradient calculation is useful for inference, \n",
    "    #when you are sure that you will not call Tensor.backward(). It will reduce memory consumption\n",
    "    #Note: Can't use np operations on tensors where requires_grad = True\n",
    "#gpytorch.settings.fast_pred_var() \n",
    "    #Use this for improved performance when computing predictive variances. \n",
    "    #Good up to 10,000 data points\n",
    "#Predicts data points for model (sse) by sending the model through the likelihood\n",
    "    observed_pred = likelihood(model(test_T)) #1 x 25\n",
    "    \n",
    "#Calculates model mean  \n",
    "model_mean = observed_pred.mean\n",
    "#Calculates the variance of each data point\n",
    "model_variance = observed_pred.variance\n",
    "#Calculates the standard deviation of each data point\n",
    "model_stdev = np.sqrt(observed_pred.variance)\n",
    "sse_model = observed_pred.loc.numpy() #1 x 25\n",
    "\n",
    "#Formats sse data points into a suitable graphing form    \n",
    "sse_map = sse_model.reshape(len(test_Theta1),-1) #5 x 5\n",
    "#Formats stdev data points into suitable graphing form\n",
    "stdev_map = model_stdev.reshape(len(test_Theta1),-1) #5 x 5\n",
    "\n",
    "#Finds the index where sse is the smallest and finds which Theta combination corresponds to that value\n",
    "Theta_Opt_GP = test_T[np.argmin(sse_model)].numpy() #1x2\n",
    "print(\"The GP predicts that Theta1 =\",Theta_Opt_GP[0],\"and Theta2 =\", Theta_Opt_GP[1])\n",
    "\n",
    "explore_bias = 0\n",
    "best_error = calc_best_error(test_T, x, sse_model, noise)\n",
    "ei = calc_ei_basic(best_error,model_mean,model_variance, explore_bias)\n",
    "    \n",
    "#     #If variance is zero this is important \n",
    "#     with np.errstate(divide = 'warn'):\n",
    "#         #Creates upper and lower bounds and described by Nilay's word doc\n",
    "#         bound_upper = ((sse_true[j] - model_mean[j]) +np.sqrt(best_error))/model_variance[j]\n",
    "#         bound_lower = ((sse_true[j] - model_mean[j]) -np.sqrt(best_error))/model_variance[j]\n",
    "        \n",
    "#         #Creates EI terms in terms of Nilay's word doc\n",
    "#         ei_term1_comp1 = norm.cdf(bound_upper) - norm.cdf(bound_lower)\n",
    "#         ei_term1_comp2 = best_error - (sse_true[j] - model_mean[j])**2\n",
    "        \n",
    "#         ei_term2_comp1 = (sse_true[j] - model_mean[j])*model_stdev[j]\n",
    "#         ei_term2_comp2 = norm.pdf(bound_upper) - norm.pdf(bound_lower)\n",
    "        \n",
    "#         ei_term3_comp1 = (1/2)*norm.pdf(bound_upper/np.sqrt(2))\n",
    "#         ei_term3_comp2 = -norm.pdf(bound_upper)*bound_upper\n",
    "#         ei_term3_comp3 = (1/2)*norm.pdf(bound_lower/np.sqrt(2))\n",
    "#         ei_term3_comp4 = -norm.pdf(bound_lower)*bound_lower\n",
    "        \n",
    "#         ei_term3_psi_upper = ei_term3_comp1 + ei_term3_comp2\n",
    "#         ei_term3_psi_lower = ei_term3_comp3 + ei_term3_comp4\n",
    "        \n",
    "#         ei_term1 = ei_term1_comp1 + ei_term1_comp2\n",
    "#         ei_term2 = ei_term2_comp1 + ei_term2_comp2\n",
    "#         ei_term3 = -model_variance[j]*(ei_term3_psi_upper-ei_term3_psi_lower)\n",
    "        \n",
    "#         EI[j] = ei_term1 + ei_term2 + ei_term3\n",
    "       \n",
    "# print(\"The best error of\", round(best_error,3), \"is found at Theta 1 =\", best_theta[0].numpy(), \"and Theta 2 =\", best_theta[1].numpy())\n",
    "# print(\"The best EI of\", round(np.max(EI),3), \"is found at Theta 1 =\", test_T[np.argmax(EI)][0].numpy(), \"and Theta 2 =\", test_T[np.argmax(EI)][1].numpy())\n",
    "\n",
    "#Reshapes EI so that it can be plotted\n",
    "EI_map = ei.reshape(len(test_Theta1),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8344fb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sse_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18696/1537226461.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_plotter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_mesh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msse_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\Toy_Problem\\bo_functions.py\u001b[0m in \u001b[0;36my_plotter\u001b[1;34m(test_mesh, z)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;31m#Plots Theta1 vs Theta 2 with sse on the z axis and plots the color bar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;31m#Plot sse.T because test_mesh.T was used to calculate sse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontourf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msse_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sse_map' is not defined"
     ]
    }
   ],
   "source": [
    "y_plotter(test_mesh, sse_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf72ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Heat Map for SSE\n",
    "\n",
    "#Defines the x and y coordinates that will be used to generate the heat map, this step isn't\n",
    "#necessary, but streamlines the process\n",
    "xx , yy = test_mesh\n",
    "\n",
    "#Plots Theta1 vs Theta 2 with sse on the z axis and plots the color bar\n",
    "#Plot sse.T because test_mesh.T was used to calculate sse\n",
    "plt.contourf(xx, yy,sse_map.T)\n",
    "plt.colorbar()\n",
    "\n",
    "#Plots the true optimal value and the GP value\n",
    "plt.scatter(Theta_True[0],Theta_True[1], color=\"red\", label = \"True\", s=50)\n",
    "plt.scatter(Theta_Opt_GP[0],Theta_Opt_GP[1], color=\"orange\", label = \"GP\")\n",
    "\n",
    "#Plots axes such that they are scaled the same way (eg. circles look like circles)\n",
    "plt.axis('scaled')\n",
    "\n",
    "#Plots grid and legend\n",
    "plt.grid()\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "#Creates axis labels and title\n",
    "plt.xlabel('Theta 1',weight='bold')\n",
    "plt.ylabel('Theta 2',weight='bold')\n",
    "plt.title('Heat Map of SSE', weight='bold',fontsize = 16)\n",
    "\n",
    "#Shows plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18100b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots Theta1 vs Theta 2 with sse on the z axis and plots the color bar\n",
    "#Plot stdev.T because test_mesh.T was used to calculate stdev\n",
    "plt.contourf(xx,yy,stdev_map.T)\n",
    "plt.colorbar()\n",
    "\n",
    "#Plots the true optimal value and the GP value\n",
    "plt.scatter(Theta_True[0],Theta_True[1], color=\"red\", label = \"True\", s=50)\n",
    "plt.scatter(Theta_Opt_GP[0],Theta_Opt_GP[1], color=\"orange\", label = \"GP\")\n",
    "\n",
    "#Plots axes such that they are scaled the same way (eg. circles look like circles)\n",
    "plt.axis('scaled')\n",
    "\n",
    "#Plots grid and legend\n",
    "plt.grid()\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "#Creates axis labels and title\n",
    "plt.xlabel('Theta 1',weight='bold')\n",
    "plt.ylabel('Theta 2',weight='bold')\n",
    "plt.title('Heat Map of Standard Deviation', weight='bold',fontsize = 16)\n",
    "\n",
    "#Shows plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aecc2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots EI\n",
    "plt.contourf(xx, yy,EI_map.T)\n",
    "plt.colorbar()\n",
    "\n",
    "#Plots axes such that they are scaled the same way (eg. circles look like circles)\n",
    "plt.axis('scaled')\n",
    "\n",
    "#Plots grid and legend\n",
    "plt.grid()\n",
    "\n",
    "#Creates axis labels and title\n",
    "plt.xlabel('Theta 1',weight='bold')\n",
    "plt.ylabel('Theta 2',weight='bold')\n",
    "plt.title('Heat Map of Expected Improvement', weight='bold',fontsize = 16)\n",
    "\n",
    "#Shows plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cbf4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
